# train_pipeline.py
# -*- coding: utf-8 -*-
"""
ë©€í‹°ë¼ë²¨ í•™ìŠµ íŒŒì´í”„ë¼ì¸
- ê·œì¹™ ë¼ë²¨ë§(hybrid) â†’ (ìë™ ë¶„í• : single-class ë¼ë²¨ GMM/KMeans) â†’ (ì„ íƒ) ì»· íŠœë‹(ë¼ë²¨ìœ¨ ëª©í‘œ)
  â†’ ì‹œê°„ ë¶„í• (train/cal/test) â†’ LGBM(LogReg í´ë°±) í•™ìŠµ
- ë‹¨ì¼ í´ë˜ìŠ¤/ì €ì–‘ì„±, ìƒìˆ˜ í”¼ì²˜/ëˆ„ìˆ˜ ìë™ ë°©ì–´
- ê·œì¹™-í”¼ì²˜ ê°•ì œ ì œì™¸(Fallback): signals + gate ì „ë¶€ ì œê±°
- ìº˜ë¦¬ë¸Œë ˆì´ì…˜(isotonicâ†’sigmoid í´ë°±)
- ì„ê³„ê°’ ì„ íƒ ì „ëµ: cost / f1 / rec_at_prec(target) (+ë¼ë²¨ë³„ ë¹„ìš©ì§€ë„)
- ë©”íŠ¸ë¦­/ì„ê³„ê°’/ë©”íƒ€/ëª¨ë‹ˆí„°ë§ íˆìŠ¤í† ê·¸ë¨/ë¼ë²¨ìœ¨/ì¤‘ìš”ë„/PRì»¤ë¸Œ ì €ì¥
- (ì˜µì…˜) ê°œì¸ ê¸°ì¤€ Î”3m/Trend12m ê³„ì‚° ì§€ì›
- CLI ì¸ì ì§€ì›
"""
from pathlib import Path
from typing import Optional, List, Tuple, Dict
import os, json, warnings
from datetime import datetime

import numpy as np
import pandas as pd
warnings.filterwarnings("ignore")

# --- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ---
from .rules_loader import load_rules, rules_version
from .label_rules import apply_rule_hybrid
from .persona_soft import load_centers, soft_membership


# ========== ìœ í‹¸ ==========
def _resolve_col(df: pd.DataFrame, name: str):
    if not isinstance(name, str) or not name:
        return None
    base = name
    for suf in ["_pc_std", "_pc", "_std"]:
        if base.endswith(suf):
            base = base[: -len(suf)]
            break

    # 1ì°¨: ì •í™•/í‘œì¤€ í›„ë³´
    candidates = [name, f"{base}_std", base]
    seen = set()
    for c in candidates:
        if c and c not in seen:
            seen.add(c)
            if c in df.columns:
                return c

    # 2ì°¨: ëŠìŠ¨ ë§¤ì¹­(ê¸¸ì´ ì§§ì€ ê²ƒ ìš°ì„ , *_std ìš°ì„ )
    pool = [c for c in df.columns if isinstance(c, str) and base in c]
    if not pool:
        return None
    pool_std = [c for c in pool if c.endswith("_std")]
    if pool_std:
        pool = pool_std
    # 'í‰ê· ' ìš°ì„ , ê·¸ ë‹¤ìŒ ì „ì²´ ê¸¸ì´ ì§§ì€ ìˆœ
    pool.sort(key=lambda x: (0 if "í‰ê· " in x else 1, len(x)))
    return pool[0]


def run_diagnostic_check(df_lbl, rules_meta, lbl_cols, mask_train):
    """
    ê° ë¼ë²¨ë³„ë¡œ rules.ymlì— ì •ì˜ëœ ì‹ í˜¸ ì»¬ëŸ¼ì˜ ì¡´ì¬ ì—¬ë¶€ì™€
    í•™ìŠµ ë°ì´í„°ì…‹ ë‚´ì—ì„œì˜ ë¶„ì‚°(í‘œì¤€í¸ì°¨)ì„ ì ê²€í•˜ì—¬ ì¶œë ¥í•©ë‹ˆë‹¤.
    """
    print("\n" + "="*50)
    print("ğŸ•µï¸  STARTING DIAGNOSTIC CHECK...")
    print("="*50)

    for lbl in lbl_cols:
        print(f"\n[ì§„ë‹¨] ë¼ë²¨: {lbl}")
        tgt = lbl.replace("LBL_", "")
        obj = (rules_meta or {}).get("targets", {}).get(tgt, {}) or {}
        sigs = obj.get("signals", []) or []

        if not sigs:
            print("  - âš ï¸  rules.ymlì— ì •ì˜ëœ ì‹ í˜¸(signals)ê°€ ì—†ìŠµë‹ˆë‹¤.")
            continue

        for sig in sigs:
            col_name = sig.get("col")
            if not col_name:
                continue

            resolved = _resolve_col(df_lbl, col_name)
            if resolved is None:
                print(f"  - âŒ  '{col_name}' (ë§¤ì¹­ ì‹¤íŒ¨) | dfì— ëŒ€ì‘ ì»¬ëŸ¼ ì—†ìŒ")
                continue

            # í•™ìŠµ ê¸°ê°„ ë‚´ ë¶„ì‚° í™•ì¸
            train_series = pd.to_numeric(df_lbl.loc[mask_train, resolved], errors='coerce')
            std_dev = train_series.std()

            if pd.isna(std_dev) or std_dev < 1e-6:
                print(f"  - âš ï¸  '{col_name}' â†’ ì‚¬ìš©: '{resolved}' | í•™ìŠµ ê¸°ê°„ ë¶„ì‚° ê±°ì˜ ì—†ìŒ (std={std_dev:.4f})")
            else:
                print(f"  - âœ…  '{col_name}' â†’ ì‚¬ìš©: '{resolved}' | ìœ íš¨ (std={std_dev:.4f})")

    print("\n" + "="*50)
    print("ğŸ•µï¸  DIAGNOSTIC CHECK COMPLETE.")
    print("="*50 + "\n")




def ensure_year_month(df: pd.DataFrame) -> pd.DataFrame:
    """year_month(datetime64) ì»¬ëŸ¼ ë³´ì¥."""
    if "year_month" in df and np.issubdtype(df["year_month"].dtype, np.datetime64):
        return df
    if "month" in df:
        try:
            df["year_month"] = pd.to_datetime(df["month"]).dt.to_period("M").dt.to_timestamp()
            return df
        except Exception:
            pass
    if {"year", "month_num"}.issubset(df.columns):
        df["year_month"] = pd.to_datetime(
            dict(year=df["year"].astype(int), month=df["month_num"].astype(int), day=1)
        )
        return df
    raise ValueError("year_month ìƒì„± ì‹¤íŒ¨")


def as_pyfloat_list(arr) -> List[float]:
    return [float(x) for x in np.asarray(arr).ravel().tolist()]


# ê¸°ì¡´ _monthwise_robust_z_log1p ì™€ ensure_pc_std_all í•¨ìˆ˜ë¥¼ ì§€ìš°ê³  ì•„ë˜ ì½”ë“œë¡œ êµì²´

def ensure_all_standardized_features(df: pd.DataFrame, month_col: str = "year_month") -> pd.DataFrame:
    """
    ë°ì´í„°í”„ë ˆì„ì˜ ëª¨ë“  ìˆ«ìí˜• ì»¬ëŸ¼ì— ëŒ€í•´ ì›”ë³„ ê°•ê±´í•œ í‘œì¤€í™”(_std)ë¥¼ ì ìš©í•˜ëŠ” ìµœì¢… í•¨ìˆ˜.
    - ì›ë³¸ ì»¬ëŸ¼ì´ë¦„ì— _stdë¥¼ ë¶™ì—¬ ìƒˆ ì»¬ëŸ¼ì„ ìƒì„±.
    - ID, ë‚ ì§œ, ì¹´í…Œê³ ë¦¬ì„± ì»¬ëŸ¼ì€ ì œì™¸.
    """
    print("ëª¨ë“  ìˆ«ìí˜• í”¼ì²˜ì— ëŒ€í•œ í‘œì¤€í™”(_std)ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    out = df.copy()
    
    # í‘œì¤€í™”ì—ì„œ ì œì™¸í•  ì»¬ëŸ¼ë“¤
    exclude_cols = {month_col, "year", "month_num", "í–‰ì •ë™ì½”ë“œ", "ìì¹˜êµ¬", "ì„±ë³„", "ì—°ë ¹ëŒ€"}

    
    # dfì— ìˆëŠ” ëª¨ë“  ì»¬ëŸ¼ì„ ìˆœíšŒ
    for col in df.columns:
        if col in exclude_cols or col.endswith("_std"):
            continue
            
        # ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜ ê°€ëŠ¥í•œ ì»¬ëŸ¼ì¸ì§€ í™•ì¸ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
        if pd.api.types.is_numeric_dtype(df[col]):
            std_col_name = f"{col}_std"
            if std_col_name in out.columns:
                continue

            # ì›”ë³„ ê°•ê±´í•œ í‘œì¤€í™” (log1p + z-score)
            x = np.log1p(pd.to_numeric(df[col], errors="coerce").clip(lower=0))
            g = x.groupby(df[month_col])
            med = g.transform("median")
            iqr = g.transform(lambda v: v.quantile(0.75) - v.quantile(0.25))
            scale = iqr.where(iqr > 0, g.transform("std")).fillna(1.0)
            
            out[std_col_name] = ((x - med) / scale.where(scale > 0, 1.0)).fillna(0)
            print(f"  - '{std_col_name}' ìƒì„± ì™„ë£Œ.")

    # Î”/Trend í”¼ì²˜ë“¤ì€ ì›ë³¸ì´ ì•„ë‹Œ _std í”¼ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±í•˜ë„ë¡ ìˆ˜ì •
    base_std_cols_for_derived = [c for c in out.columns if c.endswith("_std")]
    out = ensure_delta3m(out, base_std_cols_for_derived, month_col)
    out = ensure_trend12m(out, base_std_cols_for_derived, month_col)

    return out


# ===== Î”3m/Trend12m: (A) ì›”í‰ê·  ê¸°ë°˜ ìŠ¤ëƒ…ìƒ·  =====
def ensure_delta3m(df: pd.DataFrame, base_cols: List[str], month_col: str = "year_month") -> pd.DataFrame:
    """ì›”í‰ê·  ê¸°ë°˜ 3ê°œì›” ë³€í™”(í˜„ì¬3M-ì§ì „3M). (ê°œì¸ID ì—†ì„ ë•Œì˜ ì•ˆì „í•œ í´ë°±)"""
    out = df.copy()
    base_cols = [c for c in base_cols if c in out.columns]
    if not base_cols: return out
    mdf = out.groupby([month_col], as_index=False)[base_cols].mean().sort_values(month_col)
    for c in base_cols:
        cur3 = mdf[c].rolling(3, min_periods=3).mean()
        prev3 = mdf[c].shift(3).rolling(3, min_periods=3).mean()
        mdf[c + "_delta3m"] = cur3 - prev3
    out = out.merge(mdf[[month_col] + [c + "_delta3m" for c in base_cols]], on=month_col, how="left")
    return out


def ensure_trend12m(df: pd.DataFrame, base_cols: List[str], month_col: str = "year_month") -> pd.DataFrame:
    """ì›”í‰ê·  ê¸°ë°˜ 12ê°œì›” íŠ¸ë Œë“œ(6M-6M). (ê°œì¸ID ì—†ì„ ë•Œì˜ ì•ˆì „í•œ í´ë°±)"""
    out = df.copy()
    base_cols = [c for c in base_cols if c in out.columns]
    if not base_cols: return out
    mdf = out.groupby([month_col], as_index=False)[base_cols].mean().sort_values(month_col)
    for c in base_cols:
        cur6 = mdf[c].rolling(6, min_periods=6).mean()
        prev6 = mdf[c].shift(6).rolling(6, min_periods=6).mean()
        mdf[c + "_trend12m"] = cur6 - prev6
    out = out.merge(mdf[[month_col] + [c + "_trend12m" for c in base_cols]], on=month_col, how="left")
    return out


# ===== Î”3m/Trend12m: (B) ê°œì¸ ê¸°ì¤€ ë²„ì „ (ê¶Œì¥) =====
def ensure_delta3m_per_id(df: pd.DataFrame, base_cols: List[str], id_col: str, month_col: str = "year_month") -> pd.DataFrame:
    """ê°œì¸ê¸°ì¤€ 3ê°œì›” ë³€í™”(í˜„ì¬3M-ì§ì „3M)."""
    out = df.sort_values([id_col, month_col]).copy()
    for c in [x for x in base_cols if x in out.columns]:
        g = out.groupby(id_col)[c]
        cur3  = g.transform(lambda s: s.rolling(3, min_periods=3).mean())
        prev3 = g.transform(lambda s: s.shift(3).rolling(3, min_periods=3).mean())
        out[c + "_delta3m"] = cur3 - prev3
    return out


def ensure_trend12m_per_id(df: pd.DataFrame, base_cols: List[str], id_col: str, month_col: str = "year_month") -> pd.DataFrame:
    """ê°œì¸ê¸°ì¤€ 12ê°œì›” íŠ¸ë Œë“œ(6M-6M)."""
    out = df.sort_values([id_col, month_col]).copy()
    for c in [x for x in base_cols if x in out.columns]:
        g = out.groupby(id_col)[c]
        cur6  = g.transform(lambda s: s.rolling(6, min_periods=6).mean())
        prev6 = g.transform(lambda s: s.shift(6).rolling(6, min_periods=6).mean())
        out[c + "_trend12m"] = cur6 - prev6
    return out


# ===== ì„ê³„ê°’ ì„ íƒ =====
def pick_threshold_cost(y_true, p_pred, cost_fn: float = 20.0, cost_fp: float = 1.0) -> float:
    y = np.asarray(y_true).astype(int)
    p = np.asarray(p_pred).astype(float)
    thr_grid = np.linspace(0.01, 0.99, 99)
    best_thr, best_cost = 0.5, 1e18
    for t in thr_grid:
        yhat = (p >= t).astype(int)
        fn = ((y == 1) & (yhat == 0)).sum()
        fp = ((y == 0) & (yhat == 1)).sum()
        cost = cost_fn * fn + cost_fp * fp
        if cost < best_cost:
            best_cost, best_thr = cost, t
    return float(best_thr)


def pick_threshold_f1(y_true, p_pred) -> float:
    from sklearn.metrics import f1_score
    y = np.asarray(y_true).astype(int)
    p = np.asarray(p_pred).astype(float)
    thr_grid = np.unique(np.clip(np.r_[np.linspace(0.01, 0.99, 99), p], 1e-4, 1-1e-4))
    best_thr, best_f1 = 0.5, -1.0
    for t in thr_grid:
        yhat = (p >= t).astype(int)
        f1 = f1_score(y, yhat, zero_division=0)
        if f1 > best_f1:
            best_f1, best_thr = f1, t
    return float(best_thr)


def pick_threshold_rec_at_prec(y_true, p_pred, target_prec: float = 0.6) -> float:
    from sklearn.metrics import precision_recall_curve
    y = np.asarray(y_true).astype(int)
    p = np.asarray(p_pred).astype(float)
    prec, rec, thr = precision_recall_curve(y, p)

    # precision ë°°ì—´ì€ ê¸¸ì´ê°€ recallë³´ë‹¤ 1 ê¸¸ë©°, thrëŠ” recall/precisionë³´ë‹¤ 1 ì§§ìŒ
    valid = np.where(prec[:-1] >= target_prec)[0]
    if len(valid) > 0:
        # ëª©í‘œ ì •ë°€ë„ ì´ìƒì¸ í›„ë³´ë“¤ ì¤‘ ë¦¬ì½œì´ ìµœëŒ€ì¸ ì§€ì  ì„ íƒ
        best = valid[np.argmax(rec[valid])]
        return float(thr[best])

    # ëª©í‘œ ì •ë°€ë„ë¥¼ ë§Œì¡± ëª»í•˜ë©´ F1 ê¸°ì¤€ìœ¼ë¡œ í´ë°±
    return pick_threshold_f1(y, p)


# ===== í”¼ì²˜/ëˆ„ìˆ˜ =====
def drop_low_variance_features(
    X_train: pd.DataFrame, X_cal: pd.DataFrame, X_test: pd.DataFrame
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, List[str]]:
    """Train ê¸°ì¤€ ìƒìˆ˜/ì¤€ìƒìˆ˜(ìœ ë‹ˆí¬<=1) í”¼ì²˜ ì œê±°."""
    const_cols = [c for c in X_train.columns if pd.Series(X_train[c]).nunique(dropna=False) <= 1]
    if const_cols:
        X_train = X_train.drop(columns=const_cols, errors="ignore")
        X_cal = X_cal.drop(columns=const_cols, errors="ignore")
        X_test = X_test.drop(columns=const_cols, errors="ignore")
    return X_train, X_cal, X_test, const_cols


def _safe_proba(est, X: pd.DataFrame) -> np.ndarray:
    """predict_proba/decision_function ìœ ë¬´ì™€ ë‹¨ì¼í´ë˜ìŠ¤ê¹Œì§€ ì•ˆì „ ì²˜ë¦¬."""
    if hasattr(est, "predict_proba"):
        proba = est.predict_proba(X)
        if proba.ndim == 1:
            return proba.astype(float)
        if proba.shape[1] == 1:
            return np.zeros(len(X), dtype=float)
        return proba[:, 1].astype(float)
    s = est.decision_function(X)
    s = (s - np.min(s)) / (np.max(s) - np.min(s) + 1e-12)
    return s.astype(float)


def _find_trivial_leak_cols(X: pd.DataFrame, y: np.ndarray, max_report: int = 5) -> List[str]:
    """í”¼ì²˜ê°€ ë¼ë²¨ê³¼ ì™„ì „íˆ ë™ì¼/ë³´ìƒ‰ì´ë©´ ëˆ„ìˆ˜ë¡œ ê°„ì£¼."""
    leaks = []
    yf = y.astype(float)
    for c in X.columns:
        xc = pd.to_numeric(X[c], errors="coerce").astype(float).fillna(-9999).values
        if np.array_equal(xc, yf):
            leaks.append(c)
        elif set(np.unique(yf)).issubset({0.0, 1.0}) and np.array_equal(1.0 - xc, yf):
            leaks.append(c)
        if len(leaks) >= max_report:
            break
    return leaks


# === ê·œì¹™-í”¼ì²˜ ì¶”ì¶œ: signals + gate + RULESRC_/REASON_ ===
# === ê·œì¹™-í”¼ì²˜ ì¶”ì¶œ: signals + gate + RULESRC_/REASON_ ===
def _rule_cols_for(label_name, rules_meta, candidate_feats, drop_ohe_from_gates=False):
    """
    ê·œì¹™ì— ì“°ì¸ ì»¬ëŸ¼ ì¤‘ 'ì›ë³¸'ê³¼ 'ì›ë³¸_std'ë§Œ ë“œë¡­í•˜ê³ ,
    delta/trend ë“± íŒŒìƒì€ ë³´ì¡´í•œë‹¤.
    ì˜ˆ) ê·œì¹™ì— "í‰ì¼ ì´ ì´ë™ ê±°ë¦¬ í•©ê³„_std_delta3m"ê°€ ìˆì–´ë„
        ëª¨ë¸ì—ì„œëŠ” "í‰ì¼ ì´ ì´ë™ ê±°ë¦¬ í•©ê³„" / "í‰ì¼ ì´ ì´ë™ ê±°ë¦¬ í•©ê³„_std"ë§Œ ë“œë¡­.
    """
    tgt = label_name.replace("LBL_", "")
    cols = set()

    def base_root(name: Optional[str]) -> Optional[str]:
        if not isinstance(name, str) or not name:
            return None
        s = name
        # íŒŒìƒ ì ‘ë¯¸ì–´ ì œê±° ìš°ì„ (íŒŒìƒì„ ë³´ì¡´í•˜ê¸° ìœ„í•´ rootë§Œ êµ¬í•œë‹¤)
        for suf in ["_delta3m", "_trend12m"]:
            if s.endswith(suf):
                s = s[: -len(suf)]
                break
        # í‘œì¤€ ì ‘ë¯¸ì–´ ì œê±°
        if s.endswith("_std"):
            s = s[: -len("_std")]
        return s

    def add_base(name: Optional[str]):
        """
        candidate_featsì—ì„œ 'root'ì™€ 'root_std'ë§Œ ë“œë¡­ ëŒ€ìƒì— ë„£ëŠ”ë‹¤.
        íŒŒìƒ( _delta3m / _trend12m )ì€ ë“œë¡­í•˜ì§€ ì•ŠëŠ”ë‹¤.
        """
        root = base_root(name)
        if not root:
            return
        # ì›ë³¸/í‘œì¤€ë§Œ ë“œë¡­
        for f in candidate_feats:
            if f == root or f == f"{root}_std":
                cols.add(f)
        # ê²Œì´íŠ¸ì˜ ì›-í•« ë“œë¡­ ì˜µì…˜
        if drop_ohe_from_gates:
            prefix = root + "_"
            for f in candidate_feats:
                if not isinstance(f, str):
                    continue
                if f.startswith(prefix):
                    # íŒŒìƒ í”¼ì²˜ëŠ” ìœ ì§€
                    if any(k in f for k in ["_std", "_delta3m", "_trend12m"]):
                        continue
                    cols.add(f)

    obj = (rules_meta or {}).get("targets", {}).get(tgt, {}) or {}

    # signals
    for s in obj.get("signals", []) or []:
        add_base((s or {}).get("col"))

    # gates / filters
    gates = obj.get("gate") or obj.get("gates") or obj.get("filters") or obj.get("conditions")
    if isinstance(gates, dict):
        for k in gates.keys():
            add_base(k)
    elif isinstance(gates, list):
        for g in gates:
            if isinstance(g, dict):
                add_base(g.get("col") or g.get("field") or next(iter(g.keys()), None))
            elif isinstance(g, str):
                add_base(g)

    # RULESRC_/REASON_ì€ í•­ìƒ ë“œë¡­
    for f in candidate_feats:
        if isinstance(f, str) and (f.startswith("RULESRC_") or f.startswith("REASON_")):
            cols.add(f)

    return sorted(cols)



# ====== SCORE í•©ì„± (ê·œì¹™ ê¸°ë°˜) ======
def _build_proxy_scores(df_in: pd.DataFrame, rules_meta: dict, lbl_cols: List[str]) -> pd.DataFrame:
    """
    rules.ymlì˜ signals(col, direction, weight)ë¥¼ ì´ìš©í•´ SCORE_<TARGET>ì„ í•©ì„±.
    - ì´ë¯¸ SCORE_*ê°€ ìˆìœ¼ë©´ ê±´ë“œë¦¬ì§€ ì•ŠìŒ
    - direction: high(+), low(-)ë¡œ ë¶€í˜¸ë§Œ ë°˜ì˜, weight ê°€ì¤‘í•©
    - ëª¨ë“  ì…ë ¥ì€ ê²¬ê³  ìŠ¤ì¼€ì¼(1~99%) í›„ í•©ì‚°
    - ê·œì¹™ ì»¬ëŸ¼ëª…ì´ _pc/_pc_std/_std ë“±ì´ì–´ë„ _resolve_colë¡œ ì‹¤ì œ ì»¬ëŸ¼ì— ë§¤í•‘
    """
    df = df_in.copy()
    targets = (rules_meta or {}).get("targets", {}) or {}

    def robust_minmax(x: pd.Series) -> pd.Series:
        x = pd.to_numeric(x, errors="coerce").astype(float)
        q1, q99 = x.quantile(0.01), x.quantile(0.99)
        denom = (q99 - q1) if (q99 > q1) else (x.std() or 1.0)
        out = (x - q1) / (denom if denom != 0 else 1.0)
        return out.fillna(0.0).clip(-5, 5)

    for lbl in lbl_cols:
        tgt = lbl.replace("LBL_", "")
        sname = f"SCORE_{tgt}"
        if sname in df.columns:
            continue

        obj = targets.get(tgt, {}) or {}
        sigs = obj.get("signals", []) or []
        if not sigs:
            # ì‹ í˜¸ê°€ ì „í˜€ ì—†ìœ¼ë©´ ì ìˆ˜ ìƒì„±ì„ íŒ¨ìŠ¤(íŠœë„ˆ/ì˜¤í† ìŠ¤í”Œë¦¿ì´ ì•Œì•„ì„œ ìŠ¤í‚µ)
            continue

        s_acc = pd.Series(0.0, index=df.index, dtype=float)
        used_any = False
        for sig in sigs:
            col_req = sig.get("col")
            col = _resolve_col(df, col_req)
            if not col:
                print(f"[SCORE] {lbl}: '{col_req}' â†’ ë§¤ì¹­ ì‹¤íŒ¨(ì»¬ëŸ¼ ì—†ìŒ)")
                continue
            w = float(sig.get("weight", 1.0))
            direction = (sig.get("direction") or "high").lower().strip()
            sign = 1.0 if direction == "high" else -1.0
            s_raw = robust_minmax(df[col])
            s_acc = s_acc + w * sign * s_raw
            used_any = True

        if used_any:
            # í•©ì„± ì ìˆ˜ë„ ë‹¤ì‹œ ê²¬ê³  ìŠ¤ì¼€ì¼ 0~1ë¡œ ì •ê·œí™”
            acc_scaled = robust_minmax(s_acc)
            acc_scaled = (acc_scaled - acc_scaled.min()) / (acc_scaled.max() - acc_scaled.min() + 1e-12)
            df[sname] = acc_scaled.fillna(0.0)
        else:
            # ì‚¬ìš©í•  ì‹ í˜¸ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ìƒì„± ì•ˆ í•¨(ì§„ë‹¨/ì˜¤í† ìŠ¤í”Œë¦¿ì—ì„œ ì•ˆë‚´)
            print(f"[SCORE] {lbl}: ì‚¬ìš© ê°€ëŠ¥í•œ ì‹ í˜¸ê°€ ì—†ì–´ SCORE ìƒì„± ìŠ¤í‚µ")
    return df

# ====== SCORE ê¸°ë³¸ê°’ ê°•ì œ ìƒì„±ê¸°(ê·œì¹™ ë¹„ì–´ë„ ë™ì‘) ======
def _ensure_default_scores(df_in: pd.DataFrame, lbl_cols: List[str]) -> pd.DataFrame:
    def cols_like(df, *patterns):
        pats = [p for p in patterns if isinstance(p, str)]
        return [c for c in df.columns if isinstance(c, str) and any(p in c for p in pats)]

    def robust_minmax(s: pd.Series) -> pd.Series:
        s = pd.to_numeric(s, errors="coerce").astype(float).fillna(0.0)
        q1, q99 = s.quantile(0.01), s.quantile(0.99)
        denom = (q99 - q1) if (q99 > q1) else (s.std() or 1.0)
        return ((s - q1) / (denom if denom != 0 else 1.0)).clip(0, 1)

    out = df_in.copy()

    # <- ì „ë¶€ *_std íŒ¨í„´ìœ¼ë¡œ ë§ì¶¤
    HOME   = cols_like(out, "ì§‘ ì¶”ì • ìœ„ì¹˜ í‰ì¼ ì´ ì²´ë¥˜ì‹œê°„_std")
    MOVE_D = cols_like(out, "í‰ì¼ ì´ ì´ë™ ê±°ë¦¬ í•©ê³„_std")
    MOVE_N = cols_like(out, "í‰ì¼ ì´ ì´ë™ íšŸìˆ˜_std")
    SUBWAY = cols_like(out, "ì§€í•˜ì² ì´ë™ì¼ìˆ˜ í•©ê³„_std")
    CHAT_N = cols_like(out, "í‰ê·  í†µí™”ëŒ€ìƒì ìˆ˜_std", "í‰ê·  ë¬¸ìëŒ€ìƒì ìˆ˜_std")
    CHAT_V = cols_like(out, "í‰ê·  í†µí™”ëŸ‰_std", "í‰ê·  ë¬¸ìëŸ‰_std")
    SNS    = cols_like(out, "SNS ì‚¬ìš©íšŸìˆ˜_std")
    MEDIA  = cols_like(out, "ë™ì˜ìƒ/ë°©ì†¡ ì„œë¹„ìŠ¤ ì‚¬ìš©ì¼ìˆ˜_std", "ìœ íŠœë¸Œ ì‚¬ìš©ì¼ìˆ˜_std", "ë„·í”Œë¦­ìŠ¤ ì‚¬ìš©ì¼ìˆ˜_std")
    DELTA3 = cols_like(out, "_delta3m")
    TREND12= cols_like(out, "_trend12m")
    HOME_D = [c for c in DELTA3 if "ì²´ë¥˜ì‹œê°„" in c]
    MOVE_DD= [c for c in DELTA3 if "ì´ë™" in c]
    WEEK_HOME = HOME
    HOUSING_CHG = cols_like(out, "ì£¼ê°„ìƒì£¼ì§€ ë³€ê²½íšŸìˆ˜ í‰ê· _std", "ì•¼ê°„ìƒì£¼ì§€ ë³€ê²½íšŸìˆ˜ í‰ê· _std")

    plans: Dict[str, Dict] = {
        "LBL_HOUSING": {"plus": HOUSING_CHG + HOME_D, "minus": [], "name": "SCORE_HOUSING"},
        "LBL_EMPLOYMENT": {"plus": MOVE_D + MOVE_N + SUBWAY, "minus": [], "name": "SCORE_EMPLOYMENT"},
        "LBL_ISOLATION": {"plus": WEEK_HOME, "minus": CHAT_N + CHAT_V + SNS, "name": "SCORE_ISOLATION"},
        "LBL_DAILY_LONGTERM": {"plus": WEEK_HOME + MEDIA, "minus": MOVE_D + MOVE_N + SUBWAY, "name": "SCORE_DAILY_LONGTERM"},
        "LBL_HEALTH_MENTAL_DELTA": {
            "plus": [*HOME_D, *MOVE_DD] + [c for c in TREND12 if any(k in c for k in ["ì´ë™","ì²´ë¥˜","ë™ì˜ìƒ","ì§€í•˜ì² "])],
            "minus": [], "absval": True, "name": "SCORE_HEALTH_MENTAL_DELTA",
        },
    }

    for lbl in lbl_cols:
        plan = plans.get(lbl)
        if not plan: continue
        sname = plan["name"]
        if sname in out.columns: continue

        plus  = [c for c in plan.get("plus", [])  if c in out.columns]
        minus = [c for c in plan.get("minus", []) if c in out.columns]
        if not plus and not minus:
            print(f"[SCORE/DEFAULT] {lbl}: ì‚¬ìš©í•  *_std í”¼ì²˜ê°€ ì—†ì–´ ê¸°ë³¸ SCORE ìƒì„± ìŠ¤í‚µ")
            continue

        score = pd.Series(0.0, index=out.index, dtype=float)

        def add(cols, sign=1.0, use_abs=False):
            w = 1.0 / max(len(cols), 1)
            for c in cols:
                v = pd.to_numeric(out[c], errors="coerce").astype(float).fillna(0.0)
                if use_abs: v = v.abs()
                score[:] = score.values + sign * w * v.values

        add(plus, +1.0, plan.get("absval", False))
        add(minus, -1.0, False)
        out[sname] = robust_minmax(score)

    return out




# ===== cut íŠœë„ˆ: ëª©í‘œ ë¼ë²¨ìœ¨ë¡œ ì»·ì„ ë¶„ìœ„ìˆ˜ë¡œ ì„¤ì • =====
def _tune_labels_by_scores(
    df_lbl: pd.DataFrame,
    lbl_cols: List[str],
    target_rate_map: Dict[str, float]
) -> pd.DataFrame:
    """
    SCORE_<TARGET> ë¶„í¬ì˜ (1 - rate) ë¶„ìœ„ìˆ˜ë¥¼ ì»·ìœ¼ë¡œ ì¡ì•„ ë¼ë²¨ìœ¨ì„ ë§ì¶¥ë‹ˆë‹¤.
    - ì™¸ë¶€ ë³€ìˆ˜ ì°¸ì¡° ì—†ìŒ
    - ì¬ê·€ í˜¸ì¶œ ì—†ìŒ
    - ë‹¨ì¼í´ë˜ìŠ¤ ë°©ì§€ ê°€ë“œ í¬í•¨
    """
    out = df_lbl.copy()

    # before preview
    try:
        before = {lbl: round(100.0 * float(out[lbl].astype(int).mean()), 3) for lbl in lbl_cols if lbl in out.columns}
        print(f"[TUNE] before rates(%) â†’ {before}")
    except Exception:
        pass

    for lbl in lbl_cols:
        if lbl not in out.columns:
            continue

        tgt = lbl.replace("LBL_", "")
        sname = f"SCORE_{tgt}"
        if sname not in out.columns:
            print(f"[TUNE/skip] {lbl}: '{sname}' ì—†ìŒ â†’ ìŠ¤í‚µ")
            continue

        s = pd.to_numeric(out[sname], errors="coerce").astype(float)
        if s.notna().sum() == 0:
            print(f"[TUNE/skip] {lbl}: ì ìˆ˜ê°€ ëª¨ë‘ NaN â†’ ìŠ¤í‚µ")
            continue

        # ëª©í‘œ ë¼ë²¨ìœ¨ (ì•ˆì „ í´ë¦¬í•‘)
        rate = float(target_rate_map.get(lbl, 0.05))
        rate = min(max(rate, 0.01), 0.99)

        thr = np.quantile(s.dropna(), 1.0 - rate)
        lab = (s >= thr).astype(bool)

        # ë‹¨ì¼í´ë˜ìŠ¤ ë°©ì§€ ê°€ë“œ
        m = float(lab.mean())
        if m in (0.0, 1.0):
            # ë” ì™„í™”/ê°•í™”í•´ì„œ ì¬ì‹œë„
            r2 = 0.01 if m == 1.0 else 0.99
            thr2 = np.quantile(s.dropna(), 1.0 - r2)
            lab = (s >= thr2).astype(bool)

        out[lbl] = lab

    # after preview
    try:
        after = {lbl: round(100.0 * float(out[lbl].astype(int).mean()), 2) for lbl in lbl_cols if lbl in out.columns}
        print(f"[TUNE] after rates(%)  â†’ {after}")
    except Exception:
        pass

    return out


# ===== single-class ìë™ ë¶„í• ê¸° (1D: GMM â†’ KMeans â†’ Quantile) =====
def auto_split_gmm_1d(scores: np.ndarray) -> Tuple[np.ndarray, float]:
    from sklearn.mixture import GaussianMixture
    x = scores.reshape(-1, 1)
    gmm = GaussianMixture(n_components=2, covariance_type='full', random_state=42)
    gmm.fit(x)
    means = gmm.means_.ravel()
    hi = int(np.argmax(means))
    post = gmm.predict_proba(x)[:, hi]
    # 0.5 ê¸°ì¤€ ë¶„ë¥˜, ì„ê³„ê°’ì€ ë‘ í‰ê· ì˜ ì¤‘ê°„ ì •ë„ë¡œ ë¦¬í¬íŠ¸
    thr = float(np.mean(means))
    y = (post >= 0.5).astype(bool)
    return y, thr


def auto_split_kmeans_1d(scores: np.ndarray) -> Tuple[np.ndarray, float]:
    from sklearn.cluster import KMeans
    x = scores.reshape(-1, 1)
    km = KMeans(n_clusters=2, n_init=10, random_state=42)
    lab = km.fit_predict(x)
    centers = km.cluster_centers_.ravel()
    hi = int(np.argmax(centers))
    y = (lab == hi)
    thr = float(np.mean(centers))
    return y, thr


def auto_split_single_class_labels(
    df_lbl: pd.DataFrame,
    lbl_cols: List[str],
    mask_train: pd.Series,
    min_pos_rate: float = 0.02,
    max_pos_rate: float = 0.20,
) -> pd.DataFrame:
    out = df_lbl.copy()
    single = []
    for lbl in lbl_cols:
        y_tr = out.loc[mask_train, lbl].astype(int)
        n_tr, n_pos = int(len(y_tr)), int(y_tr.sum())
        if n_tr == 0:
            continue
        if n_pos == 0 or n_pos == n_tr:
            single.append(lbl)
    if single:
        print("[AUTO] single-class labels â†’", single)
    else:
        return out

    for lbl in single:
        sname = f"SCORE_{lbl.replace('LBL_', '')}"
        if sname not in out.columns:
            print(f"[AUTO] {lbl}: auto-split skipped (not enough signals/score)")
            continue
        s_train = pd.to_numeric(out.loc[mask_train, sname], errors="coerce").astype(float)
        s_all   = pd.to_numeric(out[sname], errors="coerce").astype(float)
        valid_tr = s_train.notna().values
        if valid_tr.sum() < 100:
            print(f"[AUTO] {lbl}: auto-split skipped (too few valid scores)")
            continue
        try:
            y_split, thr = auto_split_gmm_1d(s_train[valid_tr].values)
        except Exception:
            try:
                y_split, thr = auto_split_kmeans_1d(s_train[valid_tr].values)
            except Exception:
                # ìµœí›„: ì¤‘ì•™ê°’ ê¸°ì¤€
                thr = float(np.nanmedian(s_train))
                y_split = (s_train[valid_tr].values >= thr)

        # ì „ì²´ ë°ì´í„°ì— ì„ê³„ê°’ ì ìš©
        new_lab = (s_all.values >= thr)
        rate = float(np.nanmean(new_lab))
        # ê³¼ë„í•œ ë¹„ìœ¨ì´ë©´ clip (ë¶„í•  íƒìš•ë„ ì œì–´)
        if rate < min_pos_rate:
            qthr = np.nanquantile(s_all, 1.0 - min_pos_rate)
            new_lab = (s_all.values >= qthr)
            rate = float(np.nanmean(new_lab))
        elif rate > max_pos_rate:
            qthr = np.nanquantile(s_all, 1.0 - max_pos_rate)
            new_lab = (s_all.values >= qthr)
            rate = float(np.nanmean(new_lab))

        out[lbl] = pd.Series(new_lab, index=out.index).astype(bool)
        print(f"[AUTO] {lbl}: auto-split done (rate={rate*100:.2f}%, thrâ‰ˆ{thr:.4f})")

    return out



def timeaware_backstop_tuner(df_lbl, lbl, score_col, month_col="year_month",
                             base_rate=0.0513, min_pos_per_month=15,
                             max_relax_quantile=0.95):
    out = df_lbl.copy()
    if score_col not in out.columns or lbl not in out.columns:
        return out

    y = out[lbl].astype(bool).copy()

    for m, idx in out.groupby(month_col).groups.items():
        sub = out.loc[idx]
        n = len(sub)
        if n == 0:
            continue

        p_now = int(sub[lbl].sum())
        if p_now >= min_pos_per_month:
            continue

        s = pd.to_numeric(sub[score_col], errors="coerce")
        valid = s.notna()
        n_valid = int(valid.sum())
        if n_valid == 0:
            continue

        k_rate = int(np.ceil(base_rate * n_valid))
        k_cap  = max(1, int(np.ceil((1.0 - max_relax_quantile) * n_valid)))  # ì˜ˆ: q=0.95 â†’ ìƒìœ„ 5%
        k_goal = min(max(min_pos_per_month, min(k_rate, k_cap)), n_valid)

        if p_now >= k_goal:
            continue

        need = k_goal - p_now
        not_pos_mask = ~sub[lbl].astype(bool) & valid
        if not_pos_mask.sum() == 0 or need <= 0:
            continue

        s_candidates = s[not_pos_mask]
        order = np.argsort(s_candidates.values)
        chosen = s_candidates.iloc[order[-need:]].index  # ìƒìœ„ needê°œë§Œ

        new_mask = pd.Series(False, index=sub.index)
        new_mask.loc[chosen] = True
        y.loc[idx] = (sub[lbl].astype(bool) | new_mask).values

    out[lbl] = y.astype(bool)
    return out





def apply_persistence(df_lbl, lbl, id_col=None, month_col="year_month",
                      window=3, min_hits=2, group_col=None):
    """
    ìµœê·¼ 'window'ê°œì›” ì¤‘ 'min_hits'ê°œì›” ì´ìƒ ì–‘ì„±ì´ë©´ ê·¸ ë‹¬ì„ ì–‘ì„±ìœ¼ë¡œ 'ìœ ì§€' (ê³¼í™•ëŒ€ ê¸ˆì§€).
    - id_colì´ ìˆìœ¼ë©´ id ê¸°ì¤€
    - ì—†ìœ¼ë©´ ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬(í–‰ì •ë™/ìì¹˜êµ¬/ì„±ë³„/ì—°ë ¹ëŒ€)ë¥¼ ëª¨ë‘ ë¬¶ì–´ ë³µí•©í‚¤ë¡œ ì‚¬ìš©
    - í‚¤ê°€ ì—†ìœ¼ë©´ no-op
    """
    out = df_lbl.copy()

    # 0) í‚¤ í™•ì •
    if id_col and id_col in out.columns:
        key_cols = [id_col]
    else:
        # group_col ì¸ìê°€ ë¦¬ìŠ¤íŠ¸/ë¬¸ìì—´ ëª¨ë‘ ì§€ì›
        if isinstance(group_col, (list, tuple)):
            cand = [c for c in group_col if c in out.columns]
        elif isinstance(group_col, str):
            cand = [c for c in [group_col] if c in out.columns]
        else:
            cand = []
        # ê¸°ë³¸ í›„ë³´(ìˆëŠ” ê²ƒë§Œ)
        fallback = [c for c in ["í–‰ì •ë™ì½”ë“œ", "ìì¹˜êµ¬", "ì„±ë³„", "ì—°ë ¹ëŒ€"] if c in out.columns]
        key_cols = cand or fallback

    if not key_cols:
        return out

    # 1) í‚¤+ì›” ë‹¨ìœ„ ì›” í”Œë˜ê·¸(ê·¸ í‚¤-ì›”ì— 1ê±´ì´ë¼ë„ ì–‘ì„±ì¸ì§€)
    out = out.sort_values(key_cols + [month_col])
    month_flag = (
        out.groupby(key_cols + [month_col])[lbl]
           .max()
           .reset_index(name="__mflag__")
           .sort_values(key_cols + [month_col])
    )

    # 2) í‚¤ë³„ ë¡¤ë§í•© â†’ min_hits ì´ìƒì´ë©´ 'ê·¸ í‚¤'ì—ì„œ ê·¸ ë‹¬ì€ ìœ ì§€(True)
    month_flag["__hits__"] = (
        month_flag.groupby(key_cols)["__mflag__"]
                  .transform(lambda s: s.rolling(window, min_periods=window).sum())
    )
    month_flag["__persist__"] = month_flag["__hits__"] >= float(min_hits)

    # 3) ì›ë³¸ìœ¼ë¡œ ë¸Œë¡œë“œìºìŠ¤íŠ¸ (êµì§‘í•©ìœ¼ë¡œë§Œ ìœ ì§€, ê³¼í™•ëŒ€ ë°©ì§€)
    out = out.merge(
        month_flag[key_cols + [month_col, "__persist__"]],
        on=key_cols + [month_col], how="left"
    )
    out[lbl] = out[lbl].astype(bool) & out["__persist__"].fillna(False)
    out.drop(columns=["__persist__", "__hits__", "__mflag__"], errors="ignore", inplace=True)
    return out

# ========== ë©”ì¸ ==========
def train_multilabel_with_calibration(
    csv_path: str = "telecom_group_monthly_all.csv",
    rules_path: Optional[str] = None,
    centers_path: Optional[str] = None,
    out_dir: str = "artifacts",
    exclude_months: List[pd.Timestamp] = None,
    cut_cal_from: pd.Timestamp = pd.Timestamp("2024-07-01"),
    cut_test_from: pd.Timestamp = pd.Timestamp("2025-01-01"),
    cost_fn: float = 20.0,
    cost_fp: float = 1.0,
    thr_strategy: str = "cost",          # "cost" | "f1" | "rec_at_prec"
    target_prec: float = 0.6,
    label_cost_map: Optional[Dict[str, float]] = None,  # {"LBL_CARE": 6, ...}
    dump_importance: bool = True,
    dump_prcurve: bool = True,
    id_col: Optional[str] = None,        # ê°œì¸ê¸°ì¤€ Î”/Trend ê³„ì‚°ìš© (ì—†ìœ¼ë©´ ì§‘ê³„ê¸°ì¤€ ì‚¬ìš©)
    use_target_rate_tuner: bool = True,  # ì»· íŠœë„ˆ on/off
    prec_labels: Optional[set] = None,
    per_label_target_map: Optional[Dict[str, float]] = None,
):
    # ê¸°ë³¸ ê²½ë¡œ: ì´ íŒŒì¼ ê¸°ì¤€(project/)
    here = Path(__file__).resolve().parent
    if rules_path is None:
        rules_path = str(here / "rules.yml")
    if centers_path is None:
        centers_path = str(here / "cluster_centers.csv")
    if exclude_months is None:
        exclude_months = [
            pd.Timestamp("2024-06-01"),
            pd.Timestamp("2024-08-01"),
            pd.Timestamp("2024-09-01"),
        ]
    os.makedirs(out_dir, exist_ok=True)

    # 0) ë¡œë“œ & year_month ë³´ì¥
    df = pd.read_csv(csv_path)
    df = ensure_year_month(df)
    
    df = ensure_all_standardized_features(df, month_col="year_month")


    # 1) ê·œì¹™ ë¼ë²¨ + SCORE ìƒì„±
    df_lbl = apply_rule_hybrid(df.copy())
    lbl_cols = [c for c in df_lbl.columns if c.startswith("LBL_") and df_lbl[c].dtype == bool]
    rules_meta_for_scores = load_rules(rules_path)
    df_lbl = _build_proxy_scores(df_lbl, rules_meta_for_scores, lbl_cols)
    df_lbl = _ensure_default_scores(df_lbl, lbl_cols)  # ê·œì¹™ ì—†ì–´ë„ SCORE_* ê°•ì œ ìƒì„±

    # 1-1) í’ˆì§ˆì›” ì œì™¸ ë° ì‹œê°„ ë§ˆìŠ¤í¬ (ìë™ ë¶„í• ì— ì‚¬ìš©)
    mask_ex = ~df_lbl["year_month"].isin(exclude_months)
    mask_train = (df_lbl["year_month"] < cut_cal_from) & mask_ex

    run_diagnostic_check(df_lbl, rules_meta_for_scores, lbl_cols, mask_train)

    # 1-2) single-class ë¼ë²¨ ìë™ ë¶„í• (GMM/KMeans)
    df_lbl = auto_split_single_class_labels(df_lbl, lbl_cols, mask_train,
                                            min_pos_rate=0.02, max_pos_rate=0.20)

    # 1-3) (ì„ íƒ) ë¼ë²¨ìœ¨ ëª©í‘œ ë§ì¶”ëŠ” íŠœë„ˆ
    if use_target_rate_tuner:
        target_rate_map = {
            "LBL_LIVELIHOOD": 0.05,
            "LBL_CARE": 0.08,
            "LBL_HEALTH_MENTAL_DELTA": 0.05,
            "LBL_DAILY_LONGTERM": 0.05,
            "LBL_HOUSING": 0.05,
            "LBL_EMPLOYMENT": 0.07,
            "LBL_DEBT_LAW": 0.032,
            "LBL_ISOLATION": 0.05,
        }
        df_lbl = _tune_labels_by_scores(df_lbl, lbl_cols, target_rate_map)

    # 1-4) ì‹œê°„ ì¸ì§€í˜• ë°±ìŠ¤í†± + ì§€ì†ì„± í•„í„° (ë¼ë²¨ ì†Œë©¸ ë°©ì§€)
    # Î”/Trend ì•ˆì • êµ¬ê°„(ìµœì´ˆ 6/12ê°œì›”)ì€ ë³´ìˆ˜ì ìœ¼ë¡œ False ì²˜ë¦¬ (ì´ˆê¸° í”ë“¤ë¦¼ ì œê±°)
    min_month_delta = df_lbl["year_month"].min() + pd.DateOffset(months=6)
    min_month_trend = df_lbl["year_month"].min() + pd.DateOffset(months=12)

    if "LBL_HEALTH_MENTAL_DELTA" in df_lbl.columns:
        df_lbl.loc[df_lbl["year_month"] < min_month_delta, "LBL_HEALTH_MENTAL_DELTA"] = False

    if "LBL_DAILY_LONGTERM" in df_lbl.columns:
        df_lbl.loc[df_lbl["year_month"] < min_month_trend, "LBL_DAILY_LONGTERM"] = False

    # ë‘ ë¼ë²¨ì— í•œì •í•´ ì›”ë³„ ë°±ìŠ¤í†± íŠœë‹ ì ìš© (ì›”ë³„ ìµœì†Œ ì–‘ì„± ë³´ì¥ - ë‹¤ì†Œ ë³´ìˆ˜í™”)
    for LBL, SCORE, base_rate, min_pos, relax_q in [
        ("LBL_HEALTH_MENTAL_DELTA", "SCORE_HEALTH_MENTAL_DELTA", 0.0513, 15, 0.95),
        ("LBL_DAILY_LONGTERM",      "SCORE_DAILY_LONGTERM",      0.0513, 15, 0.95),
    ]:
        if LBL in df_lbl.columns and SCORE in df_lbl.columns:
            df_lbl = timeaware_backstop_tuner(
                df_lbl, LBL, SCORE,
                month_col="year_month",
                base_rate=base_rate,
                min_pos_per_month=min_pos,
                max_relax_quantile=relax_q
            )

  # ë³µí•© í‚¤ êµ¬ì„±: idê°€ ìˆìœ¼ë©´ idë§Œ, ì—†ìœ¼ë©´ ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬ë“¤ì„ ëª¨ë‘ ì‚¬ìš©
    persistence_keys = ([id_col] if (id_col and id_col in df_lbl.columns) else
                        [c for c in ["í–‰ì •ë™ì½”ë“œ", "ìì¹˜êµ¬", "ì„±ë³„", "ì—°ë ¹ëŒ€"] if c in df_lbl.columns])

    # Î” ê¸‰ë³€
    if "LBL_HEALTH_MENTAL_DELTA" in df_lbl.columns:
        df_lbl = apply_persistence(
            df_lbl, "LBL_HEALTH_MENTAL_DELTA",
            id_col=id_col, group_col=persistence_keys, month_col="year_month",
            window=4, min_hits=3
        )

    # ì¥ê¸°
    if "LBL_DAILY_LONGTERM" in df_lbl.columns:
        df_lbl = apply_persistence(
            df_lbl, "LBL_DAILY_LONGTERM",
            id_col=id_col, group_col=persistence_keys, month_col="year_month",
            window=7, min_hits=4
        )

    if {"LBL_HEALTH_MENTAL_DELTA", "LBL_DAILY_LONGTERM"}.issubset(df_lbl.columns):
        _chk = (
            df_lbl.groupby("year_month")[["LBL_HEALTH_MENTAL_DELTA", "LBL_DAILY_LONGTERM"]]
                  .mean()
                  .tail(6)
        )
        print("\n[CHECK] ìµœê·¼ 6ê°œì›” ë¼ë²¨ìœ¨(ì›” í‰ê· ):")
        print(_chk.to_string())
        print("[CHECK] ìµœê·¼ 6ê°œì›” í‰ê· ìœ¨:",
              {c: round(float(_chk[c].mean()), 4) for c in _chk.columns})
    else:
        for c in ["LBL_HEALTH_MENTAL_DELTA", "LBL_DAILY_LONGTERM"]:
            if c in df_lbl.columns:
                print(f"[CHECK] {c} ìµœê·¼ 6ê°œì›”:",
                      df_lbl.groupby("year_month")[c].mean().tail(6).to_string())    
 

    # 2) í”¼ì²˜ì…‹(í‘œì¤€í™” + Î”/Trend + ì†Œí”„íŠ¸í˜ë¥´ì†Œë‚˜ + ì¸êµ¬í†µê³„)
    base_std = [
        # ê¸°ì¡´ í•µì‹¬
        "í‰ì¼ ì´ ì´ë™ ê±°ë¦¬ í•©ê³„_std", "íœ´ì¼ ì´ ì´ë™ ê±°ë¦¬ í•©ê³„_std",
        "ë™ì˜ìƒ/ë°©ì†¡ ì„œë¹„ìŠ¤ ì‚¬ìš©ì¼ìˆ˜_std", "ê²Œì„ ì„œë¹„ìŠ¤ ì‚¬ìš©ì¼ìˆ˜_std",
        "ì§€í•˜ì² ì´ë™ì¼ìˆ˜ í•©ê³„_std", "ì§‘ ì¶”ì • ìœ„ì¹˜ í‰ì¼ ì´ ì²´ë¥˜ì‹œê°„_std",
        "í‰ê·  í†µí™”ëŒ€ìƒì ìˆ˜_std", "í‰ê·  ë¬¸ìëŸ‰_std",
        "í‰ì¼ ì´ ì´ë™ íšŸìˆ˜_std",
        "ì£¼ê°„ìƒì£¼ì§€ ë³€ê²½íšŸìˆ˜ í‰ê· _std", "ì•¼ê°„ìƒì£¼ì§€ ë³€ê²½íšŸìˆ˜ í‰ê· _std",

        # ê·¸ë£¹ C(ìƒê³„/ì±„ë¬´) ë³´ê°•: ê¸ˆìœµ/ì†Œë¹„
        "ì†Œì•¡ê²°ì¬ ì‚¬ìš©ê¸ˆì•¡ í‰ê· _std", "ì†Œì•¡ê²°ì¬ ì‚¬ìš©íšŸìˆ˜ í‰ê· _std",
        "ìµœê·¼ 3ê°œì›” ë‚´ ìš”ê¸ˆ ì—°ì²´ ë¹„ìœ¨_std",
        "ì‡¼í•‘ ì„œë¹„ìŠ¤ ì‚¬ìš©ì¼ìˆ˜_std", "ê¸ˆìœµ ì„œë¹„ìŠ¤ ì‚¬ìš©ì¼ìˆ˜_std",
        "ë°°ë‹¬ ì„œë¹„ìŠ¤ ì‚¬ìš©ì¼ìˆ˜_std",
    ]
    # ì‹¤ì œ ì¡´ì¬ ì»¬ëŸ¼ìœ¼ë¡œ ì•ˆì „ ë§¤í•‘(_resolve_col ì‚¬ìš©)
    BASE_STD = [c for c in [_resolve_col(df_lbl, name) for name in base_std] if c]

    # Î”3m, Trend12mì€ ì‹¤ì œ _std ì»¬ëŸ¼ë“¤ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ê²ƒë§Œ ì±„íƒ
    DELTA3M = [c + "_delta3m" for c in BASE_STD if (c + "_delta3m") in df_lbl.columns]
    TREND12 = [c + "_trend12m" for c in BASE_STD if (c + "_trend12m") in df_lbl.columns]

    X = df_lbl[["year_month"] + BASE_STD + DELTA3M + TREND12].copy()


    
    # ì†Œí”„íŠ¸ í˜ë¥´ì†Œë‚˜ (ê²¬ê³  ê°€ë“œ: ì‹¤íŒ¨ ì‹œ í´ë°±)
    # 0) í˜¹ì‹œ ê¸°ì¡´ì— persona_*ê°€ ì´ë¯¸ ë¶™ì–´ ìˆìœ¼ë©´ ì œê±°(ì¤‘ë³µ ì¡°ì¸ ë°©ì§€)
    X = X.drop(columns=[c for c in X.columns if str(c).startswith("persona_")],
            errors="ignore")

    try:
        centers = load_centers(centers_path)          # persona_soft.load_centers
        P = soft_membership(df_lbl, centers, temp=1.0)  # persona_soft.soft_membership
        # (ì„ íƒ) í™•ì¸ ë¡œê·¸
        pcols = [c for c in P.columns if str(c).startswith("persona_")]
        print(f"[PERSONA] added: {pcols}")
        if "persona_label" in P.columns:
            print("[PERSONA] label ratio:",
                P["persona_label"].value_counts(normalize=True).round(3).to_dict())
    except Exception as e:
        print(f"[WARN] persona centers load/apply failed â†’ fallback to zeros ({e})")
        # ì„¼í„° ì‹¤íŒ¨ ì‹œ ì•ˆì „ í´ë°±(3ê°œ í´ëŸ¬ìŠ¤í„° ê°€ì •; í•„ìš”í•˜ë©´ ê°œìˆ˜ ë°”ê¿”ë„ ë¨)
        n = len(df_lbl)
        P = pd.DataFrame({
            "persona_p0": np.zeros(n, dtype=float),
            "persona_p1": np.zeros(n, dtype=float),
            "persona_p2": np.zeros(n, dtype=float),
            "persona_label": np.zeros(n, dtype=int),
            "persona_conf": np.zeros(n, dtype=float),
        }, index=df_lbl.index)

    # 1) í•œ ë²ˆë§Œ ì¡°ì¸
    X = X.join(P, how="left")   

    # ì¸êµ¬í†µê³„ ì›-í•«
    CAT = [c for c in ["ìì¹˜êµ¬", "ì„±ë³„", "ì—°ë ¹ëŒ€"] if c in df_lbl.columns]
    X = X.join(df_lbl[CAT])
    X = pd.get_dummies(X, columns=CAT, dummy_na=False)

    # 3) ì‹œê°„ ë¶„í•  & í’ˆì§ˆì›” ì œì™¸(ìµœì¢…)
    mask_ex2 = ~X["year_month"].isin(exclude_months)
    X = X.loc[mask_ex2].copy()
    Y = df_lbl.loc[mask_ex2, ["year_month"] + lbl_cols].copy()

    mask_train = Y["year_month"] < cut_cal_from
    mask_cal = (Y["year_month"] >= cut_cal_from) & (Y["year_month"] < cut_test_from)
    mask_test = Y["year_month"] >= cut_test_from

    X_train = X.loc[mask_train].drop(columns=["year_month"]).fillna(0)
    X_cal = X.loc[mask_cal].drop(columns=["year_month"]).fillna(0)
    X_test = X.loc[mask_test].drop(columns=["year_month"]).fillna(0)

    # ìƒìˆ˜/ì¤€ìƒìˆ˜ í”¼ì²˜ ì œê±°
    X_train, X_cal, X_test, dropped_const = drop_low_variance_features(X_train, X_cal, X_test)
    if dropped_const:
        print(f"[WARN] Drop constant features: {len(dropped_const)}")

    # 4) ëª¨ë¸ í•™ìŠµ + ìº˜ë¦¬ë¸Œë ˆì´ì…˜ + ì„ê³„ê°’
    try:
        import lightgbm as lgb
        def new_clf():
            return lgb.LGBMClassifier(
                n_estimators=400, learning_rate=0.05, num_leaves=127,
                min_data_in_leaf=30, min_child_samples=30,
                subsample=0.8, colsample_bytree=0.8,
                class_weight="balanced", objective="binary",
                random_state=42, n_jobs=-1, verbose=-1,
            )
    except Exception:
        from sklearn.linear_model import LogisticRegression
        def new_clf():
            return LogisticRegression(
                penalty="l2", C=1.0, max_iter=2000, class_weight="balanced", n_jobs=-1
            )

    from sklearn.calibration import CalibratedClassifierCV
    from sklearn.metrics import average_precision_score, f1_score, recall_score, precision_score, precision_recall_curve
    from joblib import dump, load as joblib_load

    models: Dict[str, Dict] = {}
    rows = []

    all_feat_cols = list(X_train.columns)
    rules_meta = load_rules(rules_path)  # ê·œì¹™-í”¼ì²˜ ì œì™¸ì— í™œìš©
    label_cost_map = label_cost_map or {}
    prec_labels = set(prec_labels or [])

    for lbl in lbl_cols:
        y_tr = Y.loc[mask_train, lbl].astype(int).to_numpy()
        y_ca = Y.loc[mask_cal, lbl].astype(int).to_numpy()
        y_te = Y.loc[mask_test, lbl].astype(int).to_numpy()

        n_tr_pos, n_tr = int(y_tr.sum()), int(len(y_tr))
        n_ca_pos, n_ca = int(y_ca.sum()), int(len(y_ca))
        n_te_pos, n_te = int(y_te.sum()), int(len(y_te))

        # ì¶©ë¶„ì¹˜ ì•Šìœ¼ë©´ ìŠ¤í‚µ
        if n_tr_pos < 100 or n_ca_pos < 20:
            rows.append({"label": lbl, "note": "skip(low positives)",
                         "train_pos": n_tr_pos, "cal_pos": n_ca_pos, "test_pos": n_te_pos})
            continue

        # í•™ìŠµ ì„¸íŠ¸ ë‹¨ì¼ í´ë˜ìŠ¤ë©´ ìŠ¤í‚µ
        if n_tr_pos == 0 or (n_tr_pos == n_tr):
            rows.append({"label": lbl, "note": "skip(single-class train)",
                         "train_pos": n_tr_pos, "cal_pos": n_ca_pos, "test_pos": n_te_pos})
            continue

        # ë¼ë²¨ë³„ ê·œì¹™-í”¼ì²˜ ê°•ì œ ì œì™¸(ëˆ„ìˆ˜ ë°©ì§€)
        rule_drop_cols = _rule_cols_for(lbl, rules_meta, all_feat_cols, drop_ohe_from_gates=True)
        feat_cols = [c for c in all_feat_cols if c not in rule_drop_cols]
        if rule_drop_cols:
            preview = ", ".join(sorted(rule_drop_cols)[:6]) + (" ..." if len(rule_drop_cols) > 6 else "")
            print(f"[RULE] {lbl}: drop {len(rule_drop_cols)} rule cols â†’ {preview}")

        # ë¼ë²¨ë³„ ëˆ„ìˆ˜(ì™„ì „ ì¼ì¹˜/ë³´ìƒ‰) ì œê±°
        leak_cols = _find_trivial_leak_cols(X_train[feat_cols], y_tr, max_report=50)
        feat_cols = [c for c in feat_cols if c not in leak_cols]
        if leak_cols:
            print(f"[LEAK] {lbl}: remove {len(leak_cols)} cols")

        # í•™ìŠµ/í‰ê°€ X
        Xtr = X_train[feat_cols]; Xca = X_cal[feat_cols]; Xte = X_test[feat_cols]

        clf = new_clf()
        clf.fit(Xtr, y_tr)

        # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê°€ëŠ¥ ì—¬ë¶€
        has_two_ca = (n_ca_pos > 0) and (n_ca_pos < n_ca)
        use_cal_for_thr = has_two_ca and (n_ca_pos >= 20)

        if has_two_ca:
            try:
                cal = CalibratedClassifierCV(clf, method="isotonic", cv="prefit")
                cal.fit(Xca, y_ca)
            except Exception:
                cal = CalibratedClassifierCV(clf, method="sigmoid", cv="prefit")
                cal.fit(Xca, y_ca)
            use_est = cal
        else:
            cal = None
            use_est = clf

        # ì„ê³„ê°’ ê¸°ì¤€ì„¸íŠ¸ ì„ íƒ
        if use_cal_for_thr:
            base_name, y_base, p_base = "cal",   y_ca, _safe_proba(use_est, Xca)
        else:
            base_name, y_base, p_base = "train", y_tr, _safe_proba(use_est, Xtr)

        # ---- ë¼ë²¨ë³„ ì „ëµ ì˜¤ë²„ë¼ì´ë“œ ì—¬ë¶€
        # ---- ë¼ë²¨ë³„ ì „ëµ ê²°ì •
        per_lbl_target = None if per_label_target_map is None else per_label_target_map.get(lbl)

        # rec_at_precë¥¼ ì“¸ì§€ ì—¬ë¶€: prec_labelsì— ìˆê±°ë‚˜ per_label_targetì„ ì§€ì •í–ˆì„ ë•Œ
        use_rec_override = (lbl in (prec_labels or set())) or (per_lbl_target is not None)

        if use_rec_override:
            tprec = per_lbl_target if per_lbl_target is not None else target_prec
            thr = pick_threshold_rec_at_prec(y_base, p_base, target_prec=tprec)
            cmsg = f"target_prec={tprec:.2f}" + (" (per-label)" if per_lbl_target is not None else " (override)")
            thr_used = "rec_at_prec"
        else:
            if thr_strategy == "f1":
                thr = pick_threshold_f1(y_base, p_base)
                cmsg = "cost_fn=-"
                thr_used = "f1"
            elif thr_strategy == "rec_at_prec":
                thr = pick_threshold_rec_at_prec(y_base, p_base, target_prec=target_prec)
                cmsg = f"target_prec={target_prec:.2f}"
                thr_used = "rec_at_prec"
            else:  # "cost"
                cf = float(label_cost_map.get(lbl, cost_fn))
                thr = pick_threshold_cost(y_base, p_base, cost_fn=cf, cost_fp=cost_fp)
                cmsg = f"cost_fn={cf}"
                thr_used = "cost"


    
    

        # ê¸°ì¤€ì„¸íŠ¸ì—ì„œ í”„ë¦¬ë·°
        yhat_base = (p_base >= thr).astype(int)
        prec_base = precision_score(y_base, yhat_base, zero_division=0)
        rec_base  = recall_score(y_base, yhat_base, zero_division=0)
        print(f"[THR] {lbl}: strategy={thr_used}, base={base_name}, thr={thr:0.3f}, {cmsg} â†’ prec_base={prec_base:0.3f}, rec_base={rec_base:0.3f}")

        # Test í™•ë¥  ë° ë©”íŠ¸ë¦­ ê³„ì‚°
        if len(y_te) > 0:
            p_te = _safe_proba(use_est, Xte)
            if len(p_te) == len(y_te):
                yhat = (p_te >= thr).astype(int)
                pr_auc = average_precision_score(y_te, p_te) if n_te_pos > 0 else np.nan
                rec = recall_score(y_te, yhat, average='binary', zero_division=0) if n_te_pos > 0 else np.nan
                f1 = f1_score(y_te, yhat, average='binary', zero_division=0) if n_te_pos > 0 else np.nan
            else:
                pr_auc = rec = f1 = np.nan
        else:
            p_te = np.array([])
            pr_auc = rec = f1 = np.nan

        # ëª¨ë¸ ì €ì¥
        model_path = os.path.join(out_dir, f"model_{lbl}.joblib")
        dump(use_est, model_path)

        # ì¤‘ìš”ë„ ë¤í”„
        if dump_importance:
            try:
                if hasattr(clf, "feature_importances_"):
                    fi = pd.Series(clf.feature_importances_, index=feat_cols).sort_values(ascending=False)
                elif hasattr(clf, "coef_"):
                    fi = pd.Series(np.abs(clf.coef_[0]), index=feat_cols).sort_values(ascending=False)
                else:
                    fi = None
                if fi is not None:
                    fi.head(200).to_csv(os.path.join(out_dir, f"feature_importance_{lbl}.csv"),
                                        index=True, header=["importance"], encoding="utf-8-sig")
            except Exception:
                pass

        # PR ì»¤ë¸Œ ë¤í”„
        if dump_prcurve and len(np.unique(y_base)) == 2:
            try:
                prec, rec_curve, thr_arr = precision_recall_curve(y_base, p_base)
                prdf = pd.DataFrame({
                    "threshold": np.r_[np.nan, thr_arr],
                    "precision": prec,
                    "recall": rec_curve
                })
                prdf.to_csv(os.path.join(out_dir, f"prcurve_{lbl}_{base_name}.csv"),
                            index=False, encoding="utf-8-sig")
            except Exception:
                pass

        models[lbl] = {
            "model_path": model_path,
            "threshold": float(thr),
            "threshold_strategy": thr_used,
            "target_precision": target_prec if thr_strategy == "rec_at_prec" else None,
            "cost_fn_used": float(label_cost_map.get(lbl, cost_fn)) if thr_strategy == "cost" else None,
            "calibrated": bool(cal is not None),
            "features": feat_cols,
            "dropped_rule_features": rule_drop_cols,
            "dropped_leak_features": leak_cols,
        }

        rows.append({
            "label": lbl,
            "train_pos": n_tr_pos,
            "cal_pos": n_ca_pos,
            "test_pos": n_te_pos,
            "thr": float(thr),
            "PR_AUC_test": float(pr_auc) if not np.isnan(pr_auc) else np.nan,
            "Recall@thr":  float(rec)    if not np.isnan(rec)    else np.nan,
            "F1@thr":      float(f1)     if not np.isnan(f1)     else np.nan,
            "note": None if has_two_ca else "no-cal(single-class cal)",
        })

    metrics_df = pd.DataFrame(rows).sort_values("PR_AUC_test", ascending=False, na_position="last")

    # 5) ëª¨ë‹ˆí„°ë§ ë² ì´ìŠ¤ë¼ì¸ ì €ì¥
    baseline: Dict[str, Dict] = {"feature_hist": {}, "timestamp": datetime.now().isoformat()}
    ref = X_train
    for c in ref.columns[:200]:
        s = pd.to_numeric(ref[c], errors="coerce").astype(float)
        qs = np.quantile(s.dropna().values, np.linspace(0, 1, 21))
        baseline["feature_hist"][c] = {"q": as_pyfloat_list(qs)}

    # pred_hist
    baseline["pred_hist"] = {}
   
    for lbl, obj in models.items():
        try:
            est = joblib_load(obj["model_path"])
            feats = obj["features"]
            p_ref = _safe_proba(est, ref[feats])
            qs = np.quantile(p_ref, np.linspace(0, 1, 21))
            baseline["pred_hist"][lbl] = {"q": as_pyfloat_list(qs)}
        except Exception:
            continue

    # 6) ì›”ë³„ ë¼ë²¨ìœ¨ ì €ì¥
    try:
        label_rate = Y.groupby("year_month")[lbl_cols].mean().reset_index().sort_values("year_month")
        label_rate.to_csv(os.path.join(out_dir, "label_rate_by_month.csv"),
                          index=False, encoding="utf-8-sig")
        # quick preview
        for c in ["LBL_HEALTH_MENTAL_DELTA", "LBL_DAILY_LONGTERM"]:
            if c in label_rate.columns:
                mpos = (label_rate[c] > 0).sum()
                print(f"[CHECK] {c}: ì–‘ì„± ì›” ìˆ˜ = {mpos}, ìµœê·¼ 6ê°œì›” í‰ê· ìœ¨ = {label_rate.tail(6)[c].mean():.4f}")
    except Exception:
        pass

    # 7) ì•„í‹°íŒ©íŠ¸ ì €ì¥
    with open(os.path.join(out_dir, "thresholds.json"), "w", encoding="utf-8") as f:
        json.dump({k: v["threshold"] for k, v in models.items()}, f, ensure_ascii=False, indent=2)

    rules_meta_for_meta = load_rules(rules_path)
    meta = {
        "created_at": datetime.now().isoformat(),
        "rules_version": rules_version(rules_meta_for_meta),
        "exclude_months": [str(x.date()) for x in exclude_months],
        "cut_cal_from": str(cut_cal_from.date()),
        "cut_test_from": str(cut_test_from.date()),
        "cost_fn_default": cost_fn,
        "cost_fp": cost_fp,
        "threshold_strategy": thr_strategy,
        "target_precision": target_prec,
        "label_cost_map": label_cost_map,
        "all_features": list(X_train.columns),
        "labels": [c for c in Y.columns if c.startswith("LBL_")],
        "ohe_categories": [c for c in X_train.columns if any(k in c for k in ["ìì¹˜êµ¬_", "ì„±ë³„_", "ì—°ë ¹ëŒ€_"])],
        "dropped_constant_features": dropped_const,
        "per_label_features": {k: v["features"] for k, v in models.items()},
        "per_label_rule_drops": {k: v["dropped_rule_features"] for k, v in models.items()},
        "per_label_leak_drops": {k: v["dropped_leak_features"] for k, v in models.items()},
    }
    with open(os.path.join(out_dir, "meta.json"), "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)
    with open(os.path.join(out_dir, "monitor_baseline.json"), "w", encoding="utf-8") as f:
        json.dump(baseline, f, ensure_ascii=False)

    metrics_csv = os.path.join(out_dir, "metrics.csv")
    metrics_df.to_csv(metrics_csv, index=False, encoding="utf-8-sig")

    return metrics_df, models


if __name__ == "__main__":
    import argparse
    ap = argparse.ArgumentParser()
    ap.add_argument("--csv_path", default="telecom_group_monthly_all.csv")
    ap.add_argument("--rules_path", default=None)      # None â†’ project/rules.yml
    ap.add_argument("--centers_path", default=None)    # None â†’ project/cluster_centers.csv
    ap.add_argument("--out_dir", default="artifacts")
    ap.add_argument("--cut_cal_from", default="2024-07-01")
    ap.add_argument("--cut_test_from", default="2025-01-01")
    ap.add_argument("--cost_fn", type=float, default=20.0)
    ap.add_argument("--cost_fp", type=float, default=1.0)
    ap.add_argument("--thr_strategy", choices=["cost", "f1", "rec_at_prec"], default="cost")
    ap.add_argument("--target", type=float, default=0.6, help="rec_at_prec ëª©í‘œ precision")
    ap.add_argument("--label_cost_map", default="", help='ì˜ˆ: "LBL_CARE:6,LBL_LIVELIHOOD:25"')
    ap.add_argument("--dump_importance", action="store_true")
    ap.add_argument("--dump_prcurve", action="store_true")
    ap.add_argument("--id_col", default=None, help="ê°œì¸ ê¸°ì¤€ Î”/Trend ê³„ì‚°ìš© ID ì»¬ëŸ¼ëª…")
    ap.add_argument("--no_tuner", action="store_true", help="ì»· íŠœë„ˆ ë¹„í™œì„±í™”")
    ap.add_argument(
        "--prec_labels",
        default="",
        help='rec_at_precì„ ì ìš©í•  ë¼ë²¨ ì½¤ë§ˆë¦¬ìŠ¤íŠ¸ (ì˜ˆ: "LBL_HOUSING,LBL_EMPLOYMENT,LBL_LIVELIHOOD,LBL_DEBT_LAW")'
    )
    ap.add_argument(
        "--per_label_target",
        default="",
        help='ë¼ë²¨ë³„ precision íƒ€ê²Ÿ. ì˜ˆ: "LBL_LIVELIHOOD:0.70,LBL_HOUSING:0.75"'
    )
        
    args = ap.parse_args()
    prec_labels = set([s.strip() for s in args.prec_labels.split(",") if s.strip()])

    # per_label_target íŒŒì‹±
    per_label_target_map: Dict[str, float] = {}
    if args.per_label_target:
        for token in args.per_label_target.split(","):
            token = token.strip()
            if not token: 
                continue
            k, v = token.split(":")
            per_label_target_map[k.strip()] = float(v)



    # label_cost_map íŒŒì‹±
    lmap: Dict[str, float] = {}
    if args.label_cost_map:
        for token in args.label_cost_map.split(","):
            token = token.strip()
            if not token: continue
            k, v = token.split(":")
            lmap[k.strip()] = float(v)

    m, _ = train_multilabel_with_calibration(
        csv_path=args.csv_path,
        rules_path=args.rules_path,
        centers_path=args.centers_path,
        out_dir=args.out_dir,
        cut_cal_from=pd.Timestamp(args.cut_cal_from),
        cut_test_from=pd.Timestamp(args.cut_test_from),
        cost_fn=args.cost_fn, cost_fp=args.cost_fp,
        thr_strategy=args.thr_strategy, target_prec=args.target,
        label_cost_map=lmap,
        dump_importance=args.dump_importance,
        dump_prcurve=args.dump_prcurve,
        id_col=args.id_col,
        use_target_rate_tuner=not args.no_tuner,
        prec_labels=prec_labels,
        per_label_target_map=per_label_target_map,
    )
    print(m.head(20))







