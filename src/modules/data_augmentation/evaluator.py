"""
ë°ì´í„° ì¦ê°• ì„±ëŠ¥ í‰ê°€ê¸°
ì›ë³¸ ë°ì´í„°ì™€ ì¦ê°•ëœ ë°ì´í„°ì˜ í’ˆì§ˆ ë¹„êµ ë° ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
"""

import logging
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Tuple
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import (
    accuracy_score, precision_recall_fscore_support, roc_auc_score,
    mean_squared_error, r2_score,
    classification_report, confusion_matrix
)
from sklearn.preprocessing import StandardScaler
from scipy import stats

logger = logging.getLogger(__name__)


class AugmentationEvaluator:
    """ë°ì´í„° ì¦ê°• í’ˆì§ˆ í‰ê°€ ì—”ì§„"""

    def __init__(self):
        self.evaluation_results = {}
        self.comparison_plots = {}

    def evaluate_augmentation_quality(
        self,
        original_data: pd.DataFrame,
        augmented_data: pd.DataFrame,
        target_columns: List[str] = None,
        test_size: float = 0.2
    ) -> Dict[str, Any]:
        """ì›ë³¸ ë°ì´í„°ì™€ ì¦ê°• ë°ì´í„°ì˜ ì „ì²´ì ì¸ í’ˆì§ˆ í‰ê°€"""
        try:
            logger.info("ë°ì´í„° ì¦ê°• í’ˆì§ˆ í‰ê°€ ì‹œì‘")

            evaluation_results = {
                "evaluation_timestamp": datetime.now().isoformat(),
                "data_info": {
                    "original_samples": len(original_data),
                    "augmented_samples": len(augmented_data),
                    "augmentation_ratio": len(augmented_data) / len(original_data)
                },
                "statistical_comparison": {},
                "distribution_analysis": {},
                "model_performance": {},
                "quality_metrics": {}
            }

            # 1. ê¸°ë³¸ í†µê³„ ë¹„êµ
            evaluation_results["statistical_comparison"] = self._compare_basic_statistics(
                original_data, augmented_data
            )

            # 2. ë¶„í¬ ìœ ì‚¬ì„± ë¶„ì„
            evaluation_results["distribution_analysis"] = self._analyze_distribution_similarity(
                original_data, augmented_data
            )

            # 3. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (íƒ€ê²Ÿì´ ìˆëŠ” ê²½ìš°)
            if target_columns:
                evaluation_results["model_performance"] = self._evaluate_model_performance(
                    original_data, augmented_data, target_columns, test_size
                )

            # 4. í’ˆì§ˆ ì§€í‘œ ê³„ì‚°
            evaluation_results["quality_metrics"] = self._calculate_quality_metrics(
                original_data, augmented_data
            )

            # 5. ì „ì²´ í’ˆì§ˆ ì ìˆ˜
            evaluation_results["overall_quality_score"] = self._calculate_overall_quality_score(
                evaluation_results
            )

            self.evaluation_results = evaluation_results
            logger.info("ë°ì´í„° ì¦ê°• í’ˆì§ˆ í‰ê°€ ì™„ë£Œ")
            return evaluation_results

        except Exception as e:
            logger.error(f"í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            raise

    def _compare_basic_statistics(
        self,
        original: pd.DataFrame,
        augmented: pd.DataFrame
    ) -> Dict[str, Any]:
        """ê¸°ë³¸ í†µê³„ ë¹„êµ"""
        stats_comparison = {}

        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
        numeric_columns = original.select_dtypes(include=[np.number]).columns
        common_columns = [col for col in numeric_columns if col in augmented.columns]

        for column in common_columns:
            orig_stats = original[column].describe()
            aug_stats = augmented[column].describe()

            # í†µê³„ ì°¨ì´ ê³„ì‚°
            stats_diff = {
                "mean_difference": abs(orig_stats['mean'] - aug_stats['mean']) / orig_stats['std'] if orig_stats['std'] > 0 else 0,
                "std_ratio": aug_stats['std'] / orig_stats['std'] if orig_stats['std'] > 0 else 0,
                "range_similarity": min(aug_stats['max'] - aug_stats['min'], orig_stats['max'] - orig_stats['min']) / max(aug_stats['max'] - aug_stats['min'], orig_stats['max'] - orig_stats['min']) if max(aug_stats['max'] - aug_stats['min'], orig_stats['max'] - orig_stats['min']) > 0 else 1
            }

            stats_comparison[column] = {
                "original_stats": orig_stats.to_dict(),
                "augmented_stats": aug_stats.to_dict(),
                "differences": stats_diff,
                "quality_score": (
                    (1 - min(stats_diff["mean_difference"], 1)) * 0.4 +
                    (1 - abs(1 - stats_diff["std_ratio"])) * 0.3 +
                    stats_diff["range_similarity"] * 0.3
                )
            }

        return stats_comparison

    def _analyze_distribution_similarity(
        self,
        original: pd.DataFrame,
        augmented: pd.DataFrame
    ) -> Dict[str, Any]:
        """ë¶„í¬ ìœ ì‚¬ì„± ë¶„ì„"""
        distribution_analysis = {}

        numeric_columns = original.select_dtypes(include=[np.number]).columns
        common_columns = [col for col in numeric_columns if col in augmented.columns]

        for column in common_columns:
            try:
                orig_data = original[column].dropna()
                aug_data = augmented[column].dropna()

                if len(orig_data) == 0 or len(aug_data) == 0:
                    continue

                # Kolmogorov-Smirnov í…ŒìŠ¤íŠ¸
                ks_statistic, ks_pvalue = stats.ks_2samp(orig_data, aug_data)

                # Mann-Whitney U í…ŒìŠ¤íŠ¸
                mannwhitney_statistic, mannwhitney_pvalue = stats.mannwhitneyu(
                    orig_data, aug_data, alternative='two-sided'
                )

                # ë¶„í¬ ìœ ì‚¬ì„± ì ìˆ˜ (KS í†µê³„ëŸ‰ì´ ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬)
                similarity_score = max(0, 1 - ks_statistic)

                distribution_analysis[column] = {
                    "ks_statistic": float(ks_statistic),
                    "ks_pvalue": float(ks_pvalue),
                    "mannwhitney_pvalue": float(mannwhitney_pvalue),
                    "similarity_score": similarity_score,
                    "distribution_match": ks_pvalue > 0.05  # ê°™ì€ ë¶„í¬ì—ì„œ ì™”ë‹¤ê³  ë³¼ ìˆ˜ ìˆëŠ”ì§€
                }

            except Exception as e:
                logger.warning(f"ì»¬ëŸ¼ {column} ë¶„í¬ ë¶„ì„ ì‹¤íŒ¨: {e}")
                continue

        return distribution_analysis

    def _evaluate_model_performance(
        self,
        original: pd.DataFrame,
        augmented: pd.DataFrame,
        target_columns: List[str],
        test_size: float
    ) -> Dict[str, Any]:
        """ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ í‰ê°€"""
        model_performance = {}

        for target_col in target_columns:
            if target_col not in original.columns or target_col not in augmented.columns:
                continue

            try:
                # íŠ¹ì„± ì»¬ëŸ¼ ì„ íƒ (íƒ€ê²Ÿ ì œì™¸)
                feature_columns = [col for col in original.columns
                                 if col != target_col and col in augmented.columns
                                 and original[col].dtype in ['int64', 'float64']]

                if len(feature_columns) == 0:
                    continue

                # ë°ì´í„° ì¤€ë¹„
                X_orig = original[feature_columns].fillna(0)
                y_orig = original[target_col].fillna(0)

                X_aug = augmented[feature_columns].fillna(0)
                y_aug = augmented[target_col].fillna(0)

                # ê²°í•©ëœ ë°ì´í„°
                X_combined = pd.concat([X_orig, X_aug], ignore_index=True)
                y_combined = pd.concat([y_orig, y_aug], ignore_index=True)

                # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
                X_orig_train, X_orig_test, y_orig_train, y_orig_test = train_test_split(
                    X_orig, y_orig, test_size=test_size, random_state=42
                )

                X_comb_train, X_comb_test, y_comb_train, y_comb_test = train_test_split(
                    X_combined, y_combined, test_size=test_size, random_state=42
                )

                # ìŠ¤ì¼€ì¼ë§
                scaler = StandardScaler()
                X_orig_train_scaled = scaler.fit_transform(X_orig_train)
                X_orig_test_scaled = scaler.transform(X_orig_test)

                scaler_comb = StandardScaler()
                X_comb_train_scaled = scaler_comb.fit_transform(X_comb_train)
                X_comb_test_scaled = scaler_comb.transform(X_comb_test)

                # ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
                performance_results = {}

                # íšŒê·€ ë˜ëŠ” ë¶„ë¥˜ ê²°ì •
                if len(np.unique(y_orig)) <= 10:  # ë¶„ë¥˜
                    models = {
                        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
                        'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000)
                    }
                    performance_results['task_type'] = 'classification'
                else:  # íšŒê·€
                    models = {
                        'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),
                        'LinearRegression': LinearRegression()
                    }
                    performance_results['task_type'] = 'regression'

                for model_name, model in models.items():
                    # ì›ë³¸ ë°ì´í„°ë§Œ ì‚¬ìš©
                    model_orig = model.__class__(**model.get_params())
                    model_orig.fit(X_orig_train_scaled, y_orig_train)
                    pred_orig = model_orig.predict(X_orig_test_scaled)

                    # ê²°í•©ëœ ë°ì´í„° ì‚¬ìš©
                    model_comb = model.__class__(**model.get_params())
                    model_comb.fit(X_comb_train_scaled, y_comb_train)
                    pred_comb = model_comb.predict(X_comb_test_scaled)

                    if performance_results['task_type'] == 'classification':
                        # ë¶„ë¥˜ ë©”íŠ¸ë¦­
                        orig_acc = accuracy_score(y_orig_test, pred_orig)
                        comb_acc = accuracy_score(y_comb_test, pred_comb)

                        performance_results[model_name] = {
                            'original_accuracy': float(orig_acc),
                            'combined_accuracy': float(comb_acc),
                            'improvement': float(comb_acc - orig_acc)
                        }
                    else:
                        # íšŒê·€ ë©”íŠ¸ë¦­
                        orig_mse = mean_squared_error(y_orig_test, pred_orig)
                        orig_r2 = r2_score(y_orig_test, pred_orig)

                        comb_mse = mean_squared_error(y_comb_test, pred_comb)
                        comb_r2 = r2_score(y_comb_test, pred_comb)

                        performance_results[model_name] = {
                            'original_mse': float(orig_mse),
                            'original_r2': float(orig_r2),
                            'combined_mse': float(comb_mse),
                            'combined_r2': float(comb_r2),
                            'mse_improvement': float(orig_mse - comb_mse),
                            'r2_improvement': float(comb_r2 - orig_r2)
                        }

                model_performance[target_col] = performance_results

            except Exception as e:
                logger.warning(f"íƒ€ê²Ÿ {target_col} ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì‹¤íŒ¨: {e}")
                continue

        return model_performance

    def _calculate_quality_metrics(
        self,
        original: pd.DataFrame,
        augmented: pd.DataFrame
    ) -> Dict[str, Any]:
        """í’ˆì§ˆ ì§€í‘œ ê³„ì‚°"""
        quality_metrics = {}

        # ê¸°ë³¸ í’ˆì§ˆ ì§€í‘œ
        quality_metrics['data_completeness'] = {
            'original_completeness': (1 - original.isnull().sum().sum() / (len(original) * len(original.columns))),
            'augmented_completeness': (1 - augmented.isnull().sum().sum() / (len(augmented) * len(augmented.columns)))
        }

        # ë‹¤ì–‘ì„± ì§€í‘œ
        numeric_columns = original.select_dtypes(include=[np.number]).columns
        common_columns = [col for col in numeric_columns if col in augmented.columns]

        diversity_scores = []
        for column in common_columns:
            orig_std = original[column].std()
            aug_std = augmented[column].std()
            if orig_std > 0:
                diversity_ratio = aug_std / orig_std
                diversity_scores.append(min(diversity_ratio, 2.0))  # Cap at 2.0

        quality_metrics['diversity_score'] = np.mean(diversity_scores) if diversity_scores else 0

        # í˜„ì‹¤ì„± ì§€í‘œ (ì´ìƒê°’ ë¹„ìœ¨)
        realism_scores = []
        for column in common_columns:
            orig_q1, orig_q3 = original[column].quantile([0.25, 0.75])
            orig_iqr = orig_q3 - orig_q1

            if orig_iqr > 0:
                orig_lower = orig_q1 - 1.5 * orig_iqr
                orig_upper = orig_q3 + 1.5 * orig_iqr

                aug_outliers = augmented[
                    (augmented[column] < orig_lower) | (augmented[column] > orig_upper)
                ]
                outlier_ratio = len(aug_outliers) / len(augmented)
                realism_score = max(0, 1 - outlier_ratio * 2)  # ì´ìƒê°’ì´ ì ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
                realism_scores.append(realism_score)

        quality_metrics['realism_score'] = np.mean(realism_scores) if realism_scores else 0

        return quality_metrics

    def _calculate_overall_quality_score(self, evaluation_results: Dict[str, Any]) -> float:
        """ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        try:
            scores = []

            # í†µê³„ì  ìœ ì‚¬ì„± ì ìˆ˜
            stat_scores = [
                result['quality_score']
                for result in evaluation_results['statistical_comparison'].values()
            ]
            if stat_scores:
                scores.append(np.mean(stat_scores))

            # ë¶„í¬ ìœ ì‚¬ì„± ì ìˆ˜
            dist_scores = [
                result['similarity_score']
                for result in evaluation_results['distribution_analysis'].values()
            ]
            if dist_scores:
                scores.append(np.mean(dist_scores))

            # í’ˆì§ˆ ì§€í‘œ ì ìˆ˜
            quality_metrics = evaluation_results['quality_metrics']
            if 'diversity_score' in quality_metrics:
                # ë‹¤ì–‘ì„± ì ìˆ˜ ì •ê·œí™” (1.0ì´ ì´ìƒì )
                diversity_normalized = 1 - abs(1 - quality_metrics['diversity_score'])
                scores.append(diversity_normalized)

            if 'realism_score' in quality_metrics:
                scores.append(quality_metrics['realism_score'])

            # ì „ì²´ ì ìˆ˜ (0-1 ë²”ìœ„)
            overall_score = np.mean(scores) if scores else 0.5
            return float(np.clip(overall_score, 0, 1))

        except Exception as e:
            logger.error(f"ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5

    def generate_evaluation_report(self, output_path: str = None) -> str:
        """í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±"""
        if not self.evaluation_results:
            return "í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € evaluate_augmentation_quality()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."

        results = self.evaluation_results

        report_lines = [
            "=" * 60,
            "ğŸ“Š ë°ì´í„° ì¦ê°• í’ˆì§ˆ í‰ê°€ ë¦¬í¬íŠ¸",
            "=" * 60,
            f"í‰ê°€ ì‹œê°: {results['evaluation_timestamp']}",
            "",
            "ğŸ“ˆ ë°ì´í„° ì •ë³´:",
            f"  ì›ë³¸ ìƒ˜í”Œ ìˆ˜: {results['data_info']['original_samples']:,}ê°œ",
            f"  ì¦ê°• ìƒ˜í”Œ ìˆ˜: {results['data_info']['augmented_samples']:,}ê°œ",
            f"  ì¦ê°• ë¹„ìœ¨: {results['data_info']['augmentation_ratio']:.2f}ë°°",
            "",
            f"ğŸ¯ ì „ì²´ í’ˆì§ˆ ì ìˆ˜: {results['overall_quality_score']:.3f}/1.000",
            ""
        ]

        # í†µê³„ ë¹„êµ ìš”ì•½
        if results['statistical_comparison']:
            report_lines.extend([
                "ğŸ“Š í†µê³„ì  ìœ ì‚¬ì„±:",
                "  ì»¬ëŸ¼ë³„ í’ˆì§ˆ ì ìˆ˜:"
            ])
            for col, stats in results['statistical_comparison'].items():
                score = stats['quality_score']
                report_lines.append(f"    {col}: {score:.3f}")

        # ë¶„í¬ ìœ ì‚¬ì„± ìš”ì•½
        if results['distribution_analysis']:
            report_lines.extend([
                "",
                "ğŸ“ˆ ë¶„í¬ ìœ ì‚¬ì„±:",
                "  ì»¬ëŸ¼ë³„ ìœ ì‚¬ì„± ì ìˆ˜:"
            ])
            for col, dist in results['distribution_analysis'].items():
                score = dist['similarity_score']
                match = "âœ“" if dist['distribution_match'] else "âœ—"
                report_lines.append(f"    {col}: {score:.3f} {match}")

        # í’ˆì§ˆ ì§€í‘œ
        if results['quality_metrics']:
            quality = results['quality_metrics']
            report_lines.extend([
                "",
                "ğŸ” í’ˆì§ˆ ì§€í‘œ:",
                f"  ë°ì´í„° ì™„ì„±ë„: {quality.get('diversity_score', 0):.3f}",
                f"  ë‹¤ì–‘ì„± ì ìˆ˜: {quality.get('diversity_score', 0):.3f}",
                f"  í˜„ì‹¤ì„± ì ìˆ˜: {quality.get('realism_score', 0):.3f}"
            ])

        # ëª¨ë¸ ì„±ëŠ¥ (ìˆëŠ” ê²½ìš°)
        if results['model_performance']:
            report_lines.extend([
                "",
                "ğŸ¤– ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:"
            ])
            for target, perf in results['model_performance'].items():
                task_type = perf.get('task_type', 'unknown')
                report_lines.append(f"  íƒ€ê²Ÿ: {target} ({task_type})")

                for model, metrics in perf.items():
                    if model == 'task_type':
                        continue

                    if task_type == 'classification':
                        improvement = metrics.get('improvement', 0)
                        symbol = "ğŸ“ˆ" if improvement > 0 else "ğŸ“‰" if improvement < 0 else "â¡ï¸"
                        report_lines.append(f"    {model}: {improvement:+.3f} {symbol}")
                    else:
                        r2_improvement = metrics.get('r2_improvement', 0)
                        symbol = "ğŸ“ˆ" if r2_improvement > 0 else "ğŸ“‰" if r2_improvement < 0 else "â¡ï¸"
                        report_lines.append(f"    {model} RÂ²: {r2_improvement:+.3f} {symbol}")

        report_lines.extend([
            "",
            "=" * 60
        ])

        report_text = "\n".join(report_lines)

        if output_path:
            try:
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(report_text)
                logger.info(f"í‰ê°€ ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {output_path}")
            except Exception as e:
                logger.error(f"í‰ê°€ ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")

        return report_text

    def save_evaluation_results(self, output_path: str):
        """í‰ê°€ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            import json
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(self.evaluation_results, f, ensure_ascii=False, indent=2)
            logger.info(f"í‰ê°€ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")
        except Exception as e:
            logger.error(f"í‰ê°€ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise