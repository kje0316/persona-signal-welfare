# -*- coding: utf-8 -*-
"""
bedrock_generator.py
AWS Bedrockì„ ì‚¬ìš©í•œ EC2 ê¸°ë°˜ í˜ë¥´ì†Œë‚˜ ìƒì„± ì—”ì§„
"""

import os
import json
import asyncio
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
import numpy as np
import pandas as pd

# AWS SDK
import boto3
from botocore.exceptions import ClientError, BotoCoreError

# ê¸°ì¡´ ë°ì´í„° ë¶„ì„ ëª¨ë“ˆë“¤
import sys
sys.path.append(str(Path(__file__).parent.parent))

try:
    from data_analysis.clustering import persona_clustering
    from data_analysis.risk_scoring.rules_loader import load_rules, apply_rules_to_dataframe
except ImportError as e:
    logging.warning(f"ë°ì´í„° ë¶„ì„ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class BedrockPersonaGenerator:
    """AWS Bedrock Claude 3.5 Haikuë¥¼ ì‚¬ìš©í•œ í˜ë¥´ì†Œë‚˜ ìƒì„±ê¸°"""

    def __init__(self, region_name: str = "us-east-1"):
        """
        AWS Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”

        Args:
            region_name: AWS ë¦¬ì „ (ê¸°ë³¸ê°’: us-east-1)
        """
        self.region_name = region_name
        self.model_id = "anthropic.claude-3-5-haiku-20241022-v1:0"  # Claude 3.5 Haiku

        # Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        try:
            self.bedrock_client = boto3.client(
                'bedrock-runtime',
                region_name=region_name
            )
            logger.info(f"âœ… AWS Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ (ë¦¬ì „: {region_name})")
        except Exception as e:
            logger.error(f"âŒ AWS Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

        # ë°ì´í„° ê²½ë¡œ ì„¤ì •
        self.project_root = Path(__file__).parent.parent.parent.parent
        self.data_path = self.project_root / "src" / "modules" / "data_analysis"

        # ìºì‹œ
        self.cluster_cache = {}
        self.knowledge_cache = {}

    def load_cluster_data(self) -> Optional[pd.DataFrame]:
        """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ë°ì´í„° ë¡œë“œ"""
        try:
            # ê°€ëŠ¥í•œ ë°ì´í„° íŒŒì¼ ê²½ë¡œë“¤
            possible_paths = [
                self.data_path / "rag_aug_out" / "processed_data.csv",
                self.data_path / "risk_scoring" / "telecom_group_monthly_all_with_preds.csv",
                self.project_root / "data" / "processed_telecom_data.csv"
            ]

            for path in possible_paths:
                if path.exists():
                    logger.info(f"ğŸ“Š ë°ì´í„° ë¡œë”©: {path}")
                    return pd.read_csv(path)

            logger.warning("âš ï¸ í´ëŸ¬ìŠ¤í„° ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return None

        except Exception as e:
            logger.error(f"âŒ í´ëŸ¬ìŠ¤í„° ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            return None

    def load_knowledge_base(self) -> Dict[str, Any]:
        """RAG ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ"""
        if self.knowledge_cache:
            return self.knowledge_cache

        try:
            kb_path = self.data_path / "rag_aug_out" / "kb_chunks.jsonl"

            if not kb_path.exists():
                logger.warning("âš ï¸ kb_chunks.jsonl íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return {}

            knowledge = {}
            with open(kb_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        chunk = json.loads(line.strip())
                        chunk_type = chunk.get('meta', {}).get('type', 'general')
                        knowledge[chunk_type] = chunk.get('text', '')

            self.knowledge_cache = knowledge
            logger.info(f"ğŸ“š ì§€ì‹ ë² ì´ìŠ¤ ë¡œë”© ì™„ë£Œ: {len(knowledge)}ê°œ ì²­í¬")
            return knowledge

        except Exception as e:
            logger.error(f"âŒ ì§€ì‹ ë² ì´ìŠ¤ ë¡œë”© ì‹¤íŒ¨: {e}")
            return {}

    async def call_bedrock_async(self, prompt: str, max_tokens: int = 2000) -> str:
        """AWS Bedrock Claude ë¹„ë™ê¸° í˜¸ì¶œ"""
        try:
            # Claude 3.5 ìš”ì²­ í˜•ì‹
            request_body = {
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": max_tokens,
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "temperature": 0.7,
                "top_p": 0.9
            }

            # ë¹„ë™ê¸° í˜¸ì¶œì„ ìœ„í•œ executor ì‚¬ìš©
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.bedrock_client.invoke_model(
                    modelId=self.model_id,
                    contentType="application/json",
                    accept="application/json",
                    body=json.dumps(request_body)
                )
            )

            # ì‘ë‹µ íŒŒì‹±
            response_body = json.loads(response['body'].read())
            content = response_body.get('content', [{}])[0].get('text', '')

            logger.debug(f"ğŸ¤– Bedrock ì‘ë‹µ ê¸¸ì´: {len(content)} ë¬¸ì")
            return content

        except ClientError as e:
            error_code = e.response['Error']['Code']
            logger.error(f"âŒ Bedrock API ì˜¤ë¥˜ ({error_code}): {e}")
            raise
        except Exception as e:
            logger.error(f"âŒ Bedrock í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            raise

    def generate_cluster_summary(self, cluster_data: pd.DataFrame, cluster_id: int) -> Dict[str, Any]:
        """í´ëŸ¬ìŠ¤í„° ë°ì´í„° ìš”ì•½ ìƒì„±"""
        try:
            # ê¸°ë³¸ í†µê³„
            summary = {
                "cluster_id": cluster_id,
                "sample_count": len(cluster_data),
                "demographics": {},
                "behavior_patterns": {},
                "welfare_indicators": {}
            }

            # ì¸êµ¬í†µê³„ ì •ë³´
            if 'ì—°ë ¹ëŒ€' in cluster_data.columns:
                summary["demographics"]["avg_age"] = float(cluster_data['ì—°ë ¹ëŒ€'].mean())
                summary["demographics"]["age_distribution"] = cluster_data['ì—°ë ¹ëŒ€'].value_counts().to_dict()

            if 'ìì¹˜êµ¬' in cluster_data.columns:
                summary["demographics"]["districts"] = cluster_data['ìì¹˜êµ¬'].value_counts().head(3).to_dict()

            if 'ì„±ë³„' in cluster_data.columns:
                summary["demographics"]["gender_ratio"] = cluster_data['ì„±ë³„'].value_counts(normalize=True).to_dict()

            # í–‰ë™ íŒ¨í„´ (ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë“¤ì˜ í‰ê· )
            behavior_cols = [
                'í‰ì¼ ì´ ì´ë™ ê±°ë¦¬ í•©ê³„', 'ë™ì˜ìƒ/ë°©ì†¡ ì„œë¹„ìŠ¤ ì‚¬ìš©ì¼ìˆ˜',
                'ë°°ë‹¬ ì„œë¹„ìŠ¤ ì‚¬ìš©ì¼ìˆ˜', 'í‰ê·  í†µí™”ëŒ€ìƒì ìˆ˜', 'SNS ì‚¬ìš©íšŸìˆ˜'
            ]

            for col in behavior_cols:
                if col in cluster_data.columns:
                    value = cluster_data[col].mean()
                    if not pd.isna(value):
                        summary["behavior_patterns"][col] = float(value)

            # ë³µì§€ ì§€í‘œ (í™•ë¥  ì»¬ëŸ¼ë“¤)
            welfare_cols = [col for col in cluster_data.columns if col.startswith('proba_LBL_')]
            for col in welfare_cols:
                value = cluster_data[col].mean()
                if not pd.isna(value):
                    summary["welfare_indicators"][col] = float(value)

            return summary

        except Exception as e:
            logger.error(f"âŒ í´ëŸ¬ìŠ¤í„° ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"cluster_id": cluster_id, "error": str(e)}

    def create_persona_prompt(self, cluster_summary: Dict[str, Any], knowledge_context: str) -> str:
        """í˜ë¥´ì†Œë‚˜ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ì‘ì„±"""

        cluster_id = cluster_summary.get("cluster_id", 0)
        demographics = cluster_summary.get("demographics", {})
        behaviors = cluster_summary.get("behavior_patterns", {})
        welfare = cluster_summary.get("welfare_indicators", {})

        # ì£¼ìš” ë³µì§€ ìš•êµ¬ ì‹ë³„
        high_welfare_needs = []
        for key, value in welfare.items():
            if value > 0.6:  # ë†’ì€ ìš•êµ¬
                category = key.replace('proba_LBL_', '')
                high_welfare_needs.append(category)

        prompt = f"""
ì„œìš¸ì‹œ 1ì¸ê°€êµ¬ ë°ì´í„° ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ í˜„ì‹¤ì ì¸ í˜ë¥´ì†Œë‚˜ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

## ë°ì´í„° ë¶„ì„ ê²°ê³¼ (í´ëŸ¬ìŠ¤í„° {cluster_id})
- í‘œë³¸ ìˆ˜: {cluster_summary.get('sample_count', 0)}ëª…
- í‰ê·  ì—°ë ¹ëŒ€: {demographics.get('avg_age', 40):.1f}ì„¸
- ì£¼ìš” ê±°ì£¼ì§€ì—­: {list(demographics.get('districts', {}).keys())[:2]}
- í–‰ë™ íŒ¨í„´ ì ìˆ˜:
  * ì´ë™ì„±: {behaviors.get('í‰ì¼ ì´ ì´ë™ ê±°ë¦¬ í•©ê³„', 0):.2f}
  * ë””ì§€í„¸ í™œìš©: {behaviors.get('ë™ì˜ìƒ/ë°©ì†¡ ì„œë¹„ìŠ¤ ì‚¬ìš©ì¼ìˆ˜', 0):.2f}
  * ì‚¬íšŒì  ì†Œí†µ: {behaviors.get('í‰ê·  í†µí™”ëŒ€ìƒì ìˆ˜', 0):.2f}
  * ë°°ë‹¬ ì„œë¹„ìŠ¤: {behaviors.get('ë°°ë‹¬ ì„œë¹„ìŠ¤ ì‚¬ìš©ì¼ìˆ˜', 0):.2f}
- ì£¼ìš” ë³µì§€ ìš•êµ¬: {high_welfare_needs[:3]}

## ì»¨í…ìŠ¤íŠ¸ ì •ë³´
{knowledge_context[:1000]}

## ìš”ì²­ì‚¬í•­
ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ í˜•ì‹ì˜ JSONìœ¼ë¡œ í˜„ì‹¤ì ì¸ í˜ë¥´ì†Œë‚˜ 1ëª…ì„ ìƒì„±í•´ì£¼ì„¸ìš”:

{{
  "name": "í•œêµ­ì‹ ì´ë¦„",
  "age": êµ¬ì²´ì  ë‚˜ì´,
  "gender": "ë‚¨ì„±" ë˜ëŠ” "ì—¬ì„±",
  "district": "ì„œìš¸ì‹œ ìì¹˜êµ¬ëª…",
  "occupation": "ì§ì—…",
  "living_situation": "ê±°ì£¼ í˜•íƒœ ì„¤ëª…",
  "personality_traits": ["ì„±ê²© íŠ¹ì„± 3ê°œ"],
  "daily_routine": "í•˜ë£¨ ì¼ê³¼ ì„¤ëª… (2-3ë¬¸ì¥)",
  "digital_habits": "ë””ì§€í„¸ ê¸°ê¸°/ì„œë¹„ìŠ¤ ì‚¬ìš© íŒ¨í„´",
  "social_patterns": "ì‚¬íšŒì  ê´€ê³„ ë° ì†Œí†µ ë°©ì‹",
  "mobility_style": "ì´ë™ íŒ¨í„´ ë° êµí†µìˆ˜ë‹¨ ì´ìš©",
  "consumption_habits": "ì†Œë¹„ íŒ¨í„´ ë° ì„œë¹„ìŠ¤ ì´ìš©",
  "challenges": ["ì£¼ìš” ì–´ë ¤ì›€ 3ê°œ"],
  "welfare_needs": ["í•„ìš”í•œ ë³µì§€ ì„œë¹„ìŠ¤ 3ê°œ"],
  "goals": ["ê°œì¸ì  ëª©í‘œë‚˜ ë°”ëŒ 2ê°œ"],
  "background_story": "ì´ ì‚¬ëŒì˜ ë°°ê²½ê³¼ í˜„ì¬ ìƒí™© ì„¤ëª… (3-4ë¬¸ì¥)"
}}

ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ê³ , í•œêµ­ì˜ 1ì¸ê°€êµ¬ í˜„ì‹¤ì„ ì •í™•íˆ ë°˜ì˜í•´ì£¼ì„¸ìš”.
"""

        return prompt

    async def generate_single_persona(self, cluster_summary: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¨ì¼ í˜ë¥´ì†Œë‚˜ ìƒì„±"""
        try:
            # ì§€ì‹ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
            knowledge = self.load_knowledge_base()
            context = knowledge.get('overview', '') + '\n' + knowledge.get('feature_mapping', '')[:500]

            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.create_persona_prompt(cluster_summary, context)

            # Bedrock í˜¸ì¶œ
            logger.info(f"ğŸ¤– í´ëŸ¬ìŠ¤í„° {cluster_summary.get('cluster_id')} í˜ë¥´ì†Œë‚˜ ìƒì„± ì¤‘...")
            response = await self.call_bedrock_async(prompt, max_tokens=3000)

            # JSON ì‘ë‹µ íŒŒì‹±
            try:
                # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (ì‘ë‹µì— ë‹¤ë¥¸ í…ìŠ¤íŠ¸ê°€ í¬í•¨ë  ìˆ˜ ìˆìŒ)
                json_start = response.find('{')
                json_end = response.rfind('}') + 1
                if json_start != -1 and json_end != -1:
                    json_text = response[json_start:json_end]
                    persona_data = json.loads(json_text)
                else:
                    raise ValueError("JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")

            except (json.JSONDecodeError, ValueError) as e:
                logger.error(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                logger.debug(f"ì›ë³¸ ì‘ë‹µ: {response}")

                # í´ë°± í˜ë¥´ì†Œë‚˜ ìƒì„±
                persona_data = self.create_fallback_persona(cluster_summary)

            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            persona_data.update({
                "cluster_id": cluster_summary.get("cluster_id"),
                "confidence_score": 0.85,
                "generated_at": datetime.now().isoformat(),
                "generation_method": "bedrock_claude",
                "data_source": "seoul_single_household_telecom"
            })

            return persona_data

        except Exception as e:
            logger.error(f"âŒ í˜ë¥´ì†Œë‚˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return self.create_fallback_persona(cluster_summary)

    def create_fallback_persona(self, cluster_summary: Dict[str, Any]) -> Dict[str, Any]:
        """ì˜¤ë¥˜ ë°œìƒì‹œ ê¸°ë³¸ í˜ë¥´ì†Œë‚˜ ìƒì„±"""
        cluster_id = cluster_summary.get("cluster_id", 0)
        demographics = cluster_summary.get("demographics", {})

        fallback_names = ["ê¹€ë¯¼ìˆ˜", "ì´ì§€ì€", "ë°•ì¤€í˜¸", "ìµœì„œì—°", "ì •íƒœìš°"]
        districts = ["ê°•ë‚¨êµ¬", "ê´€ì•…êµ¬", "ì„±ë¶êµ¬", "ë§ˆí¬êµ¬", "ì†¡íŒŒêµ¬"]

        return {
            "name": fallback_names[cluster_id % len(fallback_names)],
            "age": int(demographics.get("avg_age", 35)),
            "gender": "ë‚¨ì„±" if cluster_id % 2 == 0 else "ì—¬ì„±",
            "district": districts[cluster_id % len(districts)],
            "occupation": "íšŒì‚¬ì›",
            "living_situation": "ì›ë£¸ ê±°ì£¼",
            "personality_traits": ["ë…ë¦½ì ", "ì‹¤ìš©ì ", "ì‹ ì¤‘í•¨"],
            "daily_routine": "ê·œì¹™ì ì¸ ì¶œí‡´ê·¼ê³¼ ì§‘ì—ì„œì˜ íœ´ì‹ ì‹œê°„ì„ ë³´ëƒ…ë‹ˆë‹¤.",
            "digital_habits": "ìŠ¤ë§ˆíŠ¸í°ê³¼ ì˜¨ë¼ì¸ ì„œë¹„ìŠ¤ë¥¼ ì ì ˆíˆ í™œìš©í•©ë‹ˆë‹¤.",
            "social_patterns": "ê°€ì¡±, ì¹œêµ¬ë“¤ê³¼ ì ì ˆí•œ ì—°ë½ì„ ìœ ì§€í•©ë‹ˆë‹¤.",
            "mobility_style": "ëŒ€ì¤‘êµí†µ ì£¼ë¡œ ì´ìš©",
            "consumption_habits": "í•„ìš”ì— ë”°ë¥¸ í•©ë¦¬ì  ì†Œë¹„",
            "challenges": ["ê²½ì œì  ë¶€ë‹´", "ì‚¬íšŒì  ê³ ë¦½", "ê±´ê°• ê´€ë¦¬"],
            "welfare_needs": ["ìƒê³„ì§€ì›", "ì˜ë£Œì§€ì›", "ì£¼ê±°ì§€ì›"],
            "goals": ["ì•ˆì •ì ì¸ ìƒí™œ", "ê±´ê°•í•œ ë¯¸ë˜"],
            "background_story": "ì„œìš¸ì—ì„œ ë…ë¦½ì ì¸ 1ì¸ê°€êµ¬ ìƒí™œì„ í•˜ê³  ìˆëŠ” í‰ë²”í•œ ì‹œë¯¼ì…ë‹ˆë‹¤.",
            "cluster_id": cluster_id,
            "confidence_score": 0.6,
            "generated_at": datetime.now().isoformat(),
            "generation_method": "fallback",
            "data_source": "cluster_summary"
        }

    async def generate_personas(self, n_personas: int = 5) -> List[Dict[str, Any]]:
        """ì—¬ëŸ¬ í˜ë¥´ì†Œë‚˜ ìƒì„±"""
        logger.info(f"ğŸ­ {n_personas}ê°œ í˜ë¥´ì†Œë‚˜ ìƒì„± ì‹œì‘...")

        try:
            # ë°ì´í„° ë¡œë“œ
            data = self.load_cluster_data()
            if data is None:
                logger.warning("âš ï¸ ë°ì´í„°ê°€ ì—†ì–´ ê¸°ë³¸ í˜ë¥´ì†Œë‚˜ ìƒì„±")
                return [self.create_fallback_persona({"cluster_id": i}) for i in range(n_personas)]

            # í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ (ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)
            try:
                if len(data) > 1000:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
                    # ì‹¤ì œ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
                    unit_cols = ["ìì¹˜êµ¬", "í–‰ì •ë™", "ì„±ë³„", "ì—°ë ¹ëŒ€"]
                    unit_cols = [col for col in unit_cols if col in data.columns]

                    if unit_cols:
                        labels_df, model_info = persona_clustering(
                            df=data.sample(min(10000, len(data))),  # ìƒ˜í”Œë§ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”
                            unit_cols=unit_cols,
                            K_list=[3, 4, 5, 6, 7][:n_personas+2]
                        )

                        # í´ëŸ¬ìŠ¤í„°ë³„ ìš”ì•½ ìƒì„±
                        cluster_summaries = []
                        for i in range(min(n_personas, model_info['K'])):
                            cluster_mask = labels_df['persona_id'] == i
                            cluster_data = data[data.index.isin(labels_df[cluster_mask].index)]

                            if len(cluster_data) > 0:
                                summary = self.generate_cluster_summary(cluster_data, i)
                                cluster_summaries.append(summary)
                    else:
                        raise ValueError("ì ì ˆí•œ ë‹¨ìœ„ ì»¬ëŸ¼ì´ ì—†ìŒ")

                else:
                    raise ValueError("ë°ì´í„°ê°€ ë¶€ì¡±í•¨")

            except Exception as e:
                logger.warning(f"âš ï¸ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨, ë°ì´í„° ê¸°ë°˜ ìš”ì•½ ìƒì„±: {e}")
                # ë°ì´í„°ë¥¼ ê· ë“± ë¶„í• í•˜ì—¬ ìš”ì•½ ìƒì„±
                chunk_size = len(data) // n_personas
                cluster_summaries = []

                for i in range(n_personas):
                    start_idx = i * chunk_size
                    end_idx = (i + 1) * chunk_size if i < n_personas - 1 else len(data)
                    chunk_data = data.iloc[start_idx:end_idx]

                    summary = self.generate_cluster_summary(chunk_data, i)
                    cluster_summaries.append(summary)

            # ë¹„ë™ê¸°ë¡œ í˜ë¥´ì†Œë‚˜ ìƒì„±
            tasks = []
            for summary in cluster_summaries:
                task = self.generate_single_persona(summary)
                tasks.append(task)

            personas = await asyncio.gather(*tasks, return_exceptions=True)

            # ê²°ê³¼ ì •ë¦¬ (ì˜ˆì™¸ ì²˜ë¦¬)
            valid_personas = []
            for i, persona in enumerate(personas):
                if isinstance(persona, Exception):
                    logger.error(f"âŒ í˜ë¥´ì†Œë‚˜ {i} ìƒì„± ì‹¤íŒ¨: {persona}")
                    valid_personas.append(self.create_fallback_persona({"cluster_id": i}))
                else:
                    valid_personas.append(persona)

            logger.info(f"âœ… {len(valid_personas)}ê°œ í˜ë¥´ì†Œë‚˜ ìƒì„± ì™„ë£Œ")
            return valid_personas

        except Exception as e:
            logger.error(f"âŒ í˜ë¥´ì†Œë‚˜ ìƒì„± í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
            # ì „ì²´ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ í˜ë¥´ì†Œë‚˜ë“¤ ë°˜í™˜
            return [self.create_fallback_persona({"cluster_id": i}) for i in range(n_personas)]

    def save_personas(self, personas: List[Dict[str, Any]], output_path: str):
        """ìƒì„±ëœ í˜ë¥´ì†Œë‚˜ ì €ì¥"""
        try:
            output_dir = Path(output_path).parent
            output_dir.mkdir(parents=True, exist_ok=True)

            personas_data = {
                "metadata": {
                    "generated_at": datetime.now().isoformat(),
                    "total_personas": len(personas),
                    "generator": "bedrock_claude_3_5_haiku",
                    "region": self.region_name,
                    "version": "1.0"
                },
                "personas": personas
            }

            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(personas_data, f, ensure_ascii=False, indent=2)

            logger.info(f"ğŸ’¾ í˜ë¥´ì†Œë‚˜ ì €ì¥ ì™„ë£Œ: {output_path}")

        except Exception as e:
            logger.error(f"âŒ í˜ë¥´ì†Œë‚˜ ì €ì¥ ì‹¤íŒ¨: {e}")

async def main():
    """CLI í…ŒìŠ¤íŠ¸"""
    import argparse

    parser = argparse.ArgumentParser(description="AWS Bedrock í˜ë¥´ì†Œë‚˜ ìƒì„±ê¸°")
    parser.add_argument("--personas", type=int, default=5, help="ìƒì„±í•  í˜ë¥´ì†Œë‚˜ ìˆ˜")
    parser.add_argument("--region", default="us-east-1", help="AWS ë¦¬ì „")
    parser.add_argument("--output", default="bedrock_personas.json", help="ì¶œë ¥ íŒŒì¼")

    args = parser.parse_args()

    # ìƒì„±ê¸° ì´ˆê¸°í™”
    generator = BedrockPersonaGenerator(region_name=args.region)

    try:
        # í˜ë¥´ì†Œë‚˜ ìƒì„±
        personas = await generator.generate_personas(args.personas)

        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ­ ìƒì„±ëœ í˜ë¥´ì†Œë‚˜ë“¤:")
        for i, persona in enumerate(personas):
            print(f"{i+1}. {persona.get('name', 'ì´ë¦„ì—†ìŒ')} "
                  f"({persona.get('age', '?')}ì„¸, {persona.get('district', 'ì§€ì—­ë¯¸ìƒ')})")
            print(f"   ì§ì—…: {persona.get('occupation', 'ë¯¸ìƒ')}")
            print(f"   ì£¼ìš” íŠ¹ì„±: {', '.join(persona.get('personality_traits', [])[:2])}")
            print()

        # íŒŒì¼ ì €ì¥
        generator.save_personas(personas, args.output)

        # í†µê³„
        print(f"ğŸ“Š ìƒì„± í†µê³„:")
        print(f"  - ì´ í˜ë¥´ì†Œë‚˜: {len(personas)}ê°œ")
        bedrock_count = sum(1 for p in personas if p.get('generation_method') == 'bedrock_claude')
        print(f"  - Bedrock ìƒì„±: {bedrock_count}ê°œ")
        print(f"  - í´ë°± ìƒì„±: {len(personas) - bedrock_count}ê°œ")

    except Exception as e:
        logger.error(f"âŒ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    asyncio.run(main())