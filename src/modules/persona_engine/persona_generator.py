"""
í†µí•© í˜ë¥´ì†Œë‚˜ ìƒì„± ì—”ì§„ - RDS/RAG ì‹œìŠ¤í…œ í†µí•©
í´ëŸ¬ìŠ¤í„°ë§ëœ ë°ì´í„°ì™€ ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©í•œ í˜„ì‹¤ì ì¸ í˜ë¥´ì†Œë‚˜ ìƒì„±
"""

import logging
import json
from typing import Dict, List, Optional, Any
from datetime import datetime

from ..common.database import db_manager
from ..data_analysis.enhanced_clustering import EnhancedClusteringEngine
from .rag_engine import RAGEngine

logger = logging.getLogger(__name__)


class PersonaGenerator:
    """RDS/RAG í†µí•© í˜ë¥´ì†Œë‚˜ ìƒì„± ì—”ì§„"""

    def __init__(self):
        self.clustering_engine = EnhancedClusteringEngine()
        self.rag_engine = RAGEngine()
        self.knowledge_uploaded = False

    def upload_knowledge_base(self, task_id: str, knowledge_file_paths: List[str]) -> Dict[str, Any]:
        """ë„ë©”ì¸ ì§€ì‹ íŒŒì¼ì„ S3ì— ì—…ë¡œë“œí•˜ê³  Knowledge Base ì¤€ë¹„"""
        try:
            logger.info(f"ì§€ì‹ ê¸°ë°˜ ì—…ë¡œë“œ ì‹œì‘: {task_id}")

            upload_result = self.rag_engine.upload_knowledge_base(task_id, knowledge_file_paths)

            if upload_result["success_count"] > 0:
                self.knowledge_uploaded = True
                logger.info("ì§€ì‹ ê¸°ë°˜ ì—…ë¡œë“œ ë° í™œì„±í™” ì™„ë£Œ")

            return upload_result

        except Exception as e:
            logger.error(f"ì§€ì‹ ê¸°ë°˜ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    def perform_clustering_analysis(
        self,
        task_id: str,
        table_name: str,
        dataset_metadata: Dict[str, Any]
    ) -> Dict[str, Any]:
        """RDS ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„"""
        try:
            logger.info(f"í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì‹œì‘: {task_id}")

            clustering_results = self.clustering_engine.perform_clustering(
                task_id, table_name, dataset_metadata
            )

            logger.info(f"í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {clustering_results['optimal_k']}ê°œ í´ëŸ¬ìŠ¤í„°")
            return clustering_results

        except Exception as e:
            logger.error(f"í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise

    def generate_personas_from_clusters(
        self,
        clustering_results: Dict[str, Any],
        scenario: str,
        domain: str,
        knowledge_summary: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í˜ë¥´ì†Œë‚˜ ìƒì„±"""
        try:
            logger.info("í˜ë¥´ì†Œë‚˜ ìƒì„± ì‹œì‘")
            personas = []

            cluster_analysis = clustering_results.get("cluster_analysis", {})

            for cluster_key, cluster_info in cluster_analysis.items():
                logger.info(f"í´ëŸ¬ìŠ¤í„° {cluster_info['cluster_id']} í˜ë¥´ì†Œë‚˜ ìƒì„± ì¤‘...")

                # RAGë¥¼ í™œìš©í•œ í˜ë¥´ì†Œë‚˜ ìƒì„±
                persona_result = self.rag_engine.generate_persona_with_context(
                    cluster_info, scenario, domain, knowledge_summary
                )

                # í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¶”ê°€
                persona_data = persona_result["persona_data"]
                persona_data.update({
                    "cluster_info": {
                        "cluster_id": cluster_info["cluster_id"],
                        "cluster_size": cluster_info["size"],
                        "cluster_percentage": cluster_info["percentage"]
                    },
                    "generation_metadata": {
                        "confidence_score": persona_result["confidence_score"],
                        "has_domain_knowledge": persona_result["has_domain_knowledge"],
                        "rag_context_length": len(persona_result["rag_context"]),
                        "source_documents_count": len(persona_result["source_documents"]),
                        "scenario": scenario,
                        "domain": domain,
                        "generated_at": datetime.now().isoformat()
                    }
                })

                personas.append(persona_data)
                logger.info(f"í˜ë¥´ì†Œë‚˜ '{persona_data['name']}' ìƒì„± ì™„ë£Œ")

            logger.info(f"ì´ {len(personas)}ê°œ í˜ë¥´ì†Œë‚˜ ìƒì„± ì™„ë£Œ")
            return personas

        except Exception as e:
            logger.error(f"í˜ë¥´ì†Œë‚˜ ìƒì„± ì‹¤íŒ¨: {e}")
            raise

    def generate_complete_personas(
        self,
        task_id: str,
        table_name: str,
        dataset_metadata: Dict[str, Any],
        knowledge_file_paths: List[str],
        scenario: str = "normal",
        domain: str = "general"
    ) -> Dict[str, Any]:
        """ì „ì²´ í˜ë¥´ì†Œë‚˜ ìƒì„± íŒŒì´í”„ë¼ì¸"""
        try:
            logger.info(f"í†µí•© í˜ë¥´ì†Œë‚˜ ìƒì„± íŒŒì´í”„ë¼ì¸ ì‹œì‘: {task_id}")

            # 1. ë„ë©”ì¸ ì§€ì‹ ì—…ë¡œë“œ
            knowledge_result = self.upload_knowledge_base(task_id, knowledge_file_paths)

            # 2. í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
            clustering_results = self.perform_clustering_analysis(
                task_id, table_name, dataset_metadata
            )

            # 3. í˜ë¥´ì†Œë‚˜ ìƒì„±
            personas = self.generate_personas_from_clusters(
                clustering_results,
                scenario,
                domain,
                knowledge_result["knowledge_summary"]
            )

            # 4. ê²°ê³¼ í†µí•©
            complete_result = {
                "task_id": task_id,
                "status": "completed",
                "generated_at": datetime.now().isoformat(),

                "clustering_info": {
                    "optimal_clusters": clustering_results["optimal_k"],
                    "total_samples": clustering_results["total_samples"],
                    "feature_count": clustering_results["feature_info"]["feature_count"]
                },

                "knowledge_base_info": {
                    "files_processed": knowledge_result["knowledge_summary"].get("files_processed", 0),
                    "total_content_length": knowledge_result["knowledge_summary"].get("total_content_length", 0),
                    "upload_success_count": knowledge_result["success_count"]
                },

                "personas": personas,
                "personas_count": len(personas),

                "generation_config": {
                    "scenario": scenario,
                    "domain": domain
                }
            }

            logger.info(f"í†µí•© í˜ë¥´ì†Œë‚˜ ìƒì„± ì™„ë£Œ: {len(personas)}ê°œ í˜ë¥´ì†Œë‚˜")
            return complete_result

        except Exception as e:
            logger.error(f"í†µí•© í˜ë¥´ì†Œë‚˜ ìƒì„± ì‹¤íŒ¨: {e}")
            raise

    def save_personas_to_json(self, personas_result: Dict[str, Any], output_path: str):
        """ìƒì„±ëœ í˜ë¥´ì†Œë‚˜ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(personas_result, f, ensure_ascii=False, indent=2)

            logger.info(f"í˜ë¥´ì†Œë‚˜ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")

        except Exception as e:
            logger.error(f"í˜ë¥´ì†Œë‚˜ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

    def get_persona_summary(self, personas_result: Dict[str, Any]) -> str:
        """í˜ë¥´ì†Œë‚˜ ìƒì„± ê²°ê³¼ ìš”ì•½"""
        try:
            summary_lines = [
                f"ğŸ“Š í˜ë¥´ì†Œë‚˜ ìƒì„± ê²°ê³¼ ìš”ì•½ - {personas_result['task_id']}",
                f"ìƒì„± ì‹œê°: {personas_result['generated_at']}",
                "",
                f"ğŸ” í´ëŸ¬ìŠ¤í„°ë§ ì •ë³´:",
                f"  - ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜: {personas_result['clustering_info']['optimal_clusters']}ê°œ",
                f"  - ë¶„ì„ ìƒ˜í”Œ ìˆ˜: {personas_result['clustering_info']['total_samples']}ê°œ",
                f"  - ì‚¬ìš© íŠ¹ì„± ìˆ˜: {personas_result['clustering_info']['feature_count']}ê°œ",
                "",
                f"ğŸ“š ì§€ì‹ ê¸°ë°˜ ì •ë³´:",
                f"  - ì²˜ë¦¬ëœ íŒŒì¼ ìˆ˜: {personas_result['knowledge_base_info']['files_processed']}ê°œ",
                f"  - ì´ ì½˜í…ì¸  ê¸¸ì´: {personas_result['knowledge_base_info']['total_content_length']:,} ë¬¸ì",
                "",
                f"ğŸ‘¥ ìƒì„±ëœ í˜ë¥´ì†Œë‚˜ ({personas_result['personas_count']}ê°œ):"
            ]

            for i, persona in enumerate(personas_result['personas'], 1):
                cluster_info = persona['cluster_info']
                generation_meta = persona['generation_metadata']

                summary_lines.extend([
                    f"  {i}. {persona['name']} ({persona['age']}ì„¸, {persona['gender']})",
                    f"     ì§ì—…: {persona['occupation']}, ê±°ì£¼ì§€: {persona['location']}",
                    f"     í´ëŸ¬ìŠ¤í„°: {cluster_info['cluster_id']} (í¬ê¸°: {cluster_info['cluster_size']}ëª…, {cluster_info['cluster_percentage']:.1f}%)",
                    f"     ì‹ ë¢°ë„: {generation_meta['confidence_score']:.2f}",
                    f"     ì£¼ìš” ë‹ˆì¦ˆ: {', '.join(persona['needs'][:3])}",
                    ""
                ])

            return "\n".join(summary_lines)

        except Exception as e:
            logger.error(f"í˜ë¥´ì†Œë‚˜ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}"


# í¸ì˜ë¥¼ ìœ„í•œ ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
persona_generator = PersonaGenerator()