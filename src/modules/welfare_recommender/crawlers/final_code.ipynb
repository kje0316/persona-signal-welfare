{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f1198b",
   "metadata": {},
   "source": [
    "# í¬ë¡¤ë§ì‘ì—…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6162d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas requests lxml tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eaa6db",
   "metadata": {},
   "source": [
    "## ì¤‘ì•™ë¶€ì²˜ - í¬ë¡¤ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7c9021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import ssl\n",
    "import time\n",
    "import os\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "# ==========================\n",
    "# ì‚¬ìš©ì ì…ë ¥\n",
    "# ==========================\n",
    "API_KEY = \"0f561bd1f21a64f960fff4cf634d3afeb996d252a52dae3c7fea3c70c2c9ba09\"   # â† ë°˜ë“œì‹œ ë³¸ì¸ API Keyë¡œ êµì²´\n",
    "MASTER_CSV = \"ì¤‘ì•™ë¶€ì²˜_ë³µì§€ì„œë¹„ìŠ¤.csv\"     # ì „ì²´ ì„œë¹„ìŠ¤ID ëª©ë¡\n",
    "PROCESSED_CSV = \"ì¤‘ì•™ë¶€ì²˜_batch4.csv\"    # ì´ë¯¸ ì²˜ë¦¬ëœ ê²°ê³¼ CSV\n",
    "SERVICE_ID_COLUMN = \"ì„œë¹„ìŠ¤ID\"\n",
    "BASE_URL = \"https://apis.data.go.kr/B554287/NationalWelfareInformationsV001/NationalWelfaredetailedV001\"\n",
    "\n",
    "# ==========================\n",
    "# TLS ê°•ì œ\n",
    "# ==========================\n",
    "class TLSAdapter(HTTPAdapter):\n",
    "    def init_poolmanager(self, *args, **kwargs):\n",
    "        ctx = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)\n",
    "        ctx.minimum_version = ssl.TLSVersion.TLSv1_2\n",
    "        kwargs['ssl_context'] = ctx\n",
    "        return super().init_poolmanager(*args, **kwargs)\n",
    "\n",
    "session = requests.Session()\n",
    "session.mount(\"https://\", TLSAdapter())\n",
    "\n",
    "# ==========================\n",
    "# XML íŒŒì‹±\n",
    "# ==========================\n",
    "def parse_service_detail(xml_text):\n",
    "    try:\n",
    "        root = ET.fromstring(xml_text)\n",
    "        node = root if root.tag == \"wantedDtl\" else root.find(\".//wantedDtl\")\n",
    "        if node is None:\n",
    "            return None\n",
    "        row = {\n",
    "            \"ì„œë¹„ìŠ¤ID\": node.findtext(\"servId\", \"\"),\n",
    "            \"ì„œë¹„ìŠ¤ëª…\": node.findtext(\"servNm\", \"\"),\n",
    "            \"ì†Œê´€ë¶€ì²˜\": node.findtext(\"jurMnofNm\", \"\"),\n",
    "            \"ì„œë¹„ìŠ¤ê°œìš”\": node.findtext(\"wlfareInfoOutlCn\", \"\"),\n",
    "            \"ì§€ì›ëŒ€ìƒìƒì„¸\": node.findtext(\"tgtrDtlCn\", \"\").strip(),\n",
    "            \"ì„ ì •ê¸°ì¤€\": node.findtext(\"slctCritCn\", \"\").strip(),\n",
    "            \"ì§€ì›ë‚´ìš©\": node.findtext(\"alwServCn\", \"\").strip(),\n",
    "            \"ì§€ì›ì£¼ê¸°\": node.findtext(\"sprtCycNm\", \"\"),\n",
    "            \"ì§€ê¸‰ë°©ì‹\": node.findtext(\"srvPvsnNm\", \"\"),\n",
    "        }\n",
    "        return row\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# ==========================\n",
    "# API í˜¸ì¶œ\n",
    "# ==========================\n",
    "def fetch_service_detail(sid):\n",
    "    params = {\"serviceKey\": API_KEY, \"servId\": sid}\n",
    "    try:\n",
    "        resp = session.get(BASE_URL, params=params, timeout=30)\n",
    "        if resp.status_code == 200:\n",
    "            return resp.text\n",
    "        else:\n",
    "            print(f\"âŒ ìš”ì²­ ì‹¤íŒ¨: {sid}, ì½”ë“œ {resp.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ìš”ì²­ ì˜¤ë¥˜: {sid}, {e}\")\n",
    "        return None\n",
    "\n",
    "# ==========================\n",
    "# ì‹¤í–‰ ë£¨í”„\n",
    "# ==========================\n",
    "def run_batch(service_ids, batch_num=1, batch_size=100):\n",
    "    rows, fail_ids = [], []\n",
    "    last_id = None\n",
    "\n",
    "    for idx, sid in enumerate(service_ids, start=1):\n",
    "        print(f\"[{idx}/{len(service_ids)}] ìš”ì²­: {sid}\")\n",
    "        xml_text = fetch_service_detail(sid)\n",
    "\n",
    "        if not xml_text:\n",
    "            fail_ids.append(sid)\n",
    "            continue\n",
    "\n",
    "        data = parse_service_detail(xml_text)\n",
    "        if data:\n",
    "            rows.append(data)\n",
    "            last_id = sid\n",
    "        else:\n",
    "            fail_ids.append(sid)\n",
    "\n",
    "        time.sleep(0.5)  # API ë¶€í•˜ ë°©ì§€\n",
    "\n",
    "        # 100ê°œ ë‹¨ìœ„ ì €ì¥\n",
    "        if idx % batch_size == 0:\n",
    "            file_name = f\"ì¤‘ì•™ë¶€ì²˜_batch{batch_num}.csv\"\n",
    "            pd.DataFrame(rows).to_csv(file_name, index=False, encoding=\"utf-8-sig\")\n",
    "            print(f\"ğŸ’¾ Batch {batch_num} ì €ì¥ ì™„ë£Œ ({len(rows)}ê°œ)\")\n",
    "            rows = []\n",
    "            batch_num += 1\n",
    "\n",
    "    # ë‚¨ì€ ë°ì´í„° ì €ì¥\n",
    "    if rows:\n",
    "        file_name = f\"ì¤‘ì•™ë¶€ì²˜_batch{batch_num}.csv\"\n",
    "        pd.DataFrame(rows).to_csv(file_name, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"ğŸ’¾ Batch {batch_num} ì €ì¥ ì™„ë£Œ ({len(rows)}ê°œ)\")\n",
    "\n",
    "    # ì‹¤íŒ¨ ID ì €ì¥\n",
    "    if fail_ids:\n",
    "        fail_file = f\"ì¤‘ì•™ë¶€ì²˜_batch{batch_num}_ì‹¤íŒ¨.csv\"\n",
    "        pd.DataFrame(fail_ids, columns=[\"ì‹¤íŒ¨ID\"]).to_csv(fail_file, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"âš ï¸ ì‹¤íŒ¨í•œ ID {len(fail_ids)}ê±´ ì €ì¥ë¨ â†’ {fail_file}\")\n",
    "\n",
    "    print(f\"âœ… ë§ˆì§€ë§‰ ì²˜ë¦¬ëœ ì„œë¹„ìŠ¤ID: {last_id}\")\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# ì‹œì‘ index ê³„ì‚°\n",
    "# ==========================\n",
    "def get_remaining_ids(master_csv, processed_csv):\n",
    "    df_master = pd.read_csv(master_csv)\n",
    "    df_processed = pd.read_csv(processed_csv)\n",
    "\n",
    "    all_ids = df_master[SERVICE_ID_COLUMN].dropna().astype(str).tolist()\n",
    "    done_ids = df_processed[SERVICE_ID_COLUMN].dropna().astype(str).tolist()\n",
    "\n",
    "    remaining = [sid for sid in all_ids if sid not in done_ids]\n",
    "    print(f\"ğŸ“Š ì „ì²´ {len(all_ids)}ê°œ ì¤‘ {len(done_ids)}ê°œ ì™„ë£Œ, {len(remaining)}ê°œ ë‚¨ìŒ\")\n",
    "    return remaining\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# ì‹¤í–‰\n",
    "# ==========================\n",
    "if __name__ == \"__main__\":\n",
    "    remaining_ids = get_remaining_ids(MASTER_CSV, PROCESSED_CSV)\n",
    "\n",
    "    if remaining_ids:\n",
    "        run_batch(remaining_ids, batch_num=3, batch_size=100)  \n",
    "        # ğŸ‘‰ batch_num=3ë¶€í„° ì‹œì‘ (1,2ëŠ” ì´ë¯¸ ì™„ë£Œí–ˆìœ¼ë‹ˆê¹Œ)\n",
    "    else:\n",
    "        print(\"ğŸ‰ ëª¨ë“  ì„œë¹„ìŠ¤ID í¬ë¡¤ë§ ì™„ë£Œë¨\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd40a29",
   "metadata": {},
   "source": [
    "ì¤‘ì•™ë¶€ì²˜ ì‹¤íŒ¨ id í•©ë³¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e730372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# --- ì„¤ì • ---\n",
    "\n",
    "# 1. ì°¾ì„ ì‹¤íŒ¨ íŒŒì¼ë“¤ì˜ íŒ¨í„´ì…ë‹ˆë‹¤. \n",
    "#    ì´ íŒ¨í„´ì— ë§ëŠ” ëª¨ë“  íŒŒì¼ì„ ì°¾ì•„ëƒ…ë‹ˆë‹¤. (ì˜ˆ: ì¤‘ì•™ë¶€ì²˜_batch4_ì‹¤íŒ¨.csv, ì¤‘ì•™ë¶€ì²˜_ì¬ì‹œë„_ì¬ì‹¤íŒ¨.csv ë“±)\n",
    "FILE_PATTERN = \"*_ì‹¤íŒ¨.csv\"\n",
    "\n",
    "# 2. ìµœì¢…ì ìœ¼ë¡œ ì €ì¥ë  í†µí•© íŒŒì¼ì˜ ì´ë¦„ì…ë‹ˆë‹¤.\n",
    "OUTPUT_FILENAME = \"í†µí•©_ì‹¤íŒ¨_ID_ëª©ë¡.csv\"\n",
    "\n",
    "# 3. í†µí•© í›„ ì‚¬ìš©í•  ìµœì¢… ì„œë¹„ìŠ¤ ID ì»¬ëŸ¼ ì´ë¦„ì…ë‹ˆë‹¤.\n",
    "FINAL_ID_COLUMN_NAME = \"ì„œë¹„ìŠ¤ID\"\n",
    "\n",
    "# --- ì½”ë“œ ì‹¤í–‰ ---\n",
    "\n",
    "def merge_failure_files():\n",
    "    \"\"\"\n",
    "    ì§€ì •ëœ íŒ¨í„´ê³¼ ì¼ì¹˜í•˜ëŠ” ëª¨ë“  ì‹¤íŒ¨ CSV íŒŒì¼ì„ ì°¾ì•„,\n",
    "    í•˜ë‚˜ì˜ í†µí•©ëœ íŒŒì¼ë¡œ ë³‘í•©í•˜ê³  ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    # íŒ¨í„´ì— ë§ëŠ” íŒŒì¼ ëª©ë¡ì„ ì°¾ìŠµë‹ˆë‹¤.\n",
    "    failure_files = glob.glob(FILE_PATTERN)\n",
    "\n",
    "    if not failure_files:\n",
    "        print(f\"âŒ ì˜¤ë¥˜: '{FILE_PATTERN}' íŒ¨í„´ì— ë§ëŠ” ì‹¤íŒ¨ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        print(\"ìŠ¤í¬ë¦½íŠ¸ì™€ ì‹¤íŒ¨ íŒŒì¼ë“¤ì´ ê°™ì€ í´ë”ì— ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "        return\n",
    "\n",
    "    print(f\"ğŸ“ ì´ {len(failure_files)}ê°œì˜ ì‹¤íŒ¨ íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤:\")\n",
    "    for f in failure_files:\n",
    "        print(f\"  - {f}\")\n",
    "\n",
    "    df_list = []\n",
    "    # ê° íŒŒì¼ì„ ì½ì–´ ë°ì´í„°í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "    for file in failure_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            # ì»¬ëŸ¼ ì´ë¦„ì´ í†µì¼ë˜ë„ë¡ ì²˜ë¦¬\n",
    "            # ì˜ˆ: 'ì‹¤íŒ¨ID', 'ì¬ì‹¤íŒ¨ID' -> 'ì„œë¹„ìŠ¤ID'\n",
    "            if 'ì‹¤íŒ¨ID' in df.columns:\n",
    "                df.rename(columns={'ì‹¤íŒ¨ID': FINAL_ID_COLUMN_NAME}, inplace=True)\n",
    "            elif 'ì¬ì‹¤íŒ¨ID' in df.columns:\n",
    "                df.rename(columns={'ì¬ì‹¤íŒ¨ID': FINAL_ID_COLUMN_NAME}, inplace=True)\n",
    "            \n",
    "            df_list.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ê²½ê³ : '{file}' íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ. ê±´ë„ˆëœë‹ˆë‹¤. ({e})\")\n",
    "\n",
    "    if not df_list:\n",
    "        print(\"\\nâŒ ì˜¤ë¥˜: íŒŒì¼ì„ ì½ì–´ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    # ëª¨ë“  ë°ì´í„°í”„ë ˆì„ì„ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤.\n",
    "    print(\"\\níŒŒì¼ ë³‘í•© ë° ì¤‘ë³µ ì œê±°ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # ìµœì¢… ID ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤.\n",
    "    if FINAL_ID_COLUMN_NAME in combined_df.columns:\n",
    "        initial_rows = len(combined_df)\n",
    "        # ì¤‘ë³µëœ IDë¥¼ ì œê±°í•˜ê³ , ìµœì¢…ì ìœ¼ë¡œ ìœ ë‹ˆí¬í•œ ID ëª©ë¡ë§Œ ë‚¨ê¹ë‹ˆë‹¤.\n",
    "        combined_df.drop_duplicates(subset=[FINAL_ID_COLUMN_NAME], keep='first', inplace=True)\n",
    "        final_rows = len(combined_df)\n",
    "        print(f\"  - ì´ {initial_rows}ê°œì˜ IDë¥¼ {final_rows}ê°œì˜ ê³ ìœ  IDë¡œ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ ê²½ê³ : '{FINAL_ID_COLUMN_NAME}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì¤‘ë³µ ì œê±°ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "        \n",
    "    # ìµœì¢… ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    try:\n",
    "        combined_df.to_csv(OUTPUT_FILENAME, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nâœ… ì„±ê³µ! '{OUTPUT_FILENAME}' íŒŒì¼ë¡œ ëª¨ë“  ì‹¤íŒ¨ IDë¥¼ í†µí•© ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ì˜¤ë¥˜: ìµœì¢… íŒŒì¼ì„ ì €ì¥í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n",
    "\n",
    "\n",
    "# ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    merge_failure_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1586bd",
   "metadata": {},
   "source": [
    "ì¤‘ì•™ë¶€ì²˜ ì‹¤íŒ¨id ì¬í¬ë¡¤ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f9c4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import ssl\n",
    "import time\n",
    "from requests.adapters import HTTPAdapter\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- ğŸ’¡ 1. ì—¬ê¸°ì— ëª¨ë“  API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš” ---\n",
    "API_KEYS = [\n",
    "    \"0f561bd1f21a64f960fff4cf634d3afeb996d252a52dae3c7fea3c70c2c9ba09\",\n",
    "    \"6c00ed313ec5456c58b8d55a6f5d12a65cd4984956e87be30c7a3ea22b8ca086\"\n",
    "    # ê°€ì§€ê³  ìˆëŠ” ëª¨ë“  í‚¤ë¥¼ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "]\n",
    "\n",
    "# --- ğŸ’¡ 2. ì…ë ¥ íŒŒì¼ê³¼ ê²°ê³¼ íŒŒì¼ ì´ë¦„ ì„¤ì • ---\n",
    "INPUT_FILE = \"í†µí•©_ì‹¤íŒ¨_ID_ëª©ë¡.csv\"\n",
    "ID_COLUMN = \"ì„œë¹„ìŠ¤ID\" # í†µí•© ì‹¤íŒ¨ íŒŒì¼ì˜ ID ì»¬ëŸ¼ëª…\n",
    "\n",
    "OUTPUT_SUCCESS_FILE = \"ìµœì¢…_ì¬ì‹œë„_ì„±ê³µ.csv\"\n",
    "OUTPUT_FAILURE_FILE = \"ìµœì¢…_ì¬ì‹œë„_ì‹¤íŒ¨.csv\"\n",
    "\n",
    "# --- API í˜¸ì¶œ ì„¤ì • (ìˆ˜ì • í•„ìš” ì—†ìŒ) ---\n",
    "BASE_URL = \"https://www.bokjiro.go.kr/ssis-tbu/TWAT52005M/twataa/wlfareInfo/selectWlfareInfo.do\"\n",
    "\n",
    "# (ì´ ì•„ë˜ TLSAdapter, session ì„¤ì •ì€ ê¸°ì¡´ê³¼ ë™ì¼í•©ë‹ˆë‹¤)\n",
    "class TLSAdapter(HTTPAdapter):\n",
    "    def init_poolmanager(self, *args, **kwargs):\n",
    "        ctx = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)\n",
    "        ctx.minimum_version = ssl.TLSVersion.TLSv1_2\n",
    "        kwargs['ssl_context'] = ctx\n",
    "        return super().init_poolmanager(*args, **kwargs)\n",
    "\n",
    "session = requests.Session()\n",
    "session.mount(\"https://\", TLSAdapter())\n",
    "\n",
    "# (XML íŒŒì‹± í•¨ìˆ˜ëŠ” ê¸°ì¡´ê³¼ ë™ì¼í•©ë‹ˆë‹¤)\n",
    "def parse_service_detail(xml_text):\n",
    "    try:\n",
    "        root = ET.fromstring(xml_text)\n",
    "        node = root if root.tag == \"wantedDtl\" else root.find(\".//wantedDtl\")\n",
    "        if node is None:\n",
    "            return None\n",
    "        return {\n",
    "            \"ì„œë¹„ìŠ¤ID\": node.findtext(\"servId\", \"\"), \"ì„œë¹„ìŠ¤ëª…\": node.findtext(\"servNm\", \"\"),\n",
    "            \"ì†Œê´€ë¶€ì²˜\": node.findtext(\"jurMnofNm\", \"\"), \"ì„œë¹„ìŠ¤ê°œìš”\": node.findtext(\"wlfareInfoOutlCn\", \"\"),\n",
    "            \"ì§€ì›ëŒ€ìƒìƒì„¸\": node.findtext(\"tgtrDtlCn\", \"\").strip(), \"ì„ ì •ê¸°ì¤€\": node.findtext(\"slctCritCn\", \"\").strip(),\n",
    "            \"ì§€ì›ë‚´ìš©\": node.findtext(\"alwServCn\", \"\").strip(), \"ì§€ì›ì£¼ê¸°\": node.findtext(\"sprtCycNm\", \"\"),\n",
    "            \"ì§€ê¸‰ë°©ì‹\": node.findtext(\"srvPvsnNm\", \"\"),\n",
    "        }\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def fetch_service_detail(sid, api_key):\n",
    "    \"\"\"ì§€ì •ëœ API í‚¤ë¡œ ì„œë¹„ìŠ¤ ìƒì„¸ ì •ë³´ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤.\"\"\"\n",
    "    params = {\"serviceKey\": api_key, \"servId\": sid}\n",
    "    try:\n",
    "        resp = session.get(BASE_URL, params=params, timeout=30)\n",
    "        # API í•œë„ ì´ˆê³¼ ì—ëŸ¬ëŠ” ë³´í†µ íŠ¹ì • ë¬¸ìì—´ì„ í¬í•¨í•œ ì‘ë‹µì„ ì¤ë‹ˆë‹¤.\n",
    "        # ì‹¤ì œ ì‘ë‹µì— ë”°ë¼ 'LIMIT_EXCEEDED' ë˜ëŠ” ë‹¤ë¥¸ ë¬¸ìì—´ë¡œ ë³€ê²½í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "        if \"SERVICE KEY IS NOT REGISTERED\" in resp.text or \"LIMITED\" in resp.text:\n",
    "             return \"LIMIT_EXCEEDED\"\n",
    "        return resp.text if resp.status_code == 200 else None\n",
    "    except requests.exceptions.RequestException:\n",
    "        return None\n",
    "\n",
    "# --- ìµœì¢… ì‹¤í–‰ ë¡œì§ ---\n",
    "def run_final_retry():\n",
    "    # 1. ì¬ì‹œë„í•  ID ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_FILE)\n",
    "        service_ids = df[ID_COLUMN].dropna().astype(str).tolist()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ ì…ë ¥ íŒŒì¼ '{INPUT_FILE}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ í´ë”ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "        return\n",
    "\n",
    "    # 2. ì‹¤í–‰ ì‹œì‘\n",
    "    print(f\"âœ… ì´ {len(service_ids)}ê°œì˜ ì‹¤íŒ¨ IDì— ëŒ€í•œ ìµœì¢… ì¬ì‹œë„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "    print(f\"ğŸ”‘ ì‚¬ìš© ê°€ëŠ¥í•œ API í‚¤: {len(API_KEYS)}ê°œ\")\n",
    "\n",
    "    successful_rows = []\n",
    "    failed_ids = []\n",
    "    \n",
    "    current_key_index = 0\n",
    "    pbar = tqdm(total=len(service_ids), desc=\"ID ì²˜ë¦¬ ì¤‘\")\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(service_ids):\n",
    "        sid = service_ids[i]\n",
    "        \n",
    "        if current_key_index >= len(API_KEYS):\n",
    "            print(\"\\nâš ï¸ ëª¨ë“  API í‚¤ì˜ ì‚¬ìš© í•œë„ì— ë„ë‹¬í–ˆê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.\")\n",
    "            failed_ids.extend(service_ids[i:]) # ë‚¨ì€ IDëŠ” ëª¨ë‘ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬\n",
    "            pbar.update(len(service_ids) - i)\n",
    "            break\n",
    "\n",
    "        api_key = API_KEYS[current_key_index]\n",
    "        xml_text = fetch_service_detail(sid, api_key)\n",
    "\n",
    "        if xml_text == \"LIMIT_EXCEEDED\":\n",
    "            print(f\"\\nğŸ”‘ API í‚¤ {current_key_index + 1}ì˜ í•œë„ì— ë„ë‹¬. ë‹¤ìŒ í‚¤ë¡œ ì „í™˜í•©ë‹ˆë‹¤.\")\n",
    "            current_key_index += 1\n",
    "            continue  # í˜„ì¬ ID(sid)ë¥¼ ë‹¤ìŒ í‚¤ë¡œ ë‹¤ì‹œ ì‹œë„í•˜ê¸° ìœ„í•´ ië¥¼ ì¦ê°€ì‹œí‚¤ì§€ ì•ŠìŒ\n",
    "\n",
    "        elif xml_text is None:\n",
    "            failed_ids.append(sid)\n",
    "            pbar.set_postfix_str(f\"{sid} ìš”ì²­ ì‹¤íŒ¨\")\n",
    "            i += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "        else:\n",
    "            data = parse_service_detail(xml_text)\n",
    "            if data:\n",
    "                successful_rows.append(data)\n",
    "            else:\n",
    "                failed_ids.append(sid)\n",
    "            pbar.set_postfix_str(f\"{sid} ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "            i += 1\n",
    "            pbar.update(1)\n",
    "            \n",
    "        time.sleep(0.3) # ì„œë²„ ë¶€í•˜ ê°ì†Œ\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # 3. ê²°ê³¼ ì €ì¥\n",
    "    if successful_rows:\n",
    "        pd.DataFrame(successful_rows).to_csv(OUTPUT_SUCCESS_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"\\nğŸ’¾ ì¬ì‹œë„ ì„±ê³µ: {len(successful_rows)}ê±´ì„ '{OUTPUT_SUCCESS_FILE}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    if failed_ids:\n",
    "        pd.DataFrame(failed_ids, columns=[\"ì¬ì‹¤íŒ¨ID\"]).to_csv(OUTPUT_FAILURE_FILE, index=False)\n",
    "        print(f\"ğŸ’¾ ì¬ì‹œë„ ì‹¤íŒ¨: {len(failed_ids)}ê±´ì„ '{OUTPUT_FAILURE_FILE}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "    print(\"\\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_final_retry()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79988cd5",
   "metadata": {},
   "source": [
    "ì¤‘ì•™ë¶€ì²˜ csvíŒŒì¼ í•©ì¹˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2244b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "files = ['ì¤‘ì•™ë¶€ì²˜_1.csv', 'ì¤‘ì•™ë¶€ì²˜_2.csv']\n",
    "\n",
    "dfs = [pd.read_csv(f) for f in files]\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "merged_df.to_csv(\"ì¤‘ì•™ë¶€ì²˜_1,2í•©ë³¸.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"âœ… í†µí•© ì™„ë£Œ â†’ ì¤‘ì•™ë¶€ì²˜_1,2í•©ë³¸.csv\")\n",
    "print(f\"ì´ í–‰ ê°œìˆ˜: {len(merged_df)}\")\n",
    "print(f\"ì»¬ëŸ¼: {list(merged_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe8c066",
   "metadata": {},
   "source": [
    "ì¤‘ì•™ë¶€ì²˜ í•©ë³¸ -2ì°¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b7702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- ì„¤ì • ---\n",
    "\n",
    "# 1. ê¸°ì¡´ì— ì‘ì—…í–ˆë˜ í•©ë³¸ íŒŒì¼ ì´ë¦„\n",
    "MAIN_CSV_FILE = \"ì¤‘ì•™ë¶€ì²˜_1,2í•©ë³¸.csv\"\n",
    "\n",
    "# 2. í´ë”ì—ì„œ ì°¾ì„ XML íŒŒì¼ íŒ¨í„´\n",
    "XML_PATTERN = \"*.xml\"\n",
    "\n",
    "# 3. ìµœì¢…ì ìœ¼ë¡œ ëª¨ë“  ë°ì´í„°ê°€ í•©ì³ì ¸ ì €ì¥ë  íŒŒì¼ ì´ë¦„\n",
    "FINAL_OUTPUT_FILE = \"ìµœì¢…_ì¤‘ì•™ë¶€ì²˜_í†µí•©ë³¸.csv\"\n",
    "\n",
    "# --- XML íŒŒì‹± í•¨ìˆ˜ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼) ---\n",
    "def parse_service_detail(xml_text):\n",
    "    \"\"\"XML í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ë°›ì•„ íŒŒì‹±í•˜ê³ , ë°ì´í„° ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    try:\n",
    "        root = ET.fromstring(xml_text)\n",
    "        node = root if root.tag == \"wantedDtl\" else root.find(\".//wantedDtl\")\n",
    "        if node is None:\n",
    "            return None\n",
    "        return {\n",
    "            \"ì„œë¹„ìŠ¤ID\": node.findtext(\"servId\", \"\"),\n",
    "            \"ì„œë¹„ìŠ¤ëª…\": node.findtext(\"servNm\", \"\"),\n",
    "            \"ì†Œê´€ë¶€ì²˜\": node.findtext(\"jurMnofNm\", \"\"),\n",
    "            \"ì„œë¹„ìŠ¤ê°œìš”\": node.findtext(\"wlfareInfoOutlCn\", \"\"),\n",
    "            \"ì§€ì›ëŒ€ìƒìƒì„¸\": node.findtext(\"tgtrDtlCn\", \"\").strip(),\n",
    "            \"ì„ ì •ê¸°ì¤€\": node.findtext(\"slctCritCn\", \"\").strip(),\n",
    "            \"ì§€ì›ë‚´ìš©\": node.findtext(\"alwServCn\", \"\").strip(),\n",
    "            \"ì§€ì›ì£¼ê¸°\": node.findtext(\"sprtCycNm\", \"\"),\n",
    "            \"ì§€ê¸‰ë°©ì‹\": node.findtext(\"srvPvsnNm\", \"\"),\n",
    "        }\n",
    "    except ET.ParseError:\n",
    "        # XML í˜•ì‹ì´ ì˜ëª»ëœ ê²½ìš°\n",
    "        return None\n",
    "\n",
    "# --- ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---\n",
    "def process_and_merge_files():\n",
    "    # 1. XML íŒŒì¼ ëª©ë¡ ì°¾ê¸°\n",
    "    xml_files = glob.glob(XML_PATTERN)\n",
    "    if not xml_files:\n",
    "        print(f\"âŒ ì˜¤ë¥˜: í´ë”ì—ì„œ '{XML_PATTERN}' íŒ¨í„´ì— ë§ëŠ” XML íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    print(f\"ğŸ“ ì´ {len(xml_files)}ê°œì˜ XML íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. íŒŒì‹±ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "\n",
    "    # 2. ê° XML íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "    recovered_data = []\n",
    "    for file_path in tqdm(xml_files, desc=\"XML íŒŒì¼ ì²˜ë¦¬ ì¤‘\"):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                # íŒŒì¼ ë‚´ìš©ì´ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ íŒŒì‹± ì‹œë„\n",
    "                if content.strip():\n",
    "                    data = parse_service_detail(content)\n",
    "                    if data:\n",
    "                        recovered_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ê²½ê³ : '{file_path}' íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ. ê±´ë„ˆëœë‹ˆë‹¤. ({e})\")\n",
    "            \n",
    "    if not recovered_data:\n",
    "        print(\"âŒ ì˜¤ë¥˜: XML íŒŒì¼ì—ì„œ ìœ íš¨í•œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "        \n",
    "    recovered_df = pd.DataFrame(recovered_data)\n",
    "    print(f\"âœ… {len(recovered_df)}ê°œì˜ ì„œë¹„ìŠ¤ ë°ì´í„°ë¥¼ XML íŒŒì¼ë¡œë¶€í„° ì„±ê³µì ìœ¼ë¡œ ë³µêµ¬í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    # 3. ê¸°ì¡´ í•©ë³¸ CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    try:\n",
    "        print(f\"\\nê¸°ì¡´ í•©ë³¸ íŒŒì¼ '{MAIN_CSV_FILE}'ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤...\")\n",
    "        main_df = pd.read_csv(MAIN_CSV_FILE)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ ì˜¤ë¥˜: '{MAIN_CSV_FILE}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì´ ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ í´ë”ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "        return\n",
    "\n",
    "    # 4. ë³µêµ¬ëœ ë°ì´í„°ì™€ ê¸°ì¡´ ë°ì´í„° ë³‘í•©\n",
    "    print(\"ë³µêµ¬ëœ ë°ì´í„°ì™€ ê¸°ì¡´ ë°ì´í„°ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤...\")\n",
    "    final_df = pd.concat([main_df, recovered_df], ignore_index=True)\n",
    "    \n",
    "    initial_rows = len(final_df)\n",
    "    print(f\"  - ë³‘í•© í›„ ì´ í–‰ ê°œìˆ˜: {initial_rows}ê°œ\")\n",
    "\n",
    "    # 5. ì„œë¹„ìŠ¤ID ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ë°ì´í„° ì œê±° (ë³µêµ¬ëœ ë°ì´í„°ê°€ ìš°ì„ ìˆœìœ„ë¥¼ ê°€ì§)\n",
    "    final_df.drop_duplicates(subset=['ì„œë¹„ìŠ¤ID'], keep='last', inplace=True)\n",
    "    final_rows = len(final_df)\n",
    "    print(f\"  - ì¤‘ë³µ ì œê±° í›„ ìµœì¢… í–‰ ê°œìˆ˜: {final_rows}ê°œ ({initial_rows - final_rows}ê°œ ì¤‘ë³µ ì œê±°)\")\n",
    "\n",
    "    # 6. ìµœì¢… ê²°ê³¼ë¥¼ ìƒˆ íŒŒì¼ë¡œ ì €ì¥\n",
    "    try:\n",
    "        final_df.to_csv(FINAL_OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nğŸ‰ ì„±ê³µ! ìµœì¢… í†µí•©ëœ ë°ì´í„°ë¥¼ '{FINAL_OUTPUT_FILE}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ì˜¤ë¥˜: ìµœì¢… íŒŒì¼ì„ ì €ì¥í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n",
    "\n",
    "# --- ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ---\n",
    "if __name__ == \"__main__\":\n",
    "    process_and_merge_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769acd72",
   "metadata": {},
   "source": [
    "ì¤‘ì•™ë¶€ì²˜ í•©ë³¸-3ì°¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f76521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "files = ['ì¤‘ì•™ë¶€ì²˜_1.csv', 'ì¤‘ì•™ë¶€ì²˜_2.csv']\n",
    "\n",
    "dfs = [pd.read_csv(f) for f in files]\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "merged_df.to_csv(\"ì¤‘ì•™ë¶€ì²˜_1,2í•©ë³¸.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"âœ… í†µí•© ì™„ë£Œ â†’ ì¤‘ì•™ë¶€ì²˜_1,2í•©ë³¸.csv\")\n",
    "print(f\"ì´ í–‰ ê°œìˆ˜: {len(merged_df)}\")\n",
    "print(f\"ì»¬ëŸ¼: {list(merged_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b950179",
   "metadata": {},
   "source": [
    "## ì§€ìì²´ íŒŒì¼ í¬ë¡¤ë§-ë³µì§€ë¡œì‚¬ì´íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6d8dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# ì…ë ¥ CSV íŒŒì¼\n",
    "MASTER_CSV = \"ì§€ìì²´_ë³µì§€ì„œë¹„ìŠ¤ë¦¬ìŠ¤íŠ¸_í†µí•©.csv\"\n",
    "SERVICE_ID_COLUMN = \"ì„œë¹„ìŠ¤ID\"\n",
    "\n",
    "# URL í¬ë§·\n",
    "URL_TEMPLATE = \"https://www.bokjiro.go.kr/ssis-tbu/twataa/wlfareInfo/moveTWAT52011M.do?wlfareInfoId={}&wlfareInfoReldBztpCd=02\"\n",
    "\n",
    "def scrape_service(driver, service_id):\n",
    "    \"\"\"ë‹¨ì¼ ì„œë¹„ìŠ¤ ID í¬ë¡¤ë§\"\"\"\n",
    "    url = URL_TEMPLATE.format(service_id)\n",
    "    details = {\n",
    "        \"ì„œë¹„ìŠ¤ID\": service_id,\n",
    "        \"ì¹´í…Œê³ ë¦¬\": \"\",\n",
    "        \"ì§€ì›ëŒ€ìƒ\": \"\",\n",
    "        \"ì„ ì •ê¸°ì¤€\": \"\",\n",
    "        \"ì„œë¹„ìŠ¤ë‚´ìš©\": \"\",\n",
    "        \"ì‹ ì²­ë°©ë²•\": \"\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        # ë¡œë”© ëŒ€ê¸°\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.cl-text\"))\n",
    "        )\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ\n",
    "        categories = []\n",
    "        category_divs = soup.find_all(\"div\", class_=\"cl-text\", style=lambda x: x and \"vertical-align:inherit\" in x)\n",
    "        for div in category_divs:\n",
    "            txt = div.get_text(strip=True)\n",
    "            if txt not in [\"ì§€ì›ëŒ€ìƒ\", \"ì„ ì •ê¸°ì¤€\", \"ì„œë¹„ìŠ¤ ë‚´ìš©\", \"ì‹ ì²­ë°©ë²•\"]:  # ë³¸ë¬¸ í‚¤ ì œì™¸\n",
    "                categories.append(txt)\n",
    "        details[\"ì¹´í…Œê³ ë¦¬\"] = \", \".join(categories)\n",
    "\n",
    "        # ë³¸ë¬¸ ì¶”ì¶œ\n",
    "        mapping = {\n",
    "            \"ì§€ì›ëŒ€ìƒ\": \"ì§€ì›ëŒ€ìƒ\",\n",
    "            \"ì„ ì •ê¸°ì¤€\": \"ì„ ì •ê¸°ì¤€\",\n",
    "            \"ì„œë¹„ìŠ¤ ë‚´ìš©\": \"ì„œë¹„ìŠ¤ë‚´ìš©\",\n",
    "            \"ì‹ ì²­ë°©ë²•\": \"ì‹ ì²­ë°©ë²•\"\n",
    "        }\n",
    "        for title_div in soup.find_all(\"div\", class_=\"cl-text\"):\n",
    "            title = title_div.get_text(strip=True)\n",
    "            if title in mapping:\n",
    "                content_div = title_div.find_next(\"div\", style=lambda x: x and \"overflow:hidden\" in x)\n",
    "                if content_div:\n",
    "                    details[mapping[title]] = content_div.get_text(\" \", strip=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜¤ë¥˜ (ID: {service_id}): {e}\")\n",
    "\n",
    "    return details\n",
    "\n",
    "\n",
    "def get_resume_point(batch_prefix=\"ì§€ìì²´_ë³µì§€ì„œë¹„ìŠ¤_í¬ë¡¤ë§ê²°ê³¼_batch\"):\n",
    "    \"\"\"ë§ˆì§€ë§‰ ì €ì¥ëœ ë°°ì¹˜ íŒŒì¼ì„ ì°¾ì•„ ì´ì–´í•˜ê¸° ì‹œì‘ì  ë°˜í™˜\"\"\"\n",
    "    batch_files = [f for f in os.listdir() if f.startswith(batch_prefix) and f.endswith(\".csv\")]\n",
    "    if not batch_files:\n",
    "        return 0, 1  # (ì‹œì‘ index, ë‹¤ìŒ batch ë²ˆí˜¸)\n",
    "\n",
    "    batch_files.sort()\n",
    "    last_file = batch_files[-1]\n",
    "    print(f\"ğŸ“‚ ë§ˆì§€ë§‰ ì €ì¥ëœ íŒŒì¼: {last_file}\")\n",
    "\n",
    "    try:\n",
    "        df_last = pd.read_csv(last_file)\n",
    "        if not df_last.empty:\n",
    "            last_id = df_last[\"ì„œë¹„ìŠ¤ID\"].iloc[-1]\n",
    "            print(f\"ğŸ‘‰ ë§ˆì§€ë§‰ ì €ì¥ëœ ì„œë¹„ìŠ¤ID: {last_id}\")\n",
    "            return df_last.index[-1] + 1 + (len(batch_files)-1)*100, len(batch_files)+1\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ì´ì–´í•˜ê¸° íŒŒì¼ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "    return 0, 1\n",
    "\n",
    "\n",
    "def main(batch_size=100):\n",
    "    # ì„œë¹„ìŠ¤ID ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    df_master = pd.read_csv(MASTER_CSV)\n",
    "    service_ids = df_master[SERVICE_ID_COLUMN].dropna().astype(str).tolist()\n",
    "\n",
    "    # ì´ì–´í•˜ê¸° ì‹œì‘ì  í™•ì¸\n",
    "    start_idx, batch_num = get_resume_point()\n",
    "    if start_idx >= len(service_ids):\n",
    "        print(\"ğŸ‰ ëª¨ë“  ì„œë¹„ìŠ¤IDì— ëŒ€í•œ í¬ë¡¤ë§ì´ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    # í¬ë¡¬ ë“œë¼ì´ë²„ ì¤€ë¹„\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--start-maximized\")\n",
    "    service = ChromeService(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    results = []\n",
    "    last_id = None\n",
    "\n",
    "    for idx, sid in enumerate(service_ids[start_idx:], start=start_idx+1):\n",
    "        print(f\"[{idx}/{len(service_ids)}] í¬ë¡¤ë§ ì¤‘: {sid}\")\n",
    "        data = scrape_service(driver, sid)\n",
    "        results.append(data)\n",
    "        last_id = sid\n",
    "\n",
    "        # 100ê°œ ë‹¨ìœ„ ì €ì¥\n",
    "        if idx % batch_size == 0:\n",
    "            out_df = pd.DataFrame(results)\n",
    "            out_df.to_csv(f\"ì§€ìì²´_ë³µì§€ì„œë¹„ìŠ¤_í¬ë¡¤ë§ê²°ê³¼_batch{batch_num}.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "            print(f\"ğŸ’¾ Batch {batch_num} ì €ì¥ ì™„ë£Œ ({len(results)}ê°œ)\")\n",
    "            batch_num += 1\n",
    "            results = []\n",
    "\n",
    "        time.sleep(0.5)  # ì„œë²„ ë¶€í•˜ ë°©ì§€\n",
    "\n",
    "    # ë§ˆì§€ë§‰ ë‚¨ì€ ê²°ê³¼ ì €ì¥\n",
    "    if results:\n",
    "        out_df = pd.DataFrame(results)\n",
    "        out_df.to_csv(f\"ì§€ìì²´_ë³µì§€ì„œë¹„ìŠ¤_í¬ë¡¤ë§ê²°ê³¼_batch{batch_num}.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"ğŸ’¾ Batch {batch_num} ì €ì¥ ì™„ë£Œ ({len(results)}ê°œ)\")\n",
    "\n",
    "    driver.quit()\n",
    "    print(f\"âœ… ë§ˆì§€ë§‰ ì €ì¥ëœ ì„œë¹„ìŠ¤ID: {last_id}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d5a90f",
   "metadata": {},
   "source": [
    "ì§€ìì²´ í¬ë¡¤ë§ í†µí•©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a566c187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# í•©ì¹  íŒŒì¼ íŒ¨í„´ ì§€ì •\n",
    "file_list = sorted(glob.glob(\"ì§€ìì²´_ë³µì§€ì„œë¹„ìŠ¤_í¬ë¡¤ë§ê²°ê³¼_batch*.csv\"))\n",
    "\n",
    "print(f\"ì´ {len(file_list)}ê°œ íŒŒì¼ì„ ë³‘í•©í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# íŒŒì¼ ì½ì–´ì„œ í•©ì¹˜ê¸°\n",
    "dfs = [pd.read_csv(f) for f in file_list]\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# ìµœì¢… ì €ì¥\n",
    "merged_df.to_csv(\"ì§€ìì²´_ë³µì§€ì„œë¹„ìŠ¤_í¬ë¡¤ë§ê²°ê³¼_í†µí•©ë³¸.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"âœ… ë³‘í•© ì™„ë£Œ: ì§€ìì²´_ë³µì§€ì„œë¹„ìŠ¤_í¬ë¡¤ë§ê²°ê³¼_í†µí•©ë³¸.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fd61d1",
   "metadata": {},
   "source": [
    "ì§€ìì²´ csv í†µí•© - ì„¸ë¶€ë‚´ìš© + ë¦¬ìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59b4208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "base_df = pd.read_csv(\"ì§€ìì²´_ë³µì§€ì„œë¹„ìŠ¤ë¦¬ìŠ¤íŠ¸_í†µí•©.csv\", encoding=\"utf-8-sig\")\n",
    "crawl_df = pd.read_csv(\"ì§€ìì²´_ë³µì§€ì„œë¹„ìŠ¤_í¬ë¡¤ë§ê²°ê³¼_í†µí•©ë³¸.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "# ì„œë¹„ìŠ¤ID ê¸°ì¤€ ë³‘í•© (ì¢Œì¸¡ ê¸°ì¤€)\n",
    "merged_df = pd.merge(base_df, crawl_df, on=\"ì„œë¹„ìŠ¤ID\", how=\"left\")\n",
    "\n",
    "# ì €ì¥ (ìµœì¢… íŒŒì¼ ì´ë¦„ ì§€ì •)\n",
    "output_file = \"local_final_welfare_list.csv\"\n",
    "merged_df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"âœ… ë³‘í•© ì™„ë£Œ: {output_file}\")\n",
    "print(f\"ì´ í–‰ ìˆ˜: {len(merged_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98b89f2",
   "metadata": {},
   "source": [
    "## ë¯¼ê°„ë³µì§€ë¦¬ìŠ¤íŠ¸ í¬ë¡¤ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c11ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "URL = \"https://www.bokjiro.go.kr/ssis-tbu/TWAT52005M/twataa/wlfareInfo/selectWlfareInfo.do\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36\",\n",
    "    \"Referer\": \"https://www.bokjiro.go.kr/ssis-tbu/twataa/wlfareInfo/moveTWAT52005M.do\",\n",
    "    \"Content-Type\": \"application/json;charset=UTF-8\",\n",
    "}\n",
    "\n",
    "def fetch_page(page: int):\n",
    "    payload = {\n",
    "        \"dmSearchParam\": {\n",
    "            \"page\": str(page),\n",
    "            \"orderBy\": \"date\",\n",
    "            \"tabId\": \"3\",   # âœ… ë¯¼ê°„ íƒ­\n",
    "            \"searchTerm\": \"\",\n",
    "            \"onlineYn\": \"\",\n",
    "            \"bkjrLftmCycCd\": \"\",\n",
    "            \"daesang\": \"\",\n",
    "            \"period\": \"\",\n",
    "            \"age\": \"\",\n",
    "            \"region\": \"\",\n",
    "            \"jjim\": \"\",\n",
    "            \"subject\": \"\",\n",
    "            \"favoriteKeyword\": \"Y\",\n",
    "            \"sido\": \"\",\n",
    "            \"gungu\": \"\",\n",
    "            \"endYn\": \"N\"\n",
    "        },\n",
    "        \"dmScr\": {\n",
    "            \"curScrId\": \"tbu/app/twat/twata/twataa/TWAT52005M\",\n",
    "            \"befScrId\": \"\"\n",
    "        }\n",
    "    }\n",
    "    res = requests.post(URL, headers=headers, json=payload)\n",
    "    res.raise_for_status()\n",
    "    return res.json().get(\"dsServiceList3\", [])\n",
    "\n",
    "# âœ… ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§\n",
    "all_data = []\n",
    "for page in range(1, 39):  # 1~38 í˜ì´ì§€\n",
    "    data = fetch_page(page)\n",
    "    print(f\"{page}í˜ì´ì§€: {len(data)}ê±´ ìˆ˜ì§‘\")\n",
    "    all_data.extend(data)\n",
    "    time.sleep(0.3)\n",
    "\n",
    "print(\"ì´ ë¯¼ê°„ ë°ì´í„°:\", len(all_data))\n",
    "\n",
    "# âœ… ê¸°ë³¸ í•„ë“œ + RETURN_STR ì„¸ë¶€í•­ëª© íŒŒì‹±\n",
    "rows = []\n",
    "for svc in all_data:\n",
    "    base_info = {\n",
    "        \"ì„œë¹„ìŠ¤ID\": svc.get(\"WLFARE_INFO_ID\"),\n",
    "        \"ì„œë¹„ìŠ¤ëª…\": svc.get(\"WLFARE_INFO_NM\"),\n",
    "        \"ì„œë¹„ìŠ¤ ìš”ì•½\": svc.get(\"WLFARE_INFO_OUTL_CN\"),\n",
    "        \"ìƒì„¸ ë§í¬\": f\"https://www.bokjiro.go.kr/ssis-tbu/twataa/wlfareInfo/moveTWAT52011M.do?wlfareInfoId={svc.get('WLFARE_INFO_ID')}&wlfareInfoReldBztpCd=03\",\n",
    "        \"ì†Œê´€ê¸°ê´€\": svc.get(\"BIZ_CHR_INST_NM\"),\n",
    "        \"ëŒ€í‘œì—°ë½ì²˜\": svc.get(\"RPRS_CTADR\"),\n",
    "        \"ì‹ ì²­ê°€ëŠ¥ì—¬ë¶€\": svc.get(\"ONLINEYN\"),\n",
    "        \"ì£¼ì†Œ\": svc.get(\"ADDR\"),\n",
    "        \"íƒœê·¸\": svc.get(\"TAG_NM\"),\n",
    "        \"ì‹œì‘ì¼\": svc.get(\"ENFC_BGNG_YMD\"),\n",
    "        \"ì¢…ë£Œì¼\": svc.get(\"ENFC_END_YMD\"),\n",
    "        \"ì§„í–‰ìƒíƒœ\": svc.get(\"CVL_PROGRSS_STATUS\"),\n",
    "    }\n",
    "    \n",
    "    # RETURN_STR íŒŒì‹±\n",
    "    return_str = svc.get(\"RETURN_STR\", \"\")\n",
    "    parsed_info = {}\n",
    "    if return_str:\n",
    "        for item in return_str.split(\";\"):\n",
    "            if \":\" in item:\n",
    "                k, v = item.split(\":\", 1)\n",
    "                parsed_info[k.strip()] = v.strip()\n",
    "    \n",
    "    # ê¸°ë³¸ì •ë³´ + RETURN_STR í™•ì¥ì •ë³´ ë³‘í•©\n",
    "    rows.append({**base_info, **parsed_info})\n",
    "\n",
    "# âœ… CSV ì €ì¥\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"ë¯¼ê°„_ë³µì§€ì„œë¹„ìŠ¤_í™•ì¥.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"ğŸ“ ë¯¼ê°„_ë³µì§€ì„œë¹„ìŠ¤_í™•ì¥.csv ì €ì¥ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58bce38",
   "metadata": {},
   "source": [
    "ë¯¼ê°„ë³µì§€ ìƒì„¸ë‚´ì—­ í¬ë¡¤ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef99aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# ì…ë ¥ CSV íŒŒì¼\n",
    "MASTER_CSV = \"ë¯¼ê°„_ë³µì§€ì„œë¹„ìŠ¤_í™•ì¥.csv\"   # ë¯¼ê°„ ì„œë¹„ìŠ¤ ID ë“¤ì–´ìˆëŠ” íŒŒì¼\n",
    "SERVICE_ID_COLUMN = \"ì„œë¹„ìŠ¤ID\"\n",
    "\n",
    "# URL í¬ë§·\n",
    "URL_TEMPLATE = \"https://www.bokjiro.go.kr/ssis-tbu/twataa/wlfareInfo/moveTWAT52015M.do?wlfareInfoId={}&wlfareInfoReldBztpCd=03\"\n",
    "\n",
    "def scrape_service(driver, service_id):\n",
    "    \"\"\"ë‹¨ì¼ ë¯¼ê°„ ì„œë¹„ìŠ¤ ID ìƒì„¸ í¬ë¡¤ë§\"\"\"\n",
    "    url = URL_TEMPLATE.format(service_id)\n",
    "    details = {\n",
    "        \"ì„œë¹„ìŠ¤ID\": service_id,\n",
    "        \"ì¹´í…Œê³ ë¦¬\": \"\",\n",
    "        \"ì‚¬ì—…ìƒíƒœ\": \"\",\n",
    "        \"ì‚¬ì—…ê¸°ê°„\": \"\",\n",
    "        \"ì—°ë½ì²˜\": \"\",\n",
    "        \"ì´ë©”ì¼\": \"\",\n",
    "        \"ì‚¬ì—…ëª©ì \": \"\",\n",
    "        \"ì§€ì›ëŒ€ìƒ\": \"\",\n",
    "        \"ì§€ì›ë‚´ìš©\": \"\",\n",
    "        \"ì‹ ì²­ë°©ë²•\": \"\",\n",
    "        \"ì œì¶œì„œë¥˜\": \"\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        # ë¡œë”© ëŒ€ê¸°\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.cl-text\"))\n",
    "        )\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # âœ… ì¹´í…Œê³ ë¦¬ (ìƒë‹¨ badge-wlfare ì˜ì—­)\n",
    "        category_divs = soup.select(\"div.badge-wlfare div.cl-text\")\n",
    "        categories = [c.get_text(strip=True) for c in category_divs if c.get_text(strip=True)]\n",
    "        details[\"ì¹´í…Œê³ ë¦¬\"] = \", \".join(categories)\n",
    "\n",
    "        # âœ… ë‹¨ì¼ í•„ë“œ (ì‚¬ì—…ìƒíƒœ, ì‚¬ì—…ê¸°ê°„, ì—°ë½ì²˜, ì´ë©”ì¼)\n",
    "        mapping_simple = {\n",
    "            \"ì‚¬ì—…ìƒíƒœ\": \"ì‚¬ì—…ìƒíƒœ\",\n",
    "            \"ì‚¬ì—…ê¸°ê°„\": \"ì‚¬ì—…ê¸°ê°„\",\n",
    "            \"ì—°ë½ì²˜\": \"ì—°ë½ì²˜\",\n",
    "            \"ì´ë©”ì¼\": \"ì´ë©”ì¼\"\n",
    "        }\n",
    "        for title, col in mapping_simple.items():\n",
    "            title_div = soup.find(\"div\", class_=\"cl-text\", string=title)\n",
    "            if title_div:\n",
    "                sibling_div = title_div.find_parent().find_next_sibling(\"div\")\n",
    "                if sibling_div and \"cl-text\" in sibling_div.get(\"class\", []):\n",
    "                    details[col] = sibling_div.get_text(\" \", strip=True)\n",
    "\n",
    "        # âœ… ë³¸ë¬¸ í•„ë“œ (ì‚¬ì—…ëª©ì , ì§€ì›ëŒ€ìƒ, ì§€ì›ë‚´ìš©, ì‹ ì²­ë°©ë²•, ì œì¶œì„œë¥˜)\n",
    "        mapping_long = {\n",
    "            \"ì‚¬ì—…ëª©ì \": \"ì‚¬ì—…ëª©ì \",\n",
    "            \"ì§€ì›ëŒ€ìƒ\": \"ì§€ì›ëŒ€ìƒ\",\n",
    "            \"ì§€ì›ë‚´ìš©\": \"ì§€ì›ë‚´ìš©\",\n",
    "            \"ì‹ ì²­ë°©ë²•\": \"ì‹ ì²­ë°©ë²•\",\n",
    "            \"ì œì¶œì„œë¥˜\": \"ì œì¶œì„œë¥˜\"\n",
    "        }\n",
    "        for title, col in mapping_long.items():\n",
    "            title_div = soup.find(\"div\", class_=\"cl-text\", string=title)\n",
    "            if title_div:\n",
    "                content_block = title_div.find_parent().find_next(\"div\", class_=\"cl-control\")\n",
    "                if content_block:\n",
    "                    text_div = content_block.find(\"div\", class_=\"cl-text\")\n",
    "                    if text_div:\n",
    "                        details[col] = text_div.get_text(\"\\n\", strip=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜¤ë¥˜ (ID: {service_id}): {e}\")\n",
    "\n",
    "    return details\n",
    "\n",
    "\n",
    "def get_resume_point(batch_prefix=\"ë¯¼ê°„_ë³µì§€ì„œë¹„ìŠ¤_í¬ë¡¤ë§ê²°ê³¼_batch\"):\n",
    "    \"\"\"ë§ˆì§€ë§‰ ì €ì¥ëœ ë°°ì¹˜ íŒŒì¼ì„ ì°¾ì•„ ì´ì–´í•˜ê¸° ì‹œì‘ì  ë°˜í™˜\"\"\"\n",
    "    batch_files = [f for f in os.listdir() if f.startswith(batch_prefix) and f.endswith(\".csv\")]\n",
    "    if not batch_files:\n",
    "        return 0, 1  # (ì‹œì‘ index, ë‹¤ìŒ batch ë²ˆí˜¸)\n",
    "\n",
    "    batch_files.sort()\n",
    "    last_file = batch_files[-1]\n",
    "    print(f\"ğŸ“‚ ë§ˆì§€ë§‰ ì €ì¥ëœ íŒŒì¼: {last_file}\")\n",
    "\n",
    "    try:\n",
    "        df_last = pd.read_csv(last_file)\n",
    "        if not df_last.empty:\n",
    "            last_id = df_last[\"ì„œë¹„ìŠ¤ID\"].iloc[-1]\n",
    "            print(f\"ğŸ‘‰ ë§ˆì§€ë§‰ ì €ì¥ëœ ì„œë¹„ìŠ¤ID: {last_id}\")\n",
    "            return df_last.index[-1] + 1 + (len(batch_files)-1)*100, len(batch_files)+1\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ì´ì–´í•˜ê¸° íŒŒì¼ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "    return 0, 1\n",
    "\n",
    "\n",
    "def main(batch_size=100):\n",
    "    # ì„œë¹„ìŠ¤ID ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    df_master = pd.read_csv(MASTER_CSV)\n",
    "    service_ids = df_master[SERVICE_ID_COLUMN].dropna().astype(str).tolist()\n",
    "\n",
    "    # ì´ì–´í•˜ê¸° ì‹œì‘ì  í™•ì¸\n",
    "    start_idx, batch_num = get_resume_point()\n",
    "    if start_idx >= len(service_ids):\n",
    "        print(\"ğŸ‰ ëª¨ë“  ì„œë¹„ìŠ¤IDì— ëŒ€í•œ í¬ë¡¤ë§ì´ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    # í¬ë¡¬ ë“œë¼ì´ë²„ ì¤€ë¹„\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--start-maximized\")\n",
    "    service = ChromeService(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    results = []\n",
    "    last_id = None\n",
    "\n",
    "    for idx, sid in enumerate(service_ids[start_idx:], start=start_idx+1):\n",
    "        print(f\"[{idx}/{len(service_ids)}] í¬ë¡¤ë§ ì¤‘: {sid}\")\n",
    "        data = scrape_service(driver, sid)\n",
    "        results.append(data)\n",
    "        last_id = sid\n",
    "\n",
    "        # 100ê°œ ë‹¨ìœ„ ì €ì¥\n",
    "        if idx % batch_size == 0:\n",
    "            out_df = pd.DataFrame(results)\n",
    "            out_df.to_csv(f\"ë¯¼ê°„_ë³µì§€ì„œë¹„ìŠ¤_í¬ë¡¤ë§ê²°ê³¼_batch{batch_num}.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "            print(f\"ğŸ’¾ Batch {batch_num} ì €ì¥ ì™„ë£Œ ({len(results)}ê°œ)\")\n",
    "            batch_num += 1\n",
    "            results = []\n",
    "\n",
    "        time.sleep(0.5)  # ì„œë²„ ë¶€í•˜ ë°©ì§€\n",
    "\n",
    "    # ë§ˆì§€ë§‰ ë‚¨ì€ ê²°ê³¼ ì €ì¥\n",
    "    if results:\n",
    "        out_df = pd.DataFrame(results)\n",
    "        out_df.to_csv(f\"ë¯¼ê°„_ë³µì§€ì„œë¹„ìŠ¤_í¬ë¡¤ë§ê²°ê³¼_batch{batch_num}.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"ğŸ’¾ Batch {batch_num} ì €ì¥ ì™„ë£Œ ({len(results)}ê°œ)\")\n",
    "\n",
    "    driver.quit()\n",
    "    print(f\"âœ… ë§ˆì§€ë§‰ ì €ì¥ëœ ì„œë¹„ìŠ¤ID: {last_id}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f6a011",
   "metadata": {},
   "source": [
    "íŒŒì¼ í•©ì¹˜ê¸° - ìƒì„¸ë‚´ì—­ë§Œ ë‚˜ì˜¨ê²ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6421cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# ë³‘í•©í•  íŒŒì¼ íŒ¨í„´\n",
    "file_pattern = \"ë¯¼ê°„_ë³µì§€ì„œë¹„ìŠ¤_í¬ë¡¤ë§ê²°ê³¼_batch*.csv\"\n",
    "\n",
    "# ëª¨ë“  ë°°ì¹˜ íŒŒì¼ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°\n",
    "files = sorted(glob.glob(file_pattern))\n",
    "\n",
    "print(\"ğŸ“‚ í•©ì¹  íŒŒì¼:\", files)\n",
    "\n",
    "# íŒŒì¼ ì½ì–´ì„œ ì„¸ë¡œë¡œ ì´ì–´ë¶™ì´ê¸°\n",
    "dfs = [pd.read_csv(f) for f in files]\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# ì €ì¥\n",
    "output_file = \"ë¯¼ê°„_ë³µì§€ì„œë¹„ìŠ¤_í¬ë¡¤ë§ê²°ê³¼_total.csv\"\n",
    "merged_df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"âœ… ì €ì¥ ì™„ë£Œ: {output_file}, ì´ {len(merged_df)} í–‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4304c70f",
   "metadata": {},
   "source": [
    "ë¦¬ìŠ¤íŠ¸ + ìƒì„¸ë‚´ì—­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26984e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ\n",
    "base_file = \"ë¯¼ê°„_ë³µì§€ì„œë¹„ìŠ¤_í™•ì¥.csv\"          # ê¸°ì¤€ íŒŒì¼\n",
    "crawl_file = \"ë¯¼ê°„_ë³µì§€ì„œë¹„ìŠ¤_í¬ë¡¤ë§ê²°ê³¼_total.csv\"  # í¬ë¡¤ë§ ê²°ê³¼ íŒŒì¼\n",
    "output_file = \"private_final_welfare_list.csv\"\n",
    "\n",
    "# CSV ì½ê¸°\n",
    "df_base = pd.read_csv(base_file)\n",
    "df_crawl = pd.read_csv(crawl_file)\n",
    "\n",
    "# ê¸°ì¤€ ì»¬ëŸ¼ëª… í™•ì¸\n",
    "print(\"ê¸°ì¤€ íŒŒì¼ ì»¬ëŸ¼:\", df_base.columns.tolist())\n",
    "print(\"í¬ë¡¤ë§ íŒŒì¼ ì»¬ëŸ¼:\", df_crawl.columns.tolist())\n",
    "\n",
    "# ì„œë¹„ìŠ¤ID ê¸°ì¤€ìœ¼ë¡œ ë³‘í•© (ì˜¤ë¥¸ìª½ ë°©í–¥ìœ¼ë¡œ ë¶™ì´ê¸°)\n",
    "df_merged = pd.merge(df_base, df_crawl, on=\"ì„œë¹„ìŠ¤ID\", how=\"left\")\n",
    "\n",
    "# ì €ì¥\n",
    "df_merged.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"âœ… ë³‘í•© ì™„ë£Œ! ì €ì¥ëœ íŒŒì¼: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2c014b",
   "metadata": {},
   "source": [
    "# ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4186114d",
   "metadata": {},
   "source": [
    "## local_welfare ì „ì²˜ë¦¬ì •ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f8c474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# CSV ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "file_path = \"local_final_welfare_list.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# ==========================\n",
    "# 1. ì¹´í…Œê³ ë¦¬ ì»¬ëŸ¼ ë¶ˆí•„ìš”í•œ ê°’ ì œê±°\n",
    "# ==========================\n",
    "remove_list = [\n",
    "    \"í™”ë©´í¬ê¸°\", \"ë‹˜\", \"ë¡œê·¸ì•„ì›ƒê¹Œì§€ ë‚¨ì€ ì‹œê°„\", \"30:00\",\n",
    "    \"ëŒ€í•œë¯¼êµ­ êµ­ë¯¼ ëˆ„êµ¬ë‚˜!\", \"ì§€ìì²´ ë³µì§€ì„œë¹„ìŠ¤\", \"ì°œí•˜ê¸°\",\n",
    "    \"ìµœì¢… ìˆ˜ì •ì¼(ë°˜ì˜ì¼)\", \"ì§€ì›ì£¼ê¸°\", \"ì œê³µìœ í˜•\", \"ì²˜ë¦¬ì ˆì°¨\"\n",
    "]\n",
    "\n",
    "def clean_category(val):\n",
    "    if pd.isna(val):\n",
    "        return val\n",
    "    items = [x.strip() for x in str(val).split(\",\")]\n",
    "    filtered = []\n",
    "    for x in items:\n",
    "        if len(x) == 10 and x[4] == \"-\" and x[7] == \"-\":\n",
    "            continue\n",
    "        if \"íšŒì„±\" in x:\n",
    "            continue\n",
    "        if any(r == x or r in x for r in remove_list):\n",
    "            continue\n",
    "        filtered.append(x)\n",
    "    return \", \".join(filtered)\n",
    "\n",
    "df[\"ì¹´í…Œê³ ë¦¬\"] = df[\"ì¹´í…Œê³ ë¦¬\"].apply(clean_category)\n",
    "\n",
    "# ==========================\n",
    "# 2. ì‹ ì²­ë°©ë²•_x â†’ ì‹ ì²­í™˜ê²½\n",
    "# ==========================\n",
    "if \"ì‹ ì²­ë°©ë²•_x\" in df.columns:\n",
    "    df.rename(columns={\"ì‹ ì²­ë°©ë²•_x\": \"ì‹ ì²­í™˜ê²½\"}, inplace=True)\n",
    "\n",
    "    def normalize_env(val):\n",
    "        if pd.isna(val):\n",
    "            return val\n",
    "        val = str(val).replace(\" \", \"\")\n",
    "        if \"ë°©ë¬¸\" in val and \"ì¸í„°ë„·\" in val:\n",
    "            return \"ALL\"\n",
    "        elif \"ë°©ë¬¸\" in val:\n",
    "            return \"ë°©ë¬¸\"\n",
    "        elif \"ì¸í„°ë„·\" in val:\n",
    "            return \"ì¸í„°ë„·\"\n",
    "        return val\n",
    "\n",
    "    df[\"ì‹ ì²­í™˜ê²½\"] = df[\"ì‹ ì²­í™˜ê²½\"].apply(normalize_env)\n",
    "\n",
    "# ==========================\n",
    "# 3. ì‹ ì²­ë°©ë²•_y â†’ ì‹ ì²­ë°©ë²•ìƒì„¸\n",
    "# ==========================\n",
    "if \"ì‹ ì²­ë°©ë²•_y\" in df.columns:\n",
    "    df.rename(columns={\"ì‹ ì²­ë°©ë²•_y\": \"ì‹ ì²­ë°©ë²•ìƒì„¸\"}, inplace=True)\n",
    "\n",
    "    def clean_detail(val):\n",
    "        if pd.isna(val):\n",
    "            return val\n",
    "        text = str(val).strip()\n",
    "        # ë§¨ ì• \"-\" ì œê±°\n",
    "        text = re.sub(r\"^-\\s*\", \"\", text)\n",
    "        # ë‚˜ë¨¸ì§€ \"-\" ë¥¼ \",\" ë¡œ ë³€í™˜\n",
    "        text = text.replace(\"-\", \",\")\n",
    "        return text.strip()\n",
    "\n",
    "    df= df.applymap(lambda x: clean_detail(x))\n",
    "\n",
    "# ==========================\n",
    "# ì €ì¥\n",
    "# ==========================\n",
    "output_path = \"local_final_welfare_list_cleaned_copied.csv\"\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ! ì €ì¥ëœ íŒŒì¼:\", output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42e3c5c",
   "metadata": {},
   "source": [
    "ì¹¼ëŸ¼ ë§ì¶”ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5ce0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "file_path = \"local_final_welfare_list_cleaned_copied.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# ì¹´í…Œê³ ë¦¬ ì‚¬ì „ ì •ì˜ (í‚¤ì›Œë“œ ë§¤í•‘)\n",
    "category_keywords = {\n",
    "    \"ìƒê³„ì§€ì›\": [\"ì‹ë¹„\", \"ì£¼ê±°ë¹„\", \"ê³µê³¼ê¸ˆ\", \"ì°¨ìƒìœ„\", \"ìƒí™œë¹„\", \"ê¸´ê¸‰ë³µì§€\", \"í• ì¸\"],\n",
    "    \"ëŒë´„Â·ë³´í˜¸\": [\"ê°„ë³‘\", \"ì‹œì„¤ë³´í˜¸\", \"ì£¼ì•¼ê°„\", \"ë³´í˜¸ì\", \"ì–‘ìœ¡\", \"ê°„í˜¸\"],\n",
    "    \"ê±´ê°•Â·ì˜ë£Œ\": [\"ì§ˆë³‘\", \"ì§„ë‹¨\", \"ì¹˜ë£Œ\", \"ì˜ë£Œë¹„\", \"ë³‘ì›\", \"ì„ì‹ \", \"ì¶œì‚°\"],\n",
    "    \"ì¼ìƒìƒí™œ\": [\"ê°€ì‚¬\", \"ì‹ì‚¬\", \"ì´ë™\", \"ìœ„ìƒ\", \"ìƒí™œì§€ì›\"],\n",
    "    \"ì•ˆì „ìœ„ê¸°\": [\"í­ë ¥\", \"í•™ëŒ€\", \"ìí•´\", \"ìœ„ê¸°\", \"ê¸´ê¸‰\", \"ì•ˆì „\"],\n",
    "    \"ì£¼ê±°ì§€ì›\": [\"ê±°ì²˜\", \"ì„ëŒ€\", \"ì „ì„¸\", \"ì›”ì„¸\", \"ë³´ì¦ê¸ˆ\", \"ì£¼ê±°í™˜ê²½\"],\n",
    "    \"ì¼ìë¦¬\": [\"êµ¬ì§\", \"ìí™œ\", \"ì·¨ì—…\", \"ì§ì—…\", \"í›ˆë ¨\", \"ëŠ¥ë ¥ê°œë°œ\"],\n",
    "    \"ì•„ë™ì§€ì›\": [\"ì•„ë™\", \"ë³´ìœ¡\", \"êµìœ¡\", \"ì–‘ìœ¡ìƒë‹´\", \"ëŒë´„\"],\n",
    "    \"ì±„ë¬´Â·ë²•ë¥ \": [\"ì±„ë¬´\", \"ë²•ë¥ \", \"ì†Œì†¡\", \"ë³€í˜¸\", \"ìƒë‹´\", \"ë¹š\"],\n",
    "    \"ê³ ë¦½Â·ê³ ë…\": [\"ê³ ë…\", \"ê³ ë¦½\", \"ì‚¬íšŒì \", \"ì™¸ë¡œì›€\", \"ê³ ë…ì‚¬\"]\n",
    "}\n",
    "\n",
    "# ì¹´í…Œê³ ë¦¬ ìš°ì„ ìˆœìœ„ ì •ì˜\n",
    "priority_order = [\n",
    "    \"ìƒê³„ì§€ì›\", \"ì£¼ê±°ì§€ì›\", \"ì¼ìë¦¬\", \"ì•„ë™ì§€ì›\",\n",
    "    \"ëŒë´„Â·ë³´í˜¸\", \"ê±´ê°•Â·ì˜ë£Œ\", \"ì•ˆì „ìœ„ê¸°\", \"ì±„ë¬´Â·ë²•ë¥ \", \"ê³ ë¦½Â·ê³ ë…\"\n",
    "]\n",
    "\n",
    "# ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ í•¨ìˆ˜ (í•­ìƒ í•˜ë‚˜ ì„ íƒ)\n",
    "def assign_single_category(row):\n",
    "    text = \"\"\n",
    "    for col in [\"ì„œë¹„ìŠ¤ ìš”ì•½\", \"ì„œë¹„ìŠ¤ë‚´ìš©\", \"ì‹ ì²­ë°©ë²•ìƒì„¸\"]:\n",
    "        if col in df.columns and pd.notna(row[col]):\n",
    "            text += \" \" + str(row[col])\n",
    "    matched = [cat for cat, keywords in category_keywords.items() if any(k in text for k in keywords)]\n",
    "    for cat in priority_order:  # ìš°ì„ ìˆœìœ„ëŒ€ë¡œ í•˜ë‚˜ë§Œ ì„ íƒ\n",
    "        if cat in matched:\n",
    "            return cat\n",
    "    return \"ìƒê³„ì§€ì›\"  # ë§¤ì¹­ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’\n",
    "\n",
    "# ì¹´í…Œê³ ë¦¬ ê°±ì‹ \n",
    "df[\"ì¹´í…Œê³ ë¦¬\"] = df.apply(assign_single_category, axis=1)\n",
    "\n",
    "# ì €ì¥\n",
    "output_path = \"local_final_welfare_list_categorized.csv\"\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"âœ… ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì™„ë£Œ, ì €ì¥ëœ íŒŒì¼:\", output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c9588e",
   "metadata": {},
   "source": [
    "## ì¤‘ì•™ë¶€ì²˜ ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b678d1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "file_path = \"goverment_final_welfare_list_with_features.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 1. ìƒì„¸ë§í¬ ì¶”ê°€\n",
    "df[\"ìƒì„¸ë§í¬\"] = df[\"ì„œë¹„ìŠ¤ID\"].apply(\n",
    "    lambda x: f\"https://www.bokjiro.go.kr/ssis-tbu/twataa/wlfareInfo/moveTWAT52011M.do?wlfareInfoId={x}&wlfareInfoReldBztpCd=01\"\n",
    ")\n",
    "\n",
    "# 2. ëŒ€ìƒíŠ¹ì„± ì¶”ì¶œ í•¨ìˆ˜\n",
    "def extract_target_feature(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    # ë‚˜ì´ ë²”ìœ„\n",
    "    match_range = re.search(r\"(\\d{1,2})\\s*~\\s*(\\d{1,2})ì„¸\", text)\n",
    "    if match_range:\n",
    "        return f\"{match_range.group(1)}~{match_range.group(2)}ì„¸\"\n",
    "    match_over = re.search(r\"ë§Œ?\\s*(\\d{1,2})ì„¸\\s*ì´ìƒ\", text)\n",
    "    if match_over:\n",
    "        return f\"{match_over.group(1)}ì„¸ ì´ìƒ\"\n",
    "    match_under = re.search(r\"ë§Œ?\\s*(\\d{1,2})ì„¸\\s*ì´í•˜\", text)\n",
    "    if match_under:\n",
    "        return f\"{match_under.group(1)}ì„¸ ì´í•˜\"\n",
    "\n",
    "    # í‚¤ì›Œë“œ ë§¤í•‘\n",
    "    if re.search(r\"(ì²­ë…„|ì²­ì†Œë…„)\", text):\n",
    "        return \"ì²­ë…„\"\n",
    "    if re.search(r\"(ë…¸ì¸|ë…¸ë…„|ê³ ë ¹|ì–´ë¥´ì‹ )\", text):\n",
    "        return \"ë…¸ë…„\"\n",
    "    if re.search(r\"(ì €ì†Œë“|ì°¨ìƒìœ„|ê¸°ì´ˆìƒí™œ|ì¤‘ìœ„ì†Œë“)\", text):\n",
    "        return \"ì €ì†Œë“\"\n",
    "    if re.search(r\"(ì„ì‹ |ì¶œì‚°|ì‚°ëª¨|ì„ë¶€)\", text):\n",
    "        return \"ì„ì‚°ë¶€\"\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "# ì§€ì›ëŒ€ìƒìƒì„¸ ê¸°ë°˜ ëŒ€ìƒíŠ¹ì„± ìƒì„±\n",
    "if \"ì§€ì›ëŒ€ìƒìƒì„¸\" in df.columns:\n",
    "    df[\"ëŒ€ìƒíŠ¹ì„±\"] = df[\"ì§€ì›ëŒ€ìƒìƒì„¸\"].apply(extract_target_feature)\n",
    "else:\n",
    "    df[\"ëŒ€ìƒíŠ¹ì„±\"] = \"\"\n",
    "\n",
    "# 3. ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜\n",
    "category_keywords = {\n",
    "    \"ìƒê³„ì§€ì›\": [\"ì‹ë¹„\", \"ì£¼ê±°ë¹„\", \"ê³µê³¼ê¸ˆ\", \"ìƒí™œë¹„\", \"ì°¨ìƒìœ„\", \"ê¸´ê¸‰ë³µì§€\", \"í• ì¸\"],\n",
    "    \"ëŒë´„Â·ë³´í˜¸\": [\"ê°„ë³‘\", \"ì‹œì„¤ë³´í˜¸\", \"ì£¼ì•¼ê°„\", \"ë³´í˜¸ì\", \"ì–‘ìœ¡\", \"ê°„í˜¸\"],\n",
    "    \"ê±´ê°•Â·ì˜ë£Œ\": [\"ì§ˆë³‘\", \"ì§„ë‹¨\", \"ì¹˜ë£Œ\", \"ì˜ë£Œë¹„\", \"ë³‘ì›\", \"ì„ì‹ \", \"ì¶œì‚°\"],\n",
    "    \"ì¼ìƒìƒí™œ\": [\"ê°€ì‚¬\", \"ì‹ì‚¬\", \"ì´ë™\", \"ìœ„ìƒ\", \"ìƒí™œì§€ì›\"],\n",
    "    \"ì•ˆì „ìœ„ê¸°\": [\"í­ë ¥\", \"í•™ëŒ€\", \"ìí•´\", \"ìœ„ê¸°\", \"ê¸´ê¸‰\", \"ì•ˆì „\"],\n",
    "    \"ì£¼ê±°ì§€ì›\": [\"ê±°ì²˜\", \"ì„ëŒ€\", \"ì „ì„¸\", \"ì›”ì„¸\", \"ë³´ì¦ê¸ˆ\", \"ì£¼ê±°í™˜ê²½\"],\n",
    "    \"ì¼ìë¦¬\": [\"êµ¬ì§\", \"ìí™œ\", \"ì·¨ì—…\", \"ì§ì—…\", \"í›ˆë ¨\", \"ëŠ¥ë ¥ê°œë°œ\"],\n",
    "    \"ì•„ë™ì§€ì›\": [\"ì•„ë™\", \"ë³´ìœ¡\", \"êµìœ¡\", \"ì–‘ìœ¡ìƒë‹´\", \"ëŒë´„\"],\n",
    "    \"ì±„ë¬´Â·ë²•ë¥ \": [\"ì±„ë¬´\", \"ë²•ë¥ \", \"ì†Œì†¡\", \"ë³€í˜¸\", \"ìƒë‹´\", \"ë¹š\"],\n",
    "    \"ê³ ë¦½Â·ê³ ë…\": [\"ê³ ë…\", \"ê³ ë¦½\", \"ì‚¬íšŒì \", \"ì™¸ë¡œì›€\", \"ê³ ë…ì‚¬\"]\n",
    "}\n",
    "\n",
    "priority_order = [\n",
    "    \"ìƒê³„ì§€ì›\", \"ì£¼ê±°ì§€ì›\", \"ì¼ìë¦¬\", \"ì•„ë™ì§€ì›\",\n",
    "    \"ëŒë´„Â·ë³´í˜¸\", \"ê±´ê°•Â·ì˜ë£Œ\", \"ì•ˆì „ìœ„ê¸°\", \"ì±„ë¬´Â·ë²•ë¥ \", \"ê³ ë¦½Â·ê³ ë…\"\n",
    "]\n",
    "\n",
    "def assign_category(text):\n",
    "    if not text:\n",
    "        return \"ìƒê³„ì§€ì›\"  # ê¸°ë³¸ê°’\n",
    "    text = str(text)\n",
    "    matched = [cat for cat, keywords in category_keywords.items() if any(k in text for k in keywords)]\n",
    "    for cat in priority_order:\n",
    "        if cat in matched:\n",
    "            return cat\n",
    "    return \"ìƒê³„ì§€ì›\"\n",
    "\n",
    "# ì„œë¹„ìŠ¤ê°œìš”ì™€ ì„œë¹„ìŠ¤ëª… ê¸°ì¤€ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜\n",
    "if \"ì„œë¹„ìŠ¤ê°œìš”\" in df.columns:\n",
    "    df[\"ì¹´í…Œê³ ë¦¬\"] = df[\"ì„œë¹„ìŠ¤ê°œìš”\"].apply(assign_category)\n",
    "else:\n",
    "    df[\"ì¹´í…Œê³ ë¦¬\"] = df[\"ì„œë¹„ìŠ¤ëª…\"].apply(assign_category)\n",
    "\n",
    "# 4. ì €ì¥\n",
    "output_path = \"goverment_final_welfare_list_categorized.csv\"\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"âœ… CSV ì €ì¥ ì™„ë£Œ:\", output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264b91df",
   "metadata": {},
   "source": [
    "ë¯¼ê°„ë³µì§€ ë¹ˆ ì¹¼ëŸ¼ ì •ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af31642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ì›ë³¸ íŒŒì¼ ê²½ë¡œ\n",
    "input_path = \"private_final_welfare_list.csv\"\n",
    "\n",
    "# ì¶œë ¥ íŒŒì¼ ê²½ë¡œ\n",
    "base, ext = os.path.splitext(input_path)\n",
    "output_path = base + \"_no_empty_cols\" + ext\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(input_path, dtype=object)  # ëª¨ë“  ì¹¼ëŸ¼ì„ objectë¡œ ì½ìœ¼ë©´ ê³µë°±ì²˜ë¦¬ ì•ˆì „\n",
    "\n",
    "def col_has_any_value(s: pd.Series) -> bool:\n",
    "    \"\"\"\n",
    "    ì‹œë¦¬ì¦ˆê°€ 'ëª¨ë‘ ë¹ˆ ê°’'ì¸ì§€ ê²€ì‚¬.\n",
    "    - NaNì€ ë¹ˆê°’ìœ¼ë¡œ ì²˜ë¦¬\n",
    "    - ë¬¸ìì—´ì¼ ê²½ìš° ê³µë°±ë§Œ ìˆëŠ” ê²ƒë„ ë¹ˆê°’ìœ¼ë¡œ ì²˜ë¦¬\n",
    "    - ìˆ«ì/ê¸°íƒ€ non-NaN ê°’ì€ ê°’ì´ ìˆëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼\n",
    "    \"\"\"\n",
    "    # NaNì„ ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´(ë²¡í„°í™”)\n",
    "    filled = s.where(s.notna(), \"\")\n",
    "    # ë¬¸ìì—´ë¡œ ë³€í™˜ í›„ ì–‘ìª½ ê³µë°± ì œê±°\n",
    "    stripped = filled.astype(str).str.strip()\n",
    "    # í•˜ë‚˜ë¼ë„ ë¹ˆë¬¸ìì—´ì´ ì•„ë‹Œ ê°’ì´ ìˆìœ¼ë©´ True\n",
    "    return (stripped != \"\").any()\n",
    "\n",
    "# ê° ì»¬ëŸ¼ì— ëŒ€í•´ ë¹„ì–´ìˆëŠ”ì§€ ê²€ì‚¬\n",
    "has_value_mask = df.apply(col_has_any_value, axis=0)\n",
    "kept_columns = has_value_mask[has_value_mask].index.tolist()\n",
    "dropped_columns = has_value_mask[~has_value_mask].index.tolist()\n",
    "\n",
    "print(\"ìœ ì§€í•  ì»¬ëŸ¼ ê°œìˆ˜:\", len(kept_columns))\n",
    "print(\"ì œê±°í•  ë¹ˆ ì»¬ëŸ¼ë“¤ (ì´ {}ê°œ):\".format(len(dropped_columns)))\n",
    "for c in dropped_columns:\n",
    "    print(\" -\", c)\n",
    "\n",
    "# ë¹ˆ ì»¬ëŸ¼ ì œê±°í•œ ë°ì´í„°í”„ë ˆì„\n",
    "df_clean = df.loc[:, kept_columns]\n",
    "\n",
    "# ì €ì¥\n",
    "df_clean.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"ì €ì¥ ì™„ë£Œ:\", output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcc47a0",
   "metadata": {},
   "source": [
    "ë¯¼ê°„ë³µì§€ ì „ì²˜ë¦¬-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec14302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "input_path = \"private_final_welfare_list.csv\"\n",
    "output_path = \"private_final_welfare_list_standardized.csv\"\n",
    "\n",
    "# ì•ˆì „í•˜ê²Œ ë¬¸ìì—´ë¡œ ì½ê¸°\n",
    "df = pd.read_csv(input_path, dtype=str)\n",
    "\n",
    "# (ìƒëµ: ì—°ë ¹ ì¶”ì¶œ í•¨ìˆ˜, ì»¬ëŸ¼ rename ë“± ê¸°ì¡´ ì½”ë“œ ìœ ì§€)\n",
    "# --- ì—¬ê¸°ì— ì•ì„œ ì‚¬ìš©í•˜ì‹  derive_age_from_text, ë‚˜ì´ ìƒì„± ì½”ë“œ ë“±ì„ ë„£ìœ¼ì‹œë©´ ë©ë‹ˆë‹¤ ---\n",
    "# (ì˜ˆ: derive_age_from_text ì •ì˜, 'ë‚˜ì´(ì½”ë“œê¸°ë°˜)'/'ë‚˜ì´' ìƒì„± ë“±)\n",
    "\n",
    "# ----------------------------\n",
    "# ë¹ˆ ì¹¼ëŸ¼ ê²€ì‚¬ í•¨ìˆ˜ (ì•ˆì „í•œ ë²„ì „)\n",
    "# ----------------------------\n",
    "def col_has_any_value(s):\n",
    "    \"\"\"\n",
    "    ì‹œë¦¬ì¦ˆë‚˜ 1-ì»¬ëŸ¼ DataFrameì„ ë°›ì•„ì„œ 'í•˜ë‚˜ë¼ë„ ê°’ì´ ìˆëŠ”ì§€' ê²€ì‚¬.\n",
    "    - NaN/None -> ë¹ˆê°’\n",
    "    - ê³µë°± ë¬¸ìì—´ -> ë¹ˆê°’\n",
    "    - ë¦¬ìŠ¤íŠ¸/ìˆ«ì/ê¸°íƒ€ ê°ì²´ë„ str()ë¡œ ë³€í™˜ í›„ ê³µë°± íŒì •\n",
    "    \"\"\"\n",
    "    # ë§Œì•½ DataFrameì´ ë“¤ì–´ì˜¤ë©´ ëª¨ë“  ì›ì†Œë¥¼ ìˆœíšŒí•˜ë„ë¡ flatten\n",
    "    if isinstance(s, pd.DataFrame):\n",
    "        iterator = s.itertuples(index=False, name=None)\n",
    "        for row in iterator:\n",
    "            # rowëŠ” íŠœí”Œì¼ ìˆ˜ ìˆìŒ: ê° ì›ì†Œ ê²€ì‚¬\n",
    "            for val in row:\n",
    "                if val is None:\n",
    "                    continue\n",
    "                v = str(val).strip()\n",
    "                if v != \"\":\n",
    "                    return True\n",
    "        return False\n",
    "    # Seriesì¸ ê²½ìš° (ì¼ë°˜ì )\n",
    "    for val in s:\n",
    "        if val is None or (isinstance(val, float) and pd.isna(val)):\n",
    "            continue\n",
    "        if str(val).strip() != \"\":\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# ----------------------------\n",
    "# ì˜ˆ: ê¸°ì¡´ rename_map ì ìš© (ì‚¬ìš©ì ì½”ë“œ ê·¸ëŒ€ë¡œ ìœ ì§€)\n",
    "# ----------------------------\n",
    "rename_map = {\n",
    "    \"ì„œë¹„ìŠ¤ID\": \"ì„œë¹„ìŠ¤ID\",\n",
    "    \"ì„œë¹„ìŠ¤ëª…\": \"ì„œë¹„ìŠ¤ëª…\",\n",
    "    \"ì„œë¹„ìŠ¤ ìš”ì•½\": \"ì„œë¹„ìŠ¤ê°œìš”\",\n",
    "    \"ì„œë¹„ìŠ¤ê°œìš”\": \"ì„œë¹„ìŠ¤ê°œìš”\",\n",
    "    \"ìƒì„¸ ë§í¬\": \"ìƒì„¸ë§í¬\",\n",
    "    \"ìƒì„¸ë§í¬\": \"ìƒì„¸ë§í¬\",\n",
    "    \"ì†Œê´€ê¸°ê´€\": \"ì†Œê´€ë¶€ì²˜\",\n",
    "    \"ëŒ€í‘œì—°ë½ì²˜\": \"ëŒ€í‘œì—°ë½ì²˜\",\n",
    "    \"ì‹œì‘ì¼\": \"ì‹œì‘ì¼\",\n",
    "    \"ì¢…ë£Œì¼\": \"ì¢…ë£Œì¼\",\n",
    "    \"ì§„í–‰ìƒíƒœ\": \"ì‚¬ì—…ìƒíƒœ\",\n",
    "    \"INTRS_THEMA_CD\": \"ê´€ì‹¬ì£¼ì œì½”ë“œ\",\n",
    "    \"FMLY_CIRC_CD\": \"ëŒ€ìƒíŠ¹ì„±ì½”ë“œ\",\n",
    "    \"BKJR_LFTM_CYC_CD\": \"ìƒì• ì£¼ê¸°ì½”ë“œ\",\n",
    "    \"WLFARE_INFO_AGGRP_CD\": \"ë‚˜ì´\",\n",
    "    \"ì¹´í…Œê³ ë¦¬\": \"ì¹´í…Œê³ ë¦¬\",\n",
    "    \"ì‚¬ì—…ëª©ì \": \"ì„œë¹„ìŠ¤ëª©ì \",\n",
    "    \"ì§€ì›ëŒ€ìƒ\": \"ì§€ì›ëŒ€ìƒìƒì„¸\",\n",
    "    \"ì§€ì›ëŒ€ìƒìƒì„¸\": \"ì§€ì›ëŒ€ìƒìƒì„¸\",\n",
    "    \"ì§€ì›ë‚´ìš©\": \"ì„œë¹„ìŠ¤ë‚´ìš©\",\n",
    "    \"ì„œë¹„ìŠ¤ë‚´ìš©\": \"ì„œë¹„ìŠ¤ë‚´ìš©\",\n",
    "    \"ì‹ ì²­ë°©ë²•\": \"ì‹ ì²­ë°©ë²•ìƒì„¸\",\n",
    "    \"ì‹ ì²­ë°©ë²•ìƒì„¸\": \"ì‹ ì²­ë°©ë²•ìƒì„¸\",\n",
    "    \"ì œì¶œì„œë¥˜\": \"í•„ìš”ì„œë¥˜\",\n",
    "}\n",
    "\n",
    "existing_map = {k:v for k,v in rename_map.items() if k in df.columns}\n",
    "df = df.rename(columns=existing_map)\n",
    "\n",
    "# ----------------------------\n",
    "# ë¹ˆ ì¹¼ëŸ¼ ê°ì§€ ë° ì‚­ì œ (ì•ˆì „)\n",
    "# ----------------------------\n",
    "empty_cols = [c for c in df.columns if not col_has_any_value(df[c])]\n",
    "if empty_cols:\n",
    "    print(\"ì‚­ì œí•  ë¹ˆ ì»¬ëŸ¼ë“¤:\", empty_cols)\n",
    "    df = df.drop(columns=empty_cols)\n",
    "else:\n",
    "    print(\"ë¹ˆ ì»¬ëŸ¼ ì—†ìŒ. ì‚­ì œí•  í•­ëª© ì—†ìŒ.\")\n",
    "\n",
    "# ----------------------------\n",
    "# ì €ì¥\n",
    "# ----------------------------\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"ì €ì¥ ì™„ë£Œ:\", os.path.abspath(output_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d3ed0e",
   "metadata": {},
   "source": [
    "ë¯¼ê°„ë³µì§€ -ìµœì¢…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa78162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ì…ë ¥ / ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (í•„ìš”í•˜ë©´ ê²½ë¡œë¥¼ ìˆ˜ì •)\n",
    "INPUT_PATH = \"private_final_welfare_list.csv\"\n",
    "OUTPUT_PATH = \"private_final_welfare_list_with_category.csv\"\n",
    "\n",
    "# 1) ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ì‚¬ì „ (ì²´í¬ë¦¬ìŠ¤íŠ¸ ê¸°ì¤€)\n",
    "category_keywords = {\n",
    "    \"ìƒê³„ì§€ì›\": [\"ìƒê³„\", \"ì‹ë¹„\", \"ì£¼ê±°ë¹„\", \"ê³µê³¼ê¸ˆ\", \"ìƒí™œë¹„\", \"ê¸´ê¸‰ë³µì§€\", \"í• ì¸\", \"ìƒí™œì§€ì›\", \"ë°”ìš°ì²˜\"],\n",
    "    \"ëŒë´„Â·ë³´í˜¸\": [\"ê°„ë³‘\", \"ì‹œì„¤ë³´í˜¸\", \"ì£¼ì•¼ê°„\", \"ëŒë´„\", \"ë³´í˜¸\", \"ì–‘ìœ¡\", \"ë°©ë¬¸ê°„í˜¸\", \"ëŒë´„ì„œë¹„ìŠ¤\"],\n",
    "    \"ê±´ê°•Â·ì˜ë£Œ\": [\"ì§ˆë³‘\", \"ì§„ë‹¨\", \"ì¹˜ë£Œ\", \"ì˜ë£Œ\", \"ë³‘ì›\", \"ê±´ê°•ê²€ì§„\", \"ì˜ë£Œë¹„\", \"ì„ì‹ \", \"ì¶œì‚°\", \"ì§„ë£Œ\"],\n",
    "    \"ì¼ìƒìƒí™œ\": [\"ê°€ì‚¬\", \"ì‹ì‚¬\", \"ì´ë™\", \"ìœ„ìƒ\", \"ì¼ìƒìƒí™œ\", \"ìƒí™œì§€ì›\"],\n",
    "    \"ì•ˆì „ìœ„ê¸°\": [\"í­ë ¥\", \"í•™ëŒ€\", \"ìí•´\", \"ìœ„ê¸°\", \"ê¸´ê¸‰\", \"ì¬ë‚œ\", \"ì•ˆì „\", \"ìœ„í—˜\"],\n",
    "    \"ì£¼ê±°ì§€ì›\": [\"ê±°ì²˜\", \"ì£¼ê±°\", \"ì „ì„¸\", \"ì›”ì„¸\", \"ë³´ì¦ê¸ˆ\", \"ì£¼ê±°í™˜ê²½\", \"ì£¼íƒ\", \"ì„ëŒ€\", \"ì£¼íƒìˆ˜ë¦¬\"],\n",
    "    \"ì¼ìë¦¬\": [\"êµ¬ì§\", \"ìí™œ\", \"ì·¨ì—…\", \"ì§ì—…\", \"ì¼ìë¦¬\", \"ì§ì—…ëŠ¥ë ¥\", \"í›ˆë ¨\", \"ì§ì—…í›ˆë ¨\"],\n",
    "    \"ì•„ë™ì§€ì›\": [\"ì•„ë™\", \"ë³´ìœ¡\", \"ì–‘ìœ¡\", \"ìœ ì•„\", \"ì˜ìœ ì•„\", \"ì´ˆë“±\", \"ì²­ì†Œë…„\"],\n",
    "    \"ì±„ë¬´Â·ë²•ë¥ \": [\"ì±„ë¬´\", \"ë²•ë¥ \", \"ì†Œì†¡\", \"ë³€í˜¸\", \"ìƒë‹´\", \"ë¹š\", \"ì±„ê¶Œ\", \"ë¶€ì±„\"],\n",
    "    \"ê³ ë¦½Â·ê³ ë…\": [\"ê³ ë¦½\", \"ê³ ë…\", \"ì™¸ë¡œì›€\", \"ì‚¬íšŒì ê³ ë¦½\", \"ê³ ë…ì‚¬\"]\n",
    "}\n",
    "\n",
    "# 2) ìš°ì„ ìˆœìœ„ (ì¤‘ë³µ ë§¤ì¹­ ì‹œ ì´ ìˆœì„œë¡œ ë¨¼ì € ì„ íƒ)\n",
    "priority_order = [\n",
    "    \"ìƒê³„ì§€ì›\", \"ì£¼ê±°ì§€ì›\", \"ì¼ìë¦¬\", \"ì•„ë™ì§€ì›\",\n",
    "    \"ëŒë´„Â·ë³´í˜¸\", \"ê±´ê°•Â·ì˜ë£Œ\", \"ì•ˆì „ìœ„ê¸°\", \"ì±„ë¬´Â·ë²•ë¥ \", \"ê³ ë¦½Â·ê³ ë…\"\n",
    "]\n",
    "\n",
    "# 3) íŒŒì¼ ì½ê¸°\n",
    "if not os.path.exists(INPUT_PATH):\n",
    "    raise FileNotFoundError(f\"ì…ë ¥ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {INPUT_PATH}\")\n",
    "\n",
    "df = pd.read_csv(INPUT_PATH, dtype=str)\n",
    "\n",
    "# 4) ìë™ìœ¼ë¡œ ê²€ìƒ‰í•  í…ìŠ¤íŠ¸ ì¹¼ëŸ¼ í›„ë³´ (ì¡´ì¬í•˜ë©´ ì‚¬ìš©)\n",
    "text_candidates = [\n",
    "    \"ì„œë¹„ìŠ¤ê°œìš”\", \"ì„œë¹„ìŠ¤ ìš”ì•½\", \"ì„œë¹„ìŠ¤ë‚´ìš©\", \"ì§€ì›ë‚´ìš©\",\n",
    "    \"ì§€ì›ëŒ€ìƒ\", \"ì§€ì›ëŒ€ìƒìƒì„¸\", \"ì„ ì •ê¸°ì¤€\", \"ì‹ ì²­ë°©ë²•ìƒì„¸\", \"ì„œë¹„ìŠ¤ëª…\"\n",
    "]\n",
    "text_cols = [c for c in text_candidates if c in df.columns]\n",
    "\n",
    "# 5) lower-case í‚¤ì›Œë“œ ì¤€ë¹„\n",
    "category_keywords_lower = {k: [kw.lower() for kw in kws] for k,kws in category_keywords.items()}\n",
    "\n",
    "# 6) í–‰ í…ìŠ¤íŠ¸ ê²°í•© í•¨ìˆ˜\n",
    "def combined_text_for_row(row):\n",
    "    parts = []\n",
    "    for c in text_cols:\n",
    "        v = row.get(c, \"\")\n",
    "        if pd.notna(v) and str(v).strip() != \"\":\n",
    "            parts.append(str(v))\n",
    "    return \" \".join(parts).lower()  # ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ë§¤ì¹­ ë‹¨ìˆœí™”\n",
    "\n",
    "# 7) ì¹´í…Œê³ ë¦¬ í• ë‹¹ í•¨ìˆ˜\n",
    "def assign_category(row):\n",
    "    text = combined_text_for_row(row)\n",
    "    matched = []\n",
    "    for cat, kws in category_keywords_lower.items():\n",
    "        for kw in kws:\n",
    "            if kw in text:\n",
    "                matched.append(cat)\n",
    "                break\n",
    "    # ìš°ì„ ìˆœìœ„ì— ë”°ë¼ í•˜ë‚˜ë§Œ ì„ íƒ\n",
    "    for cat in priority_order:\n",
    "        if cat in matched:\n",
    "            return cat\n",
    "    # ì•„ë¬´ê²ƒë„ ë§¤ì¹­ë˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ê°’ 'ìƒê³„ì§€ì›'\n",
    "    return \"ìƒê³„ì§€ì›\"\n",
    "\n",
    "# 8) ì¹´í…Œê³ ë¦¬ ì»¬ëŸ¼ ìƒì„± (ë®ì–´ì“°ê¸°)\n",
    "df[\"ì¹´í…Œê³ ë¦¬\"] = df.apply(assign_category, axis=1)\n",
    "\n",
    "# 9) ê²°ê³¼ ê°„ë‹¨ í†µê³„ ì¶œë ¥\n",
    "print(\"ì¹´í…Œê³ ë¦¬ ë¶„í¬:\")\n",
    "print(df[\"ì¹´í…Œê³ ë¦¬\"].value_counts(dropna=False))\n",
    "\n",
    "# 10) ì €ì¥\n",
    "df.to_csv(OUTPUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"ì €ì¥ ì™„ë£Œ:\", OUTPUT_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
