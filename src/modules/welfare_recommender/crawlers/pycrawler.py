# # --- ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
# import pandas as pd
# import requests
# from bs4 import BeautifulSoup
# import re
# import json
# import io
# import pdfplumber
# import time

# # --- í•¨ìˆ˜ ì •ì˜ ë¶€ë¶„ ---

# def get_pdf_info_from_service_url(service_url):
#     """
#     (ìˆ˜ì •ë¨) ë³µì§€ ì„œë¹„ìŠ¤ ìƒì„¸ í˜ì´ì§€ URLì—ì„œ 'initParameter' ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬
#     ì²¨ë¶€ëœ PDF íŒŒì¼ì˜ Payload ì •ë³´ì™€ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
#     """
#     try:
#         headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
#         response = requests.get(service_url, headers=headers)
#         response.raise_for_status()
        
#         # 1. initParameterê°€ í¬í•¨ëœ script íƒœê·¸ ì°¾ê¸°
#         match = re.search(r'cpr\.core\.Platform\.INSTANCE\.initParameter\((.*?)\);', response.text, re.DOTALL)
#         if not match:
#             return "ì •ë³´ ì—†ìŒ", "ì •ë³´ ì—†ìŒ"

#         # 2. ì „ì²´ ë°ì´í„° íŒŒì‹±
#         init_data = json.loads(match.group(1))
#         file_list_str = init_data.get("initValue", {}).get("dsWlfareInfoDtl", "[]")
#         file_list = json.loads(file_list_str)

#         # 3. íŒŒì¼ ëª©ë¡ì—ì„œ PDF ì •ë³´ ì°¾ê¸°
#         pdf_info = None
#         for file_item in file_list:
#             if file_item.get("oriFileNm", "").endswith(".pdf"):
#                 pdf_info = file_item
#                 break
        
#         if not pdf_info:
#             return "ì •ë³´ ì—†ìŒ (PDF ì—†ìŒ)", "ì •ë³´ ì—†ìŒ (PDF ì—†ìŒ)"
            
#         # 4. ì„œë²„ì— ë³´ë‚¼ Payload ì¬ì¡°ë¦½
#         payload_data = {
#             'dmFileInfo': {
#                 'atcflId': pdf_info.get('atcflId'),
#                 'atcflSn': pdf_info.get('atcflSn'),
#                 'wlfareInfoId': pdf_info.get('wlfareInfoId')
#             }
#         }

#         # 5. Payloadë¥¼ ì´ìš©í•´ PDF ë‹¤ìš´ë¡œë“œ ìš”ì²­ (POST)
#         download_url = "https://www.bokjiro.go.kr/ssis-tbu/TWAT52011M/CmmFileUtil/download.do"
#         pdf_response = requests.post(download_url, json=payload_data)
#         pdf_response.raise_for_status()
        
#         # 6. ë©”ëª¨ë¦¬ì—ì„œ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
#         pdf_file_in_memory = io.BytesIO(pdf_response.content)
#         text = ""
#         with pdfplumber.open(pdf_file_in_memory) as pdf:
#             for page in pdf.pages:
#                 page_text = page.extract_text()
#                 if page_text:
#                     text += page_text + "\n"
        
#         full_text = text.strip() if text else "í…ìŠ¤íŠ¸ ì—†ìŒ"
#         return payload_data, full_text

#     except Exception as e:
#         return f"ì˜¤ë¥˜ ë°œìƒ: {e}", f"ì˜¤ë¥˜ ë°œìƒ: {e}"


# def find_required_documents(full_text):
#     """
#     PDF ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ 'í•„ìš”ì„œë¥˜' ê´€ë ¨ ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼)
#     """
#     if not isinstance(full_text, str) or "ì •ë³´ ì—†ìŒ" in full_text or "ì˜¤ë¥˜ ë°œìƒ" in full_text or "í…ìŠ¤íŠ¸ ì—†ìŒ" in full_text:
#         return full_text

#     keywords = ['í•„ìš”ì„œë¥˜', 'ì œì¶œì„œë¥˜', 'êµ¬ë¹„ì„œë¥˜', 'ì¤€ë¹„ì„œë¥˜', 'ì‹ ì²­ì„œë¥˜']
#     lines = full_text.split('\n')
#     start_index = -1
#     for i, line in enumerate(lines):
#         for keyword in keywords:
#             if keyword in line:
#                 start_index = i
#                 break
#         if start_index != -1:
#             break
#     if start_index == -1:
#         return "í•„ìš”ì„œë¥˜ ì •ë³´ ì—†ìŒ"
#     extracted_lines = []
#     for i in range(start_index, len(lines)):
#         line = lines[i].strip()
#         if len(extracted_lines) > 15: break
#         if not line and len(extracted_lines) > 0 and not extracted_lines[-1]: break
#         extracted_lines.append(line)
#     return "\n".join(extracted_lines).strip()


# # --- ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ ---
# if __name__ == "__main__":
#     INPUT_CSV_PATH = "bokjiro_final_processed.csv"
#     OUTPUT_CSV_PATH = "bokjiro_final_with_docs_v2.csv" # ìƒˆ ë²„ì „ìœ¼ë¡œ íŒŒì¼ëª… ë³€ê²½
    
#     print(f"'{INPUT_CSV_PATH}' íŒŒì¼ì„ ì½ìŠµë‹ˆë‹¤...")
#     df = pd.read_csv(INPUT_CSV_PATH)

#     df['pdf_payload'] = ""
#     df['required_documents'] = ""
#     total_rows = len(df)
#     print(f"ì´ {total_rows}ê°œì˜ ë³µì§€ ì„œë¹„ìŠ¤ì— ëŒ€í•œ ì •ë³´ ì¶”ì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤.")

#     for index, row in df.iterrows():
#         service_id = row.get('ë³µì§€ì‚¬ì—…ID', '')
#         service_name = row.get('ì„œë¹„ìŠ¤ëª…', 'ì´ë¦„ ì—†ìŒ')
#         if not service_id:
#             continue

#         target_url = f"https://www.bokjiro.go.kr/ssis-tbu/twataa/wlfareInfo/moveTWAT52011M.do?wlfareInfoId={service_id}"
#         print(f"[{index + 1}/{total_rows}] '{service_name}' ì²˜ë¦¬ ì¤‘...")
        
#         payload, full_text = get_pdf_info_from_service_url(target_url)
#         required_docs = find_required_documents(full_text)
        
#         df.at[index, 'pdf_payload'] = str(payload)
#         df.at[index, 'required_documents'] = required_docs
        
#         time.sleep(1)

#     print("\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ! ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤...")
#     df.to_csv(OUTPUT_CSV_PATH, index=False, encoding='utf-8-sig')
#     print(f"ğŸ‰ ìµœì¢… ê²°ê³¼ê°€ '{OUTPUT_CSV_PATH}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# --- ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
import pandas as pd
import requests
from bs4 import BeautifulSoup
import re
import json
import io
import pdfplumber
import time
import os

# --- í•¨ìˆ˜ ì •ì˜ ---

def get_pdf_text_from_service(service_id):
    """
    ë³µì§€ì‚¬ì—… IDë¥¼ ì´ìš©í•´ ìƒì„¸ í˜ì´ì§€ì— ì ‘ì† â†’ ì²¨ë¶€ PDF â†’ í•„ìš”ì„œë¥˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        service_url = f"https://www.bokjiro.go.kr/ssis-tbu/twataa/wlfareInfo/moveTWAT52011M.do?wlfareInfoId={service_id}&wlfareInfoReldBztpCd=01"
        response = requests.get(service_url, headers=headers)
        response.raise_for_status()

        # ìƒì„¸í˜ì´ì§€ ì•ˆì˜ <script> íƒœê·¸ì—ì„œ initParameter JSON ì°¾ê¸°
        match = re.search(r'cpr\.core\.Platform\.INSTANCE\.initParameter\((.*?)\);',
                          response.text, re.DOTALL)
        if not match:
            return "ì²¨ë¶€íŒŒì¼ ì •ë³´ ì—†ìŒ"

        init_data = json.loads(match.group(1))
        file_list_str = init_data.get("initValue", {}).get("dsAtchFile", "[]")
        file_list = json.loads(file_list_str)

        # PDF ì²¨ë¶€íŒŒì¼ë§Œ ì„ íƒ
        pdf_info = None
        for file_item in file_list:
            if file_item.get("oriFileNm", "").endswith(".pdf"):
                pdf_info = file_item
                break

        if not pdf_info:
            return "PDF ì—†ìŒ"

        # Payload ì¤€ë¹„
        payload = {
            "dtfInfo": [{
                "atcId": pdf_info.get("atcId"),
                "atcIdSn": pdf_info.get("atcIdSn"),
                "wlfareInfoId": pdf_info.get("wlfareInfoId")
            }]
        }

        # PDF ë‹¤ìš´ë¡œë“œ
        down_url = "https://www.bokjiro.go.kr/ssis-tbu/twataa/wlfareInfo/downFile.do"
        pdf_resp = requests.post(down_url, json=payload, headers=headers)
        pdf_resp.raise_for_status()

        # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
        pdf_file = io.BytesIO(pdf_resp.content)
        text = ""
        with pdfplumber.open(pdf_file) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"

        if not text.strip():
            return "í…ìŠ¤íŠ¸ ì—†ìŒ"

        # í•„ìš”ì„œë¥˜ ë¶€ë¶„ë§Œ ë°œì·Œ
        keywords = ['í•„ìš”ì„œë¥˜', 'ì œì¶œì„œë¥˜', 'êµ¬ë¹„ì„œë¥˜', 'ì¤€ë¹„ì„œë¥˜', 'ì‹ ì²­ì„œë¥˜']
        lines = text.splitlines()
        start_index = -1
        for i, line in enumerate(lines):
            if any(k in line for k in keywords):
                start_index = i
                break
        if start_index == -1:
            return "í•„ìš”ì„œë¥˜ ì •ë³´ ì—†ìŒ"

        extracted_lines = []
        for i in range(start_index, len(lines)):
            line = lines[i].strip()
            if len(extracted_lines) > 15:  # ë„ˆë¬´ ê¸¸ê²Œ ì•ˆ ë½‘ë„ë¡ ì œí•œ
                break
            if not line and extracted_lines:  # ë¹ˆ ì¤„ ë§Œë‚˜ë©´ ì¢…ë£Œ
                break
            extracted_lines.append(line)

        return "\n".join(extracted_lines).strip()

    except Exception as e:
        return f"ì˜¤ë¥˜ ë°œìƒ: {e}"


# --- ë©”ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    INPUT_CSV = "bokjiro_final_processed.csv"
    OUTPUT_CSV = "bokjiro_final_with_docs.csv"

    df = pd.read_csv(INPUT_CSV)

    # ìƒˆ ì»¬ëŸ¼ë§Œ ì¶”ê°€
    df["í•„ìš”ì„œë¥˜_ì¶”ì¶œ"] = ""

    total = len(df)
    print(f"ì´ {total}ê°œ ë³µì§€ ì„œë¹„ìŠ¤ ì²˜ë¦¬ ì‹œì‘...")

    for idx, row in df.iterrows():
        service_id = row["ë³µì§€ì‚¬ì—…ID"]
        service_name = row["ë³µì§€ì‚¬ì—…ëª…"]

        print(f"[{idx+1}/{total}] {service_name} ({service_id}) ì²˜ë¦¬ ì¤‘...")

        required_docs = get_pdf_text_from_service(service_id)
        df.at[idx, "í•„ìš”ì„œë¥˜_ì¶”ì¶œ"] = required_docs

        time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€

    df.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")
    print(f"\nâœ… ì™„ë£Œ! ê²°ê³¼ëŠ” '{OUTPUT_CSV}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
